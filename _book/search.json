[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "JDemetra+ online documentation",
    "section": "",
    "text": "Welcome to the JDemetra+ online documentation.\nJDemetra+ is a software for seasonal adjustment and other time series functions, developed in the framework of Eurostat’s “Centre of Excellence on Statistical Methods and Tools” by the National Bank of Belgium with the support of the Bundesbank.\nTo learn more about this project https://ec.europa.eu/eurostat/cros/content/centre-excellence-statistical-methods-and-tools."
  },
  {
    "objectID": "G-what-is-jd.html",
    "href": "G-what-is-jd.html",
    "title": "JDemetra+",
    "section": "",
    "text": "JD+ is a library of algorithms for seasonal adjustment and time series econometrics. You can learn more about the history of the project here (link to below)\nlink to key references - handbooks (3) - sets of guidelines (2 or 3 ?)"
  },
  {
    "objectID": "G-what-is-jd.html#main-functions",
    "href": "G-what-is-jd.html#main-functions",
    "title": "JDemetra+",
    "section": "Main functions",
    "text": "Main functions\n\nSeasonal adjustment algorithms\nAll are available for low and high frequency data.\n\n\n\nAlgorithms\nAccess\nKey features\n\n\n\n\nX13-Arima\n\n\n\n\nTramo-Seats\n\n\n\n\nSTL\n\n\n\n\nState Space Models (STS)\n\n\n\n\n\n\n\nTemporal Disaggregation and benchmarking\n\n\n\nAlgorithms\nAccess\nKey features\n\n\n\n\nChow-lin\n\n\n\n\nFernandez\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrend-cycle estimation\n\n\n\nAlgorithms\nAccess\nKey features"
  },
  {
    "objectID": "G-what-is-jd.html#nowcasting",
    "href": "G-what-is-jd.html#nowcasting",
    "title": "JDemetra+",
    "section": "Nowcasting",
    "text": "Nowcasting\n\n\n\nAlgorithms\nAccess\nKey features"
  },
  {
    "objectID": "G-what-is-jd.html#structure-of-this-book",
    "href": "G-what-is-jd.html#structure-of-this-book",
    "title": "JDemetra+",
    "section": "Structure of this book",
    "text": "Structure of this book\nThis book is divided in four parts, allowing the user to access the resources from different perspectives.\n\nAlgorithms\nThis part provides a step by step description of all the algorithms featured in JD+, grouped by purpose - seasonal adjustment - benchmarking - temporal disaggregation - … links\n\n\nTools\nJDemetra+ offers 3 kinds of tools - A Graphical User Interface (GUI) which can be enhanced with plug-ins - A set of R packages - A Cruncher for mass production in seasonal adjustment\n\n\nUnderlying Statistical Methods\nThis part gives details about the underlying statistical methods to foster a more in-depth understanding of the algorithms.Those methods are described in the light and spirit of their use as building blocks of the algorithms presented above, not aiming at all at their comprehensive coverage."
  },
  {
    "objectID": "G-what-is-jd.html#how-to-use-this-book",
    "href": "G-what-is-jd.html#how-to-use-this-book",
    "title": "JDemetra+",
    "section": "How to use this book",
    "text": "How to use this book\n\nAudience\nThis book targets the beginner as well as seasoned methodologist interested in using JDemetra+ software for any the purposes listed below.\nThe documentation is built in layers allowing to skip details and complexity in the first steps"
  },
  {
    "objectID": "A-sa.html",
    "href": "A-sa.html",
    "title": "Seasonal Adjustment",
    "section": "",
    "text": "The primary aim of the seasonal adjustment process is to remove seasonal fluctuations from the time series.\n[insert def SA from b_ov]"
  },
  {
    "objectID": "A-sa.html#data-frequencies",
    "href": "A-sa.html#data-frequencies",
    "title": "Seasonal Adjustment",
    "section": "Data frequencies",
    "text": "Data frequencies\nThe seasonal adjustment methods available in JDemetra+ aim to decompose a time series into components and remove seasonal fluctuations from the observed time series. The X-11 method considers monthly and quarterly series while SEATS is able to decompose series with 2, 3, 4, 6 and 12 observations per year."
  },
  {
    "objectID": "A-sa.html#unobserved-components-uc",
    "href": "A-sa.html#unobserved-components-uc",
    "title": "Seasonal Adjustment",
    "section": "Unobserved Components (UC)",
    "text": "Unobserved Components (UC)\nThe main components, each representing the impact of certain types of phenomena on the time series (\\(X_{t}\\)), are:\n\nThe trend (\\(T_{t}\\)) that captures long-term and medium-term behaviour;\nThe seasonal component (\\(S_{t}\\)) representing intra-year fluctuations, monthly or quarterly, that are repeated more or less regularly year after year;\nThe irregular component (\\(I_{t}\\)) combining all the other more or less erratic fluctuations not covered by the previous components.\n\nIn general, the trend consists of 2 sub-components:\n\nThe long-term evolution of the series;\nThe cycle, that represents the smooth, almost periodic movement around the long-term evolution of the series. It reveals a succession of phases of growth and recession.\n\nTo achieve this goal, seasonal adjustment methods decompose the original time series into components that capture specific movements. These components are: trend-cycle, seasonality and irregularity. The trend-cycle component includes long-term and medium-term movements in the data. For seasonal adjustment purposes there is no need to divide this component into two parts. JDemetra+ refers to the trend-cycle as trend and consequently this convention is used here. For seasonal adjustment purposes both TRAMO-SEATS and X-13ARIMA-SEATS do not separate the long-term trend from the cycle as these two components are usually too short to perform their reliable estimation. Consequently, hereafter TRAMO-SEATS and X-13ARIMA-SEATS estimate the trend component. However, the original TRAMO-SEATS may separate the long-term trend from the cycle through the Hodrick-Precsott filter using the output of the standard decomposition. It should be remembered that JDemetra+ refers to the trend-cycle as trend (\\(T_{t}\\)), and consequently this convention is used in this document.\nTRAMO-SEATS considers two decomposition models:\n\nThe additive model: \\(X_{t} = T_{t} + S_{t} + I_{t}\\);\nThe log additive model: \\(log(X_{t}) = log(T_{t}) + log(S_{t}) + log(I_{t})\\).\n\nApart from these two decomposition types X-13ARIMA-SEATS allows the user to apply also the multiplicative model: \\(X_{t} = T_{t} \\times S_{t} \\times I_{t}\\).\nA time series \\(x_{t}\\), which is a subject to a decomposition, is assumed to be a realisation of a discrete-time stochastic, covariance-stationary linear process, which is a collection of random variables \\(x_{t}\\), where \\(t\\) denotes time. It can be shown that any stochastic, covariance-stationary process can be presented in the form:\n\\(x_{t} = \\mu_{t} + {\\widetilde{x}}_{t}\\), \\[1\\]\nwhere \\(\\mu_{t}\\) is a linearly deterministic component and \\({\\widetilde{x}}_{t}\\) is a linearly interderministic component, such as:\n\\[{\\widetilde{x}}_{t} = {\\sum_{j = 0}^{\\infty}\\psi_{j}a}_{t - j}\n  \\], \\[2\\]\nwhere \\(\\sum_{j = 0}^{\\infty}\\psi_{i}^{2} < \\infty\\) (coefficients \\(\\psi_{j}\\) are absolutely summable), \\(\\psi_{0} = 1\\) and \\(a_{t}\\) is the white noise error with zero mean and constant variance \\(V_{a}\\). The error term \\(a_{t}\\) represents the one-period ahead forecast error of \\(x_{t}\\), that is:\n\\[\n  a_{t} = {\\widetilde{x}}_{t} - {\\widehat{x}}_{t|t - 1}\n  \\], \\[3\\]\nwhere \\[{\\widehat{x}}_{t|t - 1}\\] is the forecast of \\[{\\widetilde{x}}_{t}\\] made at period \\(t - 1\\). As \\(a_{t}\\) represents what is new in \\[{\\widetilde{x}}_{t}\\] in point \\(t\\), i.e., not contained in the past values of \\[{\\widetilde{x}}_{t}\\], it is also called innovation of the process. From \\[3\\] \\[{\\widetilde{x}}_{t}\\] can be viewed as a linear filter applied to the innovations.\nThe equation 7.1 is called a Wold representation. It presents a process as a sum of linearly deterministic component \\(\\mu_{t}\\) and linearly interderministic component \\(\\sum_{j = 0}^{\\infty}\\psi_{j}a_{t - j}\\), the first one is perfectly predictable once the history of the process \\(x_{t - 1}\\) is known and the second one is impossible to predict perfectly. This explains why the stochastic process cannot be perfectly predicted.\nUnder suitable conditions \\[{\\widetilde{x}}_{t}\\] can be presented as a weighted sum of its past values and \\(a_{t}\\), i.e.:\n\\[\n  { {\\widetilde{x}}_{t} = \\sum_{j = 0}^{\\infty}\\pi_{j}{\\widetilde{x}}_{t - j} + a}_{t}\n  \\], \\[4\\]\nIn general, for the observed time series, the assumptions concerning the nature of the process \\[1\\] do not hold for various reasons. Firstly, most observed time series display a mean that cannot be assumed to be constant due to the presence of a trend and the seasonal movements. Secondly, the variance of the time series may vary in time. Finally, the observed time series usually contain outliers, calendar effects and regression effects, which are treated as deterministic. Therefore, in practice a prior transformation and an adjustment need to be applied to the time series. The constant variance is usually achieved through taking a logarithmic transformation and the correction for the deterministic effects, while stationarity of the mean is achieved by applying regular and seasonal differencing. These processes, jointly referred to as preadjustment or linearization, can be performed with the TRAMO or RegARIMA models. Besides the linearisation, forecasts and backcasts of stochastic time series are estimated with the ARIMA model, allowing for later application of linear filters at both ends of time series. The estimation performed with these models delivers the stochastic part of the time series, called the linearised series, which is assumed to be an output of a linear stochastic process.1 The deterministic effects are removed from the time series and used to form the final components.\nIn the next step the linearised series is decomposed into its components. There is a fundamental difference in how this process is performed in TRAMO-SEATS and X-13ARIMA-SEATS. In TRAMO-SEATS the decomposition is performed by the SEATS procedure, which follows a so called ARIMA model based approach. In principle, it aims to derive the components with statistical models. More information is given in the SEATS section. X-13ARIMA-SEATS offers two algorithms for decomposition: SEATS and X-11. The X-11 algorithm, which is described in the X-11 section section, decomposes a series by means of linear filters. Finally, in both methods the final components are derived by the assignment of the deterministic effects to the stochastic components. Consequently, the role of the ARIMA models is different in each method. TRAMO-SEATS applies the ARIMA models both in the preadjustment step and in the decomposition procedure. On the contrary, when the X-11 algorithm is used for decomposition, X-13ARIMA-SEATS uses the ARIMA model only in the preadjustment step. In summary, the decomposition procedure that results in an estimation of the seasonal component requires prior identification of the deterministic effects and their removal from the time series. This is achieved through the linearisation process performed by the TRAMO and the RegARIMA models, shortly discussed in the Linearisation with the TRAMO and RegARIMA models section.The linearised series is then decomposed into the stochastic components with SEATS or X-11 algorithms.\nX-13\nX-13ARIMA is a seasonal adjustment program developed and supported by the U.S. Census Bureau. It is based on the U.S. Census Bureau’s earlier X-11 program, the X-11-ARIMA program developed at Statistics Canada, the X-12-ARIMA program developed by the U.S. Census Bureau."
  },
  {
    "objectID": "A-sa.html#detecting-seasonal-patterns",
    "href": "A-sa.html#detecting-seasonal-patterns",
    "title": "Seasonal Adjustment",
    "section": "Detecting seasonal patterns",
    "text": "Detecting seasonal patterns"
  },
  {
    "objectID": "A-sa.html#pre-treatment",
    "href": "A-sa.html#pre-treatment",
    "title": "Seasonal Adjustment",
    "section": "Pre-treatment",
    "text": "Pre-treatment\n\nCalendar correction\ndetails of regressor building in calendar chapter\n\nrationale\n\n\nmethod\n\n\ntools\n\n\n\nOutliers\n\nrationale\n\n\nmethod\n\n\ntools\n\n\n\nReg-Arima Model\nTramo and Reg-Arima are very similar…details in M chapter\n\n\nModel evaluation\ngoodness-of-fit"
  },
  {
    "objectID": "A-sa.html#x-11-decompostion",
    "href": "A-sa.html#x-11-decompostion",
    "title": "Seasonal Adjustment",
    "section": "X-11 Decompostion",
    "text": "X-11 Decompostion\nthis part should allow to use x-11 via RJDemetra as well as via GUI\n\nQuick launch with default specifications\n\n\nOutput 1: series\nX-11 gives access to a great part of it’s intermediate computations\nHere we focus on the final components (Table D)\nList of series (edit : table with name and meaning)\nRetrieve in GUI\nRetrieve in R\n\n\nOutput 2: final parameters\nRelevant if parameters not set manually\nList Final trend filter final seasonal filer\nRetrieve via GUI image\nRetrieve in R\n\n\nOutput 3: diagnostics\nX11 produces the following type diagnostics or quality measures Table with link to detail\n\n\nSpecifications / parameters\n\nList\n\nGeneral settings\n\nMode\n\ncheck if this option still works, if so add and edit instructions from old page)\nif not but button present : explain that the mode is determined in pre-adjustment (function)\n\nSeasonal component\n\ncheck if still relevant, idem as above\nin v.2.3 if not ticked, S estimated but options on seasonal filter not available\n\nForecasts horizon\n\nLength of the forecasts generated by the RegARIMA model - in months (positive values) - years (negative values) - if set to is set to 0, the X-11 procedure does not use any model-based forecasts but the original X-11 type forecasts for one year. - default value: -1, thus one year from the Arima model\n\nBackcasts horizon\n\nLength of the backcasts generated by the RegARIMA model - in months (positive values) - years (negative values) - default value: 0\n\n\nIrregular correction\n\nLSigma\n\nsets lower sigma (standard deviation) limit used to down-weight the extreme irregular values in the internal seasonal adjustment iterations, learn more here (LINK to M_ chapter)\nvalues in \\([0,Usigma]\\)\ndefault value is 1.5\n\nUSigma\n\nsets upper sigma (standard deviation)\nvalues in \\([Lsigma,+\\infty]\\)\ndefault value is 2.5\n\nCalendarsigma\n\nallows to set different LSigma and USigma for each period\nvalues\n\nNone (default)\nAll: standard errors used for the extreme values detection and adjustment computed separately for each calendar month/quarter\nSignif: groups determined by cochran test (check)\nSigmavec: set two customized groups of periods\n\n\nExcludeforecasts\n\nticked : forecasts and backcasts from the RegARIMA model not used in Irregular Correction\nunticked (default): forecasts and backcasts used\n\n\n\n\nSeasonality extraction filters\n\nSeasonal filter choice\n\nSpecifies which be used to estimate the seasonal factors for the entire series (link to relevant part in M chapter)\n\nS3 × 1 – 3 × 1 moving average.\nS3 × 3 – 3 × 3 moving average.\nS3 × 5 – 3 × 5 moving average.\nS3 × 9 – 3 × 9 moving average.\nS3 × 15 – 3 × 15 moving average.\nStable – a single seasonal factor for each calendar period is generated by calculating a simple average over all values for each period (taken after detrending and outlier correction).\nX11Default – 3 × 3 moving average is used to calculate the initial seasonal factors and a 3 × 5 moving average to calculate the final seasonal factors.\nMsr – automatic choice of a seasonal filter. The seasonal filters can be selected for the entire series, or for a particular month or quarter.\ndefault value: Msr\n\nCheck: will user choice be applied to all steps or only to final phase D step\n\nDetails on seasonal filters\n\nSets different seasonal filters by period in order to account for seasonal heteroskedasticity (link to M chapter)\n\ndefault value: empty\n\n\n\nTrend estimation filters\n\nAutomatic Henderson filter our user-defined\n\ndefault: length 13\nunticked: user defined length choice\n\nHenderson filter length choice\n\nvalues: odd number in \\([3,101]\\)\ndefault value: 13\n\n\nCheck: will user choice be applied to all steps or only to final phase D step\n\n\n\nParameter setting in GUI\nhere v2, adjust to v3 asap\n\n\nParameter setting in R packages\nextensive help on functions available in package help pages Rcode snippets\nIn R, to implement any param change, it is required to retrieve current spec, modify it and apply it again (see T R packages chapter for details). (specific link)\nhere example changing all the settings (just remove irrelevant changes)\nRjdemetra (v2) Edit : here static R code link to a “worked example” wih dynamic code ticked box in GUI corresponds to …? in R\n\n#Creating a modified specification, customizing all available X11 parameters\nmodified_spec<- x13_spec(current_sa_model,\n    #x11.mode=\"?\",\n    #x11.seasonalComp = \"?\",\n    x11.fcasts = -2,\n    x11.bcasts = -1,\n    x11.lsigma = 1.2,\n    x11.usigma = 2.8,\n    x11.calendarSigma = NA, # EDIT with example\n      x11.sigmaVector = NA,\n    x11.excludeFcasts = NA\n    # filters \n    x11.trendAuto = NA, # needed inf value ?\n    x11.trendma = 23,\n    x11.seasonalma = \"S3X9\n    # details on seasonal filters)\n\n#New SA estimation : apply modified_spec\nmodified_sa_model<-x13(raw_series,modified_spec)\n\nEDIT : link to package help page v2 +v3"
  },
  {
    "objectID": "A-sa.html#stl",
    "href": "A-sa.html#stl",
    "title": "Seasonal Adjustment",
    "section": "STL",
    "text": "STL"
  },
  {
    "objectID": "A-sa.html#seats",
    "href": "A-sa.html#seats",
    "title": "Seasonal Adjustment",
    "section": "SEATS",
    "text": "SEATS"
  },
  {
    "objectID": "A-sa.html#ssf",
    "href": "A-sa.html#ssf",
    "title": "Seasonal Adjustment",
    "section": "SSF",
    "text": "SSF"
  },
  {
    "objectID": "A-sa.html#quality-assessment",
    "href": "A-sa.html#quality-assessment",
    "title": "Seasonal Adjustment",
    "section": "Quality assessment",
    "text": "Quality assessment\n\nResidual seasonality\n\n\nResidual calendar effects"
  },
  {
    "objectID": "A-sa-hf.html#tools",
    "href": "A-sa-hf.html#tools",
    "title": "Seasonal adjustment of high frequency data",
    "section": "Tools",
    "text": "Tools\ncode here and/or link to R packages chapter"
  },
  {
    "objectID": "A-sa-hf.html#unobserved-components",
    "href": "A-sa-hf.html#unobserved-components",
    "title": "Seasonal adjustment of high frequency data",
    "section": "Unobserved Components",
    "text": "Unobserved Components"
  },
  {
    "objectID": "A-sa-hf.html#identifying-seasonal-patterns",
    "href": "A-sa-hf.html#identifying-seasonal-patterns",
    "title": "Seasonal adjustment of high frequency data",
    "section": "Identifying seasonal patterns",
    "text": "Identifying seasonal patterns\n\nSpectral analysis\n\n\nSeasonality tests"
  },
  {
    "objectID": "A-sa-hf.html#pre-adjustment",
    "href": "A-sa-hf.html#pre-adjustment",
    "title": "Seasonal adjustment of high frequency data",
    "section": "Pre-adjustment",
    "text": "Pre-adjustment\n\nCalendar correction\n\n\nOutliers and intervention variables\n\n\nLinearization"
  },
  {
    "objectID": "A-sa-hf.html#decomposition",
    "href": "A-sa-hf.html#decomposition",
    "title": "Seasonal adjustment of high frequency data",
    "section": "Decomposition",
    "text": "Decomposition\n\nExtended X-11\n\n\nSTL decomposition\n\n\nArima Model Based (AMB) Decomposition\n\n\nState Space framework"
  },
  {
    "objectID": "A-sa-hf.html#quality-assessment",
    "href": "A-sa-hf.html#quality-assessment",
    "title": "Seasonal adjustment of high frequency data",
    "section": "Quality assessment",
    "text": "Quality assessment\n\nResidual seasonality\n\n\nResidual Calendar effects"
  },
  {
    "objectID": "A-outlier-detection.html",
    "href": "A-outlier-detection.html",
    "title": "Outlier detection",
    "section": "",
    "text": "(in or outside a seasonal adjustment process)"
  },
  {
    "objectID": "A-outlier-detection.html#motivation",
    "href": "A-outlier-detection.html#motivation",
    "title": "Outlier detection",
    "section": "Motivation",
    "text": "Motivation"
  },
  {
    "objectID": "A-outlier-detection.html#with-reg-arima-models",
    "href": "A-outlier-detection.html#with-reg-arima-models",
    "title": "Outlier detection",
    "section": "With Reg Arima models",
    "text": "With Reg Arima models"
  },
  {
    "objectID": "A-outlier-detection.html#specific-terror-tool",
    "href": "A-outlier-detection.html#specific-terror-tool",
    "title": "Outlier detection",
    "section": "Specific TERROR tool",
    "text": "Specific TERROR tool"
  },
  {
    "objectID": "A-outlier-detection.html#with-structural-models-bsm",
    "href": "A-outlier-detection.html#with-structural-models-bsm",
    "title": "Outlier detection",
    "section": "With structural models (BSM)",
    "text": "With structural models (BSM)"
  },
  {
    "objectID": "A-calendar-and-input.html",
    "href": "A-calendar-and-input.html",
    "title": "Calendar and user-defined corrections",
    "section": "",
    "text": "Overview of Calendar effects in JDemetra\nedit : this has a evolved a lot with v3 definition possibilities via GUI and R have to be re described\nThe following description of the calendar effects in JDemetra+ is strictly based on PALATE, J. (2014).\nA natural way for modelling calendar effects consists of distributing the days of each period into different groups. The regression variable corresponding to a type of day (a group) is simply defined by the number of days it contains for each period. Usual classifications are:\n\nTrading days (7 groups): each day of the week defines a group (Mondays,...,Sundays);\nWorking days (2 groups): week days and weekends.\n\nThe definition of a group could involve partial days. For instance, we could consider that one half of Saturdays belong to week days and the second half to weekends.\nUsually, specific holidays are handled as Sundays and they are included in the group corresponding to \"non-working days\". This approach assumes that the economic activity on national holidays is the same (or very close to) the level of activity that is typical for Sundays. Alternatively, specific holidays can be considered separately, e.g. by the specification that divided days into three groups:\n\nWorking days (Mondays to Fridays, except for specific holidays),\nNon-working days (Saturdays and Sundays, except for specific holidays),\nSpecific holidays.\n\n\n\nSummary of the method used in JDemetra+ to compute trading day and working day effects\nThe computation of trading day and working days effects is performed in four steps:\n\nComputation of the number of each weekday performed for all periods.\nCalculation of the usual contrast variables for trading day and working day.\nCorrection of the contrast variables with specific holidays (for each holiday add +1 to the number of Sundays and subtract 1 from the number of days of the holiday). The correction is not performed if the holiday falls on a Sunday, taking into account the validity period of the holiday.\nCorrection of the constant variables for long term mean effects, > taking into account the validity period of the holiday; see below > for the different cases.\n\nThe corrections of the constant variables may receive a weight corresponding to the part of the holiday considered as a Sunday.\nAn example below illustrates the application of the above algorithm for the hypothetical country in which three holidays are celebrated:\n\nNew Year (a fixed holiday, celebrated on 01 January);\nShrove Tuesday (a moving holiday, which falls 47 days before Easter Sunday, celebrated until the end of 2012);\nFreedom day (a fixed holiday, celebrated on 25 April).\n\nThe consecutive steps in calculation of the calendar for 2012 and 2013 years are explained below.\nFirst, the number of each day of the week in the given month is calculated as it is shown in table below.\nNumber of each weekday in different months\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonth\nMon\nTue\nWed\nThu\nFri\nSat\nSun\n\n\n\n\nJan-12\n5\n5\n4\n4\n4\n4\n5\n\n\nFeb-12\n4\n4\n5\n4\n4\n4\n4\n\n\nMar-12\n4\n4\n4\n5\n5\n5\n4\n\n\nApr-12\n5\n4\n4\n4\n4\n4\n5\n\n\nMay-12\n4\n5\n5\n5\n4\n4\n4\n\n\nJun-12\n4\n4\n4\n4\n5\n5\n4\n\n\nJul-12\n5\n5\n4\n4\n4\n4\n5\n\n\nAug-12\n4\n4\n5\n5\n5\n4\n4\n\n\nSep-12\n4\n4\n4\n4\n4\n5\n5\n\n\nOct-12\n5\n5\n5\n4\n4\n4\n4\n\n\nNov-12\n4\n4\n4\n5\n5\n4\n4\n\n\nDec-12\n5\n4\n4\n4\n4\n5\n5\n\n\nJan-13\n4\n5\n5\n5\n4\n4\n4\n\n\nFeb-13\n4\n4\n4\n4\n4\n4\n4\n\n\nMar-13\n4\n4\n4\n4\n5\n5\n5\n\n\nApr-13\n5\n5\n4\n4\n4\n4\n4\n\n\nMay-13\n4\n4\n5\n5\n5\n4\n4\n\n\nJun-13\n4\n4\n4\n4\n4\n5\n5\n\n\nJul-13\n5\n5\n5\n4\n4\n4\n4\n\n\nAug-13\n4\n4\n4\n5\n5\n5\n4\n\n\nSep-13\n5\n4\n4\n4\n4\n4\n5\n\n\nOct-13\n4\n5\n5\n5\n4\n4\n4\n\n\nNov-13\n4\n4\n4\n4\n5\n5\n4\n\n\nDec-13\n5\n5\n4\n4\n4\n4\n5\n\n\n\nNext, the contrast variables are calculated (table below) as a result of the linear transformation applied to the variables presented in table below.\nContrast variables (series corrected for leap year effects)\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonth\nMon\nTue\nWed\nThu\nFri\nSat\nLength\n\n\n\n\nJan-12\n0\n0\n-1\n-1\n-1\n-1\n0\n\n\nFeb-12\n0\n0\n1\n0\n0\n0\n0.75\n\n\nMar-12\n0\n0\n0\n1\n1\n1\n0\n\n\nApr-12\n0\n-1\n-1\n-1\n-1\n-1\n0\n\n\nMay-12\n0\n1\n1\n1\n0\n0\n0\n\n\nJun-12\n0\n0\n0\n0\n1\n1\n0\n\n\nJul-12\n0\n0\n-1\n-1\n-1\n-1\n0\n\n\nAug-12\n0\n0\n1\n1\n1\n0\n0\n\n\nSep-12\n-1\n-1\n-1\n-1\n-1\n0\n0\n\n\nOct-12\n1\n1\n1\n0\n0\n0\n0\n\n\nNov-12\n0\n0\n0\n1\n1\n0\n0\n\n\nDec-12\n0\n-1\n-1\n-1\n-1\n0\n0\n\n\nJan-13\n0\n1\n1\n1\n0\n0\n0\n\n\nFeb-13\n0\n0\n0\n0\n0\n0\n-0.25\n\n\nMar-13\n-1\n-1\n-1\n-1\n0\n0\n0\n\n\nApr-13\n1\n1\n0\n0\n0\n0\n0\n\n\nMay-13\n0\n0\n1\n1\n1\n0\n0\n\n\nJun-13\n-1\n-1\n-1\n-1\n-1\n0\n0\n\n\nJul-13\n1\n1\n1\n0\n0\n0\n0\n\n\nAug-13\n0\n0\n0\n1\n1\n1\n0\n\n\nSep-13\n0\n-1\n-1\n-1\n-1\n-1\n0\n\n\nOct-13\n0\n1\n1\n1\n0\n0\n0\n\n\nNov-13\n0\n0\n0\n0\n1\n1\n0\n\n\nDec-13\n5\n5\n4\n4\n4\n4\n0\n\n\n\nIn the next step the corrections for holidays is done in the following way:\n\nNew Year: In 2012 it falls on a Sunday. Therefore no correction is applied. In 2013 it falls on a Tuesday. Consequently, the following corrections are applied to the number of each weekday in January: Tuesday -1, Sunday +1, so the following corrections are applied to the contrast variables: -2 for Tuesday and -1 for the other contrast variables.\nShrove Tuesday: It is a fixed day of the week holiday that always falls on Tuesday. For this reason in 2012 the following corrections are applied to the number of each weekday in February: Tuesday -1, Sunday +1, so the following corrections are applied to the contrast variables: -2 for the contrast variable associated with Tuesday, and -1 for the other contrast variables. The holiday expires at the end of 2012. Therefore no corrections are made for 2013.\nFreedom Day: In 2012 it falls on a Wednesday. Consequently, the following corrections are applied to the number of each weekday in April: Wednesday -1, Sunday +1, so the following corrections are applied to the contrast variables: -2 for Wednesday and -1 for the other contrast variables. In 2013 it falls on Thursday. Therefore, the following corrections are applied to the number of each weekday in April: Thursday -1, Sunday +1, so the following corrections are applied to the contrast variables: -2 for Thursday, and -1 for the other contrast variables.\n\nThe result of these corrections is presented in table below.\nContrast variables corrected for holidays\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonth\nMon\nTue\nWed\nThu\nFri\nSat\nLength\n\n\n\n\nJan-12\n0\n0\n-1\n-1\n-1\n-1\n0\n\n\nFeb-12\n-1\n-2\n0\n-1\n-1\n-1\n0.75\n\n\nMar-12\n0\n0\n0\n1\n1\n1\n0\n\n\nApr-12\n-1\n-2\n-3\n-2\n-2\n-2\n0\n\n\nMay-12\n0\n1\n1\n1\n0\n0\n0\n\n\nJun-12\n0\n0\n0\n0\n1\n1\n0\n\n\nJul-12\n0\n0\n-1\n-1\n-1\n-1\n0\n\n\nAug-12\n0\n0\n1\n1\n1\n0\n0\n\n\nSep-12\n-1\n-1\n-1\n-1\n-1\n0\n0\n\n\nOct-12\n1\n1\n1\n0\n0\n0\n0\n\n\nNov-12\n0\n0\n0\n1\n1\n0\n0\n\n\nDec-12\n0\n-1\n-1\n-1\n-1\n0\n0\n\n\nJan-13\n-1\n-1\n0\n0\n-1\n-1\n0\n\n\nFeb-13\n0\n0\n0\n0\n0\n0\n-0.25\n\n\nMar-13\n-1\n-1\n-1\n-1\n0\n0\n0\n\n\nApr-13\n0\n0\n-1\n-2\n-1\n-1\n0\n\n\nMay-13\n0\n0\n1\n1\n1\n0\n0\n\n\nJun-13\n-1\n-1\n-1\n-1\n-1\n0\n0\n\n\nJul-13\n1\n1\n1\n0\n0\n0\n0\n\n\nAug-13\n0\n0\n0\n1\n1\n1\n0\n\n\nSep-13\n0\n-1\n-1\n-1\n-1\n-1\n0\n\n\nOct-13\n0\n1\n1\n1\n0\n0\n0\n\n\nNov-13\n0\n0\n0\n0\n1\n1\n0\n\n\nDec-13\n0\n0\n-1\n-1\n-1\n-1\n0\n\n\n\nFinally, the long term corrections are applied on each year of the validity period of the holiday.\n\nNew Year: Correction on the contrasts: +1, to be applied to January of 2012 and 2013.\nShrove Tuesday: It may fall either in February or in March. It will fall in March if Easter is on or after 17 April. Taking into account the theoretical distribution of Easter, it gives: prob(March) = +0.22147, prob(February) = +0.77853. The correction of the contrasts will be +1.55707 for Tuesday in February 2012 and +0.77853 for the other contrast variables. The correction of the contrasts will be +0.44293 for Tuesday in March 2012, +0.22147 for the other contrast variables.\nFreedom Day: Correction on the contrasts: +1, to be applied to April of 2012 and 2013.\n\nThe modifications due to the corrections described above are presented in table below.\nTrading day variables corrected for the long term effects\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonth\nMon\nTue\nWed\nThu\nFri\nSat\nLength\n\n\n\n\nJan-12\n1\n1\n0\n0\n0\n0\n0\n\n\nFeb-12\n-0.22115\n-0.44229\n0.778853\n-0.22115\n-0.22115\n-0.22115\n0.75\n\n\nMar-12\n0.221147\n0.442293\n0.221147\n1.221147\n1.221147\n1.221147\n0\n\n\nApr-12\n0\n-1\n-2\n-1\n-1\n-1\n0\n\n\nMay-12\n0\n1\n1\n1\n0\n0\n0\n\n\nJun-12\n0\n0\n0\n0\n1\n1\n0\n\n\nJul-12\n0\n0\n-1\n-1\n-1\n-1\n0\n\n\nAug-12\n0\n0\n1\n1\n1\n0\n0\n\n\nSep-12\n-1\n-1\n-1\n-1\n-1\n0\n0\n\n\nOct-12\n1\n1\n1\n0\n0\n0\n0\n\n\nNov-12\n0\n0\n0\n1\n1\n0\n0\n\n\nDec-12\n0\n-1\n-1\n-1\n-1\n0\n0\n\n\nJan-13\n0\n0\n1\n1\n0\n0\n0\n\n\nFeb-13\n0\n0\n0\n0\n0\n0\n-0.25\n\n\nMar-13\n-1\n-1\n-1\n-1\n0\n0\n0\n\n\nApr-13\n1\n1\n0\n-1\n0\n0\n0\n\n\nMay-13\n0\n0\n1\n1\n1\n0\n0\n\n\nJun-13\n-1\n-1\n-1\n-1\n-1\n0\n0\n\n\nJul-13\n1\n1\n1\n0\n0\n0\n0\n\n\nAug-13\n0\n0\n0\n1\n1\n1\n0\n\n\nSep-13\n0\n-1\n-1\n-1\n-1\n-1\n0\n\n\nOct-13\n0\n1\n1\n1\n0\n0\n0\n\n\nNov-13\n0\n0\n0\n0\n1\n1\n0\n\n\nDec-13\n0\n0\n-1\n-1\n-1\n-1\n0\n\n\n\n\n\nMean and seasonal effects of calendar variables\nThe calendar effects produced by the regression variables that fulfil the definition presented above include a mean effect (i.e. an effect that is independent of the period) and a seasonal effect (i.e. an effect that is dependent of the period and on average it is equal to 0). Such an outcome is inappropriate, as in the usual decomposition of a series the mean effect should be allocated to the trend component and the fixed seasonal effect should be affected to the corresponding component. Therefore, the actual calendar effect should only contain effects that don't belong to the other components.\nIn the context of JDemetra+ the mean effect and the seasonal effect are long term theoretical effects rather than the effects computed on the time span of the considered series (which should be continuously revised).\nThe mean effect of a calendar variable is the average number of days in its group. Taking into account that one year has on average 365.25 days, the monthly mean effects for a working days are, as shown in the table below, 21.7411 for week days and 8.696 for weekends.\nMonthly mean effects for the Working day variable\n\n\n\nGroups of Working day effect\nMean effect\n\n\n\n\nWeek days\n365.25/12*5/7 = 21.7411\n\n\nWeekends\n365.25/12*2/7 = 8.696\n\n\nTotal\n365.25/12 = 30.4375\n\n\n\nThe number of days by period is highly seasonal, as apart from February, the length of each month is the same every year. For this reason, any set of calendar variables will contain, at least in some variables, a significant seasonal effect, which is defined as the average number of days by period (Januaries..., first quarters...) outside the mean effect. Removing that fixed seasonal effects consists of removing for each period the long term average of days that belong to it. The calculation of a seasonal effect for the working days classification is presented in the table below.\nThe mean effect and the seasonal effect for the calendar periods\n\n\n\n\n\n\n\n\n\n\nPeriod\nAverage number of days\nAverage number of week days\nMean effect\nSeasonal effect\n\n\n\n\nJanuary\n31\n31*5/7=22.1429\n21.7411\n0.4018\n\n\nFebruary\n28.25\n28.25*5/7=20.1786\n21.7411\n-1.5625\n\n\nMarch\n31\n31*5/7=22.1429\n21.7411\n0.4018\n\n\nApril\n30\n30*5/7=21.4286\n21.7411\n-0.3125\n\n\nMay\n31\n31*5/7=22.1429\n21.7411\n0.4018\n\n\nJune\n30\n30*5/7=21.4286\n21.7411\n-0.3125\n\n\nJuly\n31\n31*5/7=22.1429\n21.7411\n0.4018\n\n\nAugust\n31\n31*5/7=22.1429\n21.7411\n0.4018\n\n\nSeptember\n30\n30*5/7=21.4286\n21.7411\n-0.3125\n\n\nOctober\n31\n31*5/7=22.1429\n21.7411\n0.4018\n\n\nNovember\n30\n30*5/7=21.4286\n21.7411\n-0.3125\n\n\nDecember\n31\n31*5/7=22.1429\n21.7411\n0.4018\n\n\nTotal\n365.25\n260.8929\n260.8929\n0\n\n\n\nFor a given time span, the actual calendar effect for week days can be easily calculated as the difference between the number of week days in a specific period and the sum of the mean effect and the seasonal effect assigned to this period, as it is shown in the table below for the period 01.2013 – 06.2013.\nThe calendar effect for the period 01.2013 - 06.2013\n\n\n\n\n\n\n\n\n\n\nTime period (t)\nWeek days\nMean effect\nSeasonal effect\nCalendar effect\n\n\n\n\nJan-2013\n23\n21.7411\n0.4018\n0.8571\n\n\nFeb-2013\n20\n21.7411\n-1.5625\n-0.1786\n\n\nMar-2013\n21\n21.7411\n0.4018\n-1.1429\n\n\nApr-2013\n22\n21.7411\n-0.3125\n0.5714\n\n\nMay-2013\n23\n21.7411\n0.4018\n0.8571\n\n\nJun-2013\n20\n21.7411\n-0.3125\n-1.4286\n\n\nJul-2013\n23\n21.7411\n0.4018\n0.8571\n\n\n\nThe distinction between the mean effect and the seasonal effect is usually unnecessary. Those effects can be considered together (simply called mean effects) and be computed by removing from each calendar variable its average number of days by period. These global means effect are considered in the next section.\n\n\nImpact of the mean effects on the decomposition\nWhen the ARIMA model contains a seasonal difference – something that should always happen with calendar variables – the mean effects contained in the calendar variables are automatically eliminated, so that they don't modify the estimation. The model is indeed estimated on the series/regression variables after differencing. However, they lead to a different linearised series (\\(y_{\\text{lin}})\\). The impact of other corrections (mean and/or fixed seasonal) on the decomposition is presented in the next paragraph. Such corrections could be obtained, for instance, by applying other solutions for the long term corrections or by computing them on the time span of the series.\nNow the model with \"correct\" calendar effects (denoted as \\(C\\)), i.e. effects without mean and fixed seasonal effects, can be considered. To simplify the problem, the model has no other regression effects.\nFor such a model the following relations hold:\n\\[y_{\\text{lin}} = \\ y - C\\]\n\\[T = \\ F_{T}\\left( y_{\\text{lin}} \\right)\\]\n\\[S = \\ F_{S}\\left( y_{\\text{lin}} \\right) + C\\]\n\\[I = \\ F_{I}\\left( y_{\\text{lin}} \\right)\\]\nwhere:\nT - the trend;\nS - the seasonal component;\nI - the irregular component;\n\\(F_{X}\\) - the linear filter for the component X.\nConsider next other calendar effects (\\(\\widetilde{C}\\)) that contain some mean (\\(\\text{cm}\\), integrated to the final trend) and fixed seasonal effects (\\(\\text{cs}\\), integrated to the final seasonal). The modified equations are now:\n\\[\\widetilde{C} = C + cm + cs\\]\n\\[{\\widetilde{y}}_{\\text{lin}} = \\ y - \\widetilde{C} = \\ y_{\\text{lin}} - cm - cs\\]\n\\[\\widetilde{T} = \\ F_{T}\\left( {\\widetilde{y}}_{\\text{lin}} \\right) + cm\\]\n\\[\\widetilde{S} = \\ F_{S}\\left( {\\widetilde{y}}_{\\text{lin}} \\right) + C + cs\\]\n\\[\\widetilde{I} = \\ F_{I}\\left( {\\widetilde{y}}_{\\text{lin}} \\right)\\]\nTaking into account that \\(F_{X}\\) is a linear transformation and that1\n\\[F_{T}\\left( \\text{cm} \\right) = cm\\]\n\\[F_{T}\\left( \\text{cs} \\right) = 0\\]\n\\[F_{S}\\left( \\text{cm} \\right) = 0\\ \\]\n\\[F_{S}\\left( \\text{cs} \\right) = cs\\]\n\\[F_{I}\\left( \\text{cm} \\right) = 0\\]\n\\[F_{I}\\left( \\text{cs} \\right) = 0\\]\nThe following relationships hold:\n\\[\\widetilde{T} = \\ F_{T}\\left( {\\widetilde{y}}_{\\text{lin}} \\right) + cm = F_{T}\\left( y_{\\text{lin}} \\right) - cm + cm = T\\]\n\\[\\widetilde{S} = \\ F_{S}\\left( {\\widetilde{y}}_{\\text{lin}} \\right) + C + cs = F_{S}\\left( y_{\\text{lin}} \\right) - cs + C + cs = S\\]\n\\[\\widetilde{I} = \\ I\\]\nIf we don’t take into account the effects and apply the same approach as in the “correct” calendar effects, we will get:\n\\[\\breve{T} = \\ F_{T}\\left( {\\widetilde{y}}_{\\text{lin}} \\right) = T - cm\\]\n\\[\\breve{S} = \\ F_{S}\\left( {\\widetilde{y}}_{\\text{lin}} \\right) + \\widetilde{C} = S + cm\\]\n\\[\\breve{I} = \\ F_{I}\\left( {\\widetilde{y}}_{\\text{lin}} \\right) = I\\]\nThe trend, seasonal and seasonally adjusted series will only differ by a (usually small) constant.\nIn summary, the decomposition does not depend on the mean and fixed seasonal effects used for the calendar effects, provided that those effects are integrated in the corresponding final components. If these corrections are not taken into account, the main series of the decomposition will only differ by a constant.\n\n\nLinear transformations of the calendar variables\nAs far as the RegARIMA and the TRAMO models are considered, any non-degenerated linear transformation of the calendar variables can be used. It will produce the same results (likelihood, residuals, parameters, joint effect of the calendar variables, joint F-test on the coefficients of the calendar variables…). The linearised series that will be further decomposed is invariant to any linear transformation of the calendar variables.\nHowever, it should be mentioned that choices of calendar corrections based on the tests on the individual t statistics are dependent on the transformation, which is rather arbitrary. This is the case in old versions of TRAMO-SEATS. That is why the joint F-test (as in the version of TRAMO-SEATS implemented in TSW+) should be preferred.\nAn example of a linear transformation is the calculation of the contrast variables. In the case of the usual trading day variables, they are defined by the following transformation: the 6 contrast variables (\\(\\text{No.}\\left( \\text{Mondays} \\right) - No.\\left( \\text{Sundays} \\right),\\ldots No.\\left( \\text{Saturdays} \\right) - No.(Sundays)\\)) used with the length of period.\n\\[\\begin{bmatrix}                 \n  1 & 0 & 0 & 0 & 0 & 0 & - 1 \\\\    \n  0 & 1 & 0 & 0 & 0 & 0 & - 1 \\\\    \n  0 & 0 & 1 & 0 & 0 & 0 & - 1 \\\\    \n  0 & 0 & 0 & 1 & 0 & 0 & - 1 \\\\    \n  0 & 0 & 0 & 0 & 1 & 0 & - 1 \\\\    \n  0 & 0 & 0 & 0 & 0 & 1 & - 1 \\\\    \n  1 & 1 & 1 & 1 & 1 & 1 & 1 \\\\      \n  \\end{bmatrix}\\begin{bmatrix}      \n  \\text{Mon} \\\\                     \n  \\text{Tue} \\\\                     \n  \\text{Wed} \\\\                     \n  \\text{Thu} \\\\                     \n  \\text{Fri} \\\\                     \n  \\text{Sat} \\\\                     \n  \\text{Sun} \\\\                     \n  \\end{bmatrix} = \\begin{bmatrix}   \n  Mon - Sun \\\\                      \n  Tue - Sun \\\\                      \n  Wed - Sun \\\\                      \n  Thu - Sun \\\\                      \n  Fri - Sun \\\\                      \n  Sat - Sun \\\\                      \n  \\text{Length of period} \\\\      \n  \\end{bmatrix}\\]\nFor the usual working day variables, two variables are used: one contrast variable and the length of period\n\\[\\begin{bmatrix}                  \n  1 & - \\frac{5}{2} \\\\              \n  1 & 1 \\\\                          \n  \\end{bmatrix}\\begin{bmatrix}      \n  \\text{Week} \\\\                    \n  \\text{Weekend} \\\\                 \n  \\end{bmatrix} = \\begin{bmatrix}   \n  \\text{Contrast week} \\\\          \n  \\text{Length of period} \\\\      \n  \\end{bmatrix}\\]\nThe \\(\\text{Length of period}\\) variable is defined as a deviation from the length of the month (in days) and the average month length, which is equal to \\(30.4375.\\) Instead, the leap-year variable can be used here (see Regression sections in RegARIMA or Tramo)2.\nSuch transformations have several advantages. They suppress from the contrast variables the mean and the seasonal effects, which are concentrated in the last variable. So, they lead to fewer correlated variables, which are more appropriate to be included in the regression model. The sum of the effects of each day of the week estimated with the trading (working) day contrast variables cancel out.\n\n\nHandling of specific holidays\ncheck vs GUI (v3) and rjd3 modelling\nThree types of holidays are implemented in JDemetra+:\n\nFixed days, corresponding to the fixed dates in the year (e.g. New Year, Christmas).\nEaster related days, corresponding to the days that are defined in relation to Easter (e.g. Easter +/- n days; example: Ascension, Pentecost).\nFixed week days, corresponding to the fixed days in a given week of a given month (e.g. Labor Day celebrated in the USA on the first Monday of September).\n\nFrom a conceptual point of view, specific holidays are handled in exactly the same way as the other days. It should be decided, however, to which group of days they belong. Usually they are handled as Sundays. This convention is also used in JDemetra+. Therefore, except if the holiday falls on a Sunday, the appearance of a holiday leads to correction in two groups, i.e. in the group that contains the weekday, in which holiday falls, and the group that contains the Sundays.\nCountry specific holidays have an impact on the mean and the seasonal effects of calendar effects. Therefore, the appropriate corrections to the number of particular days (which are usually the basis for the definition of other calendar variables) should be applied, following the kind of holidays. These corrections are applied to the period(s) that may contain the holiday. The long term corrections in JDemetra+ don't take into account the fact that some moving holidays could fall on the same day (for instance the May Day and the Ascension). However, those events are exceptional, and their impact on the final result is usually not significant.\n\nFixed day\nThe probability that the holiday falls on a given day of the week is 1/7. Therefore, the probability to have 1 day more that is treated like Sunday is 6/7. The effect on the means for the period that contains the fixed day is presented in the table below (the correction on the calendar effect has the opposite sign).\nThe effect of the fixed holiday on the period, in which it occurred\n\n\n\nSundays\nOthers days\nContrast variables\n\n\n\n\n+ 6/7\n- 1/7\n1/7 - (+ 6/7)= -1\n\n\n\n\n\nEaster related days\nEaster related days always fall the same week day (denoted as Y in the table below: The effects of the Easter Sunday on the seasonal means). However, they can fall during different periods (months or quarters). Suppose that, taking into account the distribution of the dates for Easter and the fact that this holiday falls in one of two periods, the probability that Easter falls during the period \\(m\\) is \\(p\\), which implies that the probability that it falls in the period \\(m + 1\\) is \\(1 - p\\). The effects of Easter on the seasonal means are presented in the table below.\nThe effects of the Easter Sunday on the seasonal means\n|Period | Sundays Days X Others days Contrast Y Other contrasts |————| ————- ———— —————– ——————- ——————— |m |+ p - p 0 - 2p - p |m+1 | + (1-p) - (1-p) 0 - 2\\(\\times\\)(1-p) - (1-p)\nThe distribution of the dates for Easter may be approximated in different ways. One of the solutions consists of using some well-known algorithms for computing Easter on a very long period. JDemetra+ provides the Meeus/Jones/Butcher's and the Ron Mallen's algorithms (they are identical till year 4100, but they slightly differ after that date). Another approach consists in deriving a raw theoretical distribution based on the definition of Easter. It is the solution used for Easter related days. It is shortly explained below.\nThe date of Easter in the given year is the first Sunday after the full moon (the Paschal Full Moon) following the northern hemisphere's vernal equinox. The definition is influenced by the Christian tradition, according to which the equinox is reckoned to be on 21 March3 and the full moon is not necessarily the astronomically correct date. However, when the full moon falls on Sunday, then Easter is delayed by one week. With this definition, the date of Easter Sunday varies between 22 March and 25 April. Taking into account that an average lunar month is \\(29.530595\\) days the approximated distribution of Easter can be derived. These calculations do not take into account the actual ecclesiastical moon calendar.\nFor example, the probability that Easter Sunday falls on 25 March is 0.004838 and results from the facts that the probability that 25 March falls on a Sunday is \\(1/7\\) and the probability that the full moon is on 21 March, 22 March, 23 March or 24 March is \\(5/29.53059\\). The probability that Easter falls on 24 April is 0.01708 and results from the fact that the probability that 24 April is Sunday is \\(1/7\\) and takes into account that 18 April is the last acceptable date for the full moon. Therefore the probability that the full moon is on 16 April or 17 April is \\(1/29.53059\\) and the probability that the full moon is on 18 April is \\(1.53059/29.53059\\).\nThe approximated distribution of Easter dates\n\n\n\nDay\nProbability\n\n\n\n\n22 March\n1/7 * 1/29.53059\n\n\n23 March\n1/7 * 2/29.53059\n\n\n24 March\n1/7 * 3/29.53059\n\n\n25 March\n1/7 * 4/29.53059\n\n\n26 March\n1/7 * 5/29.53059\n\n\n27 March\n1/7 * 6/29.53059\n\n\n28 March\n1/29.53059\n\n\n29 March\n1/29.53059\n\n\n…\n…\n\n\n18 April\n1/29.53059\n\n\n19 April\n1/7 * (6 + 1.53059)/29.53059\n\n\n20 April\n1/7 * (5 + 1.53059)/29.53059\n\n\n21 April\n1/7 * (4 + 1.53059)/29.53059\n\n\n22 April\n1/7 * (3 + 1.53059)/29.53059\n\n\n23 April\n1/7 * (2 + 1.53059)/29.53059\n\n\n24 April\n1/7 * (1 + 1.53059)/29.53059\n\n\n25 April\n1/7 * 1.53059/29.53059\n\n\n\n\n\nFixed week days\nFixed week days always fall on the same week day (denoted as Y in the table below) and in the same period. Their effect on the seasonal means is presented in the table below.\nThe effect of the fixed week holiday on the period, in which it occurred\n\n\n\nSundays\nDay Y\nOthers days\n\n\n\n\n+ 1\n- 1\n0\n\n\n\nThe impact of fixed week days on the regression variables is zero because the effect itself is compensated by the correction for the mean effect.\n\n\n\nHolidays with a validity period\nWhen a holiday is valid only for a given time span, JDemetra+ applies the long term mean corrections only on the corresponding period. However, those corrections are computed in the same way as in the general case.\nIt is important to note that using or not using mean corrections will impact in the estimation of the RegARIMA and TRAMO models. Indeed, the mean corrections do not disappear after differencing. The differences between the SA series computed with or without mean corrections will no longer be constant.\n\n\nDifferent Kinds of calendars\nsee link with GUI\nThis scenario presents how to define different kinds of calendars. These calendars can be applied to the specifications that take into account country-specific holidays and can be used for detecting and estimating the calendar effects.\nThe calendar effects are those parts of the movements in the time series that are caused by different number of weekdays in calendar months (or quarters). They arise as the number of occurrences of each day of the week in a month (or a quarter) differs from year to year. These differences cause regular effects in some series. In particular, such variation is caused by a leap year effect because of an extra day inserted into February every four years. As with seasonal effects, it is desirable to estimate and remove calendar effects from the time series.\nThe calendar effects can be divided into a mean effect, a seasonal part and a structural part. The mean effect is independent from the period and therefore should be allocated to the trend-cycle. The seasonal part arises from the properties of the calendar that recur each year. For one thing, the number of working days of months with 31 calendar days is on average larger than that of months with 30 calendar days. This effect is part of the seasonal pattern captured by the seasonal component (with the exception of leap year effects). The structural part of the calendar effect remains to be determined by the calendar adjustment. For example, the number of working days of the same month in different years varies from year to year.\nBoth X-12-ARIMA/X-13ARIMA-SEATS and TRAMO/SEATS estimate calendar effects by adding some regressors to the equation estimated in the pre-processing part (RegARIMA or TRAMO, respectively). Regressors mentioned above are generated from the default calendar or the user defined calendar.\nThe calendars of JDemetra+ simply correspond to the usual trading days contrast variables based on the Gregorian calendar, modified to take into account some specific holidays. Those holidays are handled as \"Sundays\" and the variables are properly adjusted to take into account the long term mean effects.\n\n\nTests for residual trading days\nWe consider below tests on the seasonally adjusted series (\\(sa_t\\)) or on the irregular component (\\(irr_t\\)). When the reasoning applies on both components, we will use \\(y_t\\). The functions \\(stdev\\) stands for “standard deviation” and \\(rms\\) for “root mean squares”\nThe tests are computed on the log-transformed components in the case of multiplicative decomposition.\nTD are the usual contrasts of trading days, 6 variables (no specific calendar).\n\nNon significant irregular\nWhen \\(irr_t\\) is not significant, we don’t compute the test on it, to avoid irrelevant results. We consider that \\(irr_t\\) is significant if \\(stdev( irr_t)>0.01\\) (multiplicative case) or if \\(stdev(irr_t)/rms(sa_t) >0.01\\) (additive case).\n\n\nF test\nThe test is the usual joint F-test on the TD coefficients, computed on the following models:\n\nAutoregressive model (AR modelling option)\nWe compute by OLS:\n\\[y_t=\\mu + \\alpha y_{t-1} + \\beta TD_t + \\epsilon_t \\]\n\n\nDifference model\nWe compute by OLS:\n\\[\\Delta y_t - \\overline{\\Delta y_t}=\\beta TD_t + \\epsilon_t \\]\nSo, the latter model is a restriction of the first one (\\(\\alpha =1, \\mu =μ=\\overline{\\Delta y_t}\\))\nThe tests are the usual joint F-tests on \\(\\beta \\quad (H_0:\\beta=0)\\).\nBy default, we compute the tests on the 8 last years of the components, so that they might highlight moving calendar effects.\nRemark:\nIn Tramo, a similar test is computed on the residuals of the Arima model. More exactly, the F-test is computed on \\(e_t=\\beta TD_t + \\epsilon_t\\), where \\(e_t\\) are the one-step-ahead forecast errors.\n\n\n\n\n\n\n\n\nIn case of SEATS the properties can be trivially derived from the matrix formulation of signal extraction. They are also valid for X-11 (additive).↩︎\nGÓMEZ, V., and MARAVALL, A (2001b).↩︎\nIn fact, astronomical observations show that the equinox occurs on 20 March in most years.↩︎"
  },
  {
    "objectID": "A-benchmarking.html",
    "href": "A-benchmarking.html",
    "title": "Algorithms for benchmarking and temporal disagreggation",
    "section": "",
    "text": "In this chapter we describe the practical implementation, the underlying theory in a dedicated chapter.(link)"
  },
  {
    "objectID": "A-benchmarking.html#benchmarking-overview",
    "href": "A-benchmarking.html#benchmarking-overview",
    "title": "Algorithms for benchmarking and temporal disagreggation",
    "section": "Benchmarking overview",
    "text": "Benchmarking overview\nOften one has two (or multiple) datasets of different frequency for the same target variable. Sometimes, however, these data sets are not coherent in the sense that they don’t match up. Benchmarking[^1] is a method to deal with this situation. An aggregate of a higher-frequency measurement variables is not necessarily equal to the corresponding lower-frequency less-aggregated measurement. Moreover, the sources of data may have different reliability levels. Usually, less frequent data are considered more trustworthy as they are based on larger samples and compiled more precisely. The more reliable measurements, hence often the less frequent, will serve as benchmark.\nIn seasonal adjustment methods benchmarking is the procedure that ensures the consistency over the year between adjusted and non-seasonally adjusted data. It should be noted that the [ESS Guidelines on Seasonal Adjustment (2015)] (https://ec.europa.eu/eurostat/documents/3859598/6830795/KS-GQ-15-001-EN-N.pdf/d8f1e5f5-251b-4a69-93e3-079031b74bd3), do not recommend benchmarking as it introduces a bias in the seasonally adjusted data. The U.S. Census Bureau also points out that “forcing the seasonal adjustment totals to be the same as the original series annual totals can degrade the quality of the seasonal adjustment, especially when the seasonal pattern is undergoing change. It is not natural if trading day adjustment is performed because the aggregate trading day effect over a year is variable and moderately different from zero”[^2]. Nevertheless, some users may need that the annual totals of the seasonally adjusted series match the annual totals of the original, non-seasonally adjusted series[^3].\nAccording to the [ESS Guidelines on Seasonal Adjustment (2015)] (https://ec.europa.eu/eurostat/documents/3859598/6830795/KS-GQ-15-001-EN-N.pdf/d8f1e5f5-251b-4a69-93e3-079031b74bd3), the only benefit of this approach is that there is consistency over the year between adjusted and the non-seasonally adjusted data; this can be of particular interest when low-frequency (e.g. annual) benchmarking figures officially exist (e.g. National Accounts, Balance of Payments, External Trade, etc.) and where users’ needs for time consistency are stronger."
  },
  {
    "objectID": "A-benchmarking.html#tools",
    "href": "A-benchmarking.html#tools",
    "title": "Algorithms for benchmarking and temporal disagreggation",
    "section": "Tools",
    "text": "Tools\n\nBenchmarking with GUI\n\nWith the pre-defined specifcations the benchmarking functionality is not applied by default following the ESS Guidelines on Seasonal Adjustment (2015) recommendations. It means that once the user has seasonally adjustd the series with a pre-defined specifcation the Benchmarking node is empty. To execute benchmarking click on the Specifications button and activate the checkbox in the Benchmarking section.\n\n\n\nText\n\n\nBenchmarking option – a default view\nThree parameters can be set here. Target specifies the target variable for the benchmarking procedure. It can be either the Original (the raw time series) or the Calendar Adjusted (the time series adjusted for calendar effects). Rho is a value of the AR(1) parameter (set between 0 and 1). By default it is set to 1. Finally, Lambda is a parameter that relates to the weights in the regression equation. It is typically equal to 0 (for an additive decomposition), 0.5 (for a proportional decomposition) or 1 (for a multiplicative decomposition). The default value is 1.\nTo launch the benchmarking procedure click on the Apply button. The results are displayed in four panels. The top-left one compares the original output from the seasonal adjustment procedure with the result from applying a benchmarking to the seasonal adjustment. The bottom-left panel highlights the differences between these two results. The outcomes are also presented in a table in the top-right panel. The relevant statistics concerning relative differences are presented in the bottom-right panel.\n\n\n\nText\n\n\nThe results of the benchmarking procedure\nBoth pictures and the table can be copied the usual way (see the Simple seasonal adjustment of a single time series scenario).\n\n\n\nText\n\n\nOptions for benchmarking results\nTo export the result of the benchmarking procedure (benchmarking.result) and the target data (benchmarking.target) one needs to once execute the seasonal adjustment with benchmarking using the muli-processing option (see the Simple seasonal adjustment of multiple time series scenario. Once the muli-processing is executed, select the Output item from the SAProcessing menu.\n\n\n\nText\n\n\nThe SAProcessing menu\nExpand the \"+\" menu and choose an appropriate data format (here Excel has been chosen). It is possible to save the results in TXT, XLS, CSV, and CSV matrix formats. Note that the available content of the output depends on the output type.\n\n\n\nText\n\n\nExporting data to an Excel file\nChose the output items that refer to the results from the benchmarking procedure, move them to the window on the right and click OK.\n\n\n\nText\n\n\nExporting the results of the benchmarking procedure\n\n\n\nBenchmarking in R\npackage rjd3bench orga doc - here - in package - example"
  },
  {
    "objectID": "A-benchmarking.html#references",
    "href": "A-benchmarking.html#references",
    "title": "Algorithms for benchmarking and temporal disagreggation",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "A-trend-cycle-estimation.html#estimation-methods",
    "href": "A-trend-cycle-estimation.html#estimation-methods",
    "title": "Trend-cycle estimation",
    "section": "Estimation Methods",
    "text": "Estimation Methods"
  },
  {
    "objectID": "A-trend-cycle-estimation.html#tools",
    "href": "A-trend-cycle-estimation.html#tools",
    "title": "Trend-cycle estimation",
    "section": "Tools",
    "text": "Tools\n\nrjd3 highfeq package\n\n\nrjdfilters package"
  },
  {
    "objectID": "A-nowcasting.html",
    "href": "A-nowcasting.html",
    "title": "Nowcasting",
    "section": "",
    "text": "Underlying Theory: references ?"
  },
  {
    "objectID": "A-nowcasting.html#tools",
    "href": "A-nowcasting.html#tools",
    "title": "Nowcasting",
    "section": "Tools",
    "text": "Tools\n\nplug in ?\nR package ?"
  },
  {
    "objectID": "T-r-packages.html",
    "href": "T-r-packages.html",
    "title": "R packages",
    "section": "",
    "text": "table"
  },
  {
    "objectID": "T-r-packages.html#organisation-overview",
    "href": "T-r-packages.html#organisation-overview",
    "title": "R packages",
    "section": "Organisation overview",
    "text": "Organisation overview\na suite (order)\ngeneral output organisation"
  },
  {
    "objectID": "T-r-packages.html#installation-procedure",
    "href": "T-r-packages.html#installation-procedure",
    "title": "R packages",
    "section": "Installation procedure",
    "text": "Installation procedure"
  },
  {
    "objectID": "T-r-packages.html#interaction-with-gui",
    "href": "T-r-packages.html#interaction-with-gui",
    "title": "R packages",
    "section": "Interaction with GUI",
    "text": "Interaction with GUI"
  },
  {
    "objectID": "T-r-packages.html#full-list",
    "href": "T-r-packages.html#full-list",
    "title": "R packages",
    "section": "Full list",
    "text": "Full list\n\nrjd3modelling\nmain functions: table"
  },
  {
    "objectID": "T-plug-ins.html",
    "href": "T-plug-ins.html",
    "title": "Plug-ins for JDemetra+",
    "section": "",
    "text": "table"
  },
  {
    "objectID": "M-spectral-analysis.html",
    "href": "M-spectral-analysis.html",
    "title": "Spectral Analysis Principles and Tools",
    "section": "",
    "text": "A time series \\(x_{t}\\) with stationary covariance, mean \\(μ\\) and \\(k^{th}\\) autocovariance \\(E(x_{t}-\\mu)(x_{t- k}\\mu))=\\gamma(k)\\) can be described as a weighted sum of periodic trigonometric functions: \\(sin(\\omega t)\\) and \\(cos(\\omega t)\\), where \\(\\omega=\\frac{2*pi}{T}\\) denotes frequency. Spectral analysis investigates this frequency domain representation of \\(x_{t}\\) to determine how important cycles of different frequencies are in accounting for the behavior of \\(x_{t}\\).\nAssuming that the autocovariances \\(\\gamma(k)\\) are absolutely summable(\\(\\sum_{k =-\\infty}^{\\infty}|\\gamma(k)|<\\infty\\)), the autocovariance generating function, which summarizes these autocovariances through a scalar valued function, is given by equation [1]1.\n\\(acgf(z)=\\sum_{k=-\\infty}^{\\infty}{z^{k}\\gamma(k)}\\),\nwhere \\(z\\) denotes complex scalar.\nOnce the equation [1]is divided by \\(\\pi\\) and evaluated at some \\(z{= e}^{- i\\omega} = cos\\omega - isin\\omega\\), where \\(i = \\sqrt{- 1}\\) and \\(\\omega\\) is a real scalar, \\(\\  - \\infty < \\ \\omega < \\infty\\), the result of this transformation is called a population spectrum \\(f(\\omega)\\)for \\(\\ x_{t}\\), given in equation [2]2.\n\\[\nf(\\omega) = \\frac{1}{\\pi}\\sum_{k=- \\infty}^{\\infty}{e^{- ik\\omega}\\gamma(k)}\n\\]\nTherefore, the analysis of the population spectrum in the frequency domain is equivalent to the examination of the autocovariance function in the time domain analysis; however it provides an alternative way of inspecting the process. Because \\(f(\\omega)\\text{dω}\\) is interpreted as a contribution to the variance of components with frequencies in the range \\((\\omega,\\ \\omega + d\\omega)\\), a peak in the spectrum indicates an important contribution to the variance at frequencies near the value that corresponds to this peak.\nAs \\(e\\^{- i\\omega} = cos\\omega - isin\\omega\\), the spectrum can be also expressed as in equation [3].\n\\[\nf(\\omega) = \\frac{1}{\\pi}\\sum_{k = - \\infty}^{\\infty}{(cos\\omega k - isin\\omega k)\\gamma(k)}\n\\]\nSince \\(\\gamma(k) = \\gamma( - k)\\) (i.e. \\(\\gamma(k)\\) is an even function of \\(k\\)) and \\(\\sin{( - x)}\\  = \\operatorname{-sin}x\\), [3] can be presented as equation\n\\[\nf(\\omega) = \\frac{1}{\\pi}\\lbrack \\ \\gamma(0) + 2\\sum_{k = 1}^{\\infty}{\\ \\gamma(k)}cos\\text{ωk} \\rbrack\n\\],\nThis implies that if autocovariances are absolutely summable the population spectrum exists and is a continuous, real-valued function of \\(\\omega\\). Due to the properties of trigonometric functions \\((\\cos( - \\omega k) = \\cos(\\text{ωk})\\) and \\(\\cos(\\omega + 2\\pi j)k = cos(\\omega k))\\) the spectrum is a periodic, even function of \\(\\omega\\), symmetric around \\(\\omega = 0\\). Therefore, the analysis of the spectrum can be reduced to the interval \\(( - \\pi,\\pi)\\). The spectrum is non-negative for all \\(\\omega \\in ( - \\pi,\\pi)\\).\nThe shortest cycle that can be distinguished in a time series lasts two periods. The frequency which corresponds to this cycle is \\(\\omega = \\pi\\) and is called the Nyquist frequency. The frequency of the longest cycles that can be observed in the time series with \\(n\\) observations is \\(\\omega = \\frac{2\\pi}{n}\\) and is called the fundamental (Fourier) frequency.\nNote that if \\(x_{t}\\) is a white noise process with zero mean and variance \\(\\sigma^{2}\\), then for all \\(|k|> 0\\) \\(\\gamma(k)=0\\) and the spectrum of \\(x_{t}\\) is constant (\\(f(\\omega)= \\frac{\\sigma^{2}}{\\pi}\\)) since each frequency in the spectrum contributes equally to the variance of the process3.\nThe aim of spectral analysis is to determine how important cycles of different frequencies are in accounting for the behaviour of a time series4. Since spectral analysis can be used to detect the presence of periodic components, it is a natural diagnostic tool for detecting trading day effects as well as seasonal effects5. Among the tools used for spectral analysis are the autoregressive spectrum and the periodogram.\nThe explanations given in the subsections of this node derive mainly from DE ANTONIO, D., and PALATE, J. (2015) and BROCKWELL, P.J., and DAVIS, R.A. (2006).\ncomment1: end old intro: ok"
  },
  {
    "objectID": "M-spectral-analysis.html#spectral-density-estimation",
    "href": "M-spectral-analysis.html#spectral-density-estimation",
    "title": "Spectral Analysis Principles and Tools",
    "section": "Spectral density estimation",
    "text": "Spectral density estimation\n\nMethod 1: The periodogram\nFor any given frequency \\(\\omega\\) the sample periodogram is the sample analog of the sample spectrum. In general, the periodogram is used to identify the periodic components of unknown frequency in the time series. X-13ARIMA-SEATS and TRAMO-SEATS use this tool for detecting seasonality in raw time series and seasonally adjusted series. Apart from this it is applied for checking randomness of the residuals from the ARIMA model.\nTo define the periodogram, first consider the vector of complex numbers6:\n\\[\n\\mathbf{x} = \\begin{bmatrix}\n  x_{1} \\\\\n  x_{2} \\\\\n  . \\\\\n  . \\\\\n  . \\\\\n  x_{n} \\\\\n  \\end{bmatrix} \\in \\mathbb{C}^{n}\n\\]\nwhere \\(\\mathbb{C}^{n}\\) is the set of all column vectors with complex-valued components.\nThe Fourier frequencies associated with the sample size \\(n\\) are defined as a set of values \\(ω_{j} = \\frac{2\\pi j}{n}\\), \\(j = - \\lbrack \\frac{n-1}{2}\\rbrack,\\ldots,\\lbrack\\frac{n}{2}\\rbrack\\), \\(-\\pi< \\omega_{j} \\leq \\pi\\), \\(j\\in F_{n}\\), where \\({\\lbrack n\\rbrack}\\) denotes the largest integer less than or equal to \\(n\\). The Fourier frequencies, which are called harmonics, are given by integer multiples of the fundamental frequency \\(\\ \\frac{2\\pi}{n}\\).\nNow the \\(n\\) vectors \\(e_{j} = n^{- \\frac{1}{2}}(e^{-i\\omega_{j}},e^{-i{2\\omega}_{j}},\\ldots,e^{- inω_{j}})^{'}\\) can be defined. Vectors \\(e_{1},\\ldots, e_{n}\\) are orthonormal in the sense that:\n\\[\n{\\mathbf{e}_{j}^{*}\\mathbf{e}}_{k} = n^{- 1}\\sum_{r = 1}^{n}e^{ir(\\omega_{j} - \\omega_{k})} = { \\begin{matrix}\n  1,\\ if\\ j = k \\\\\n  0,\\ if\\ j \\neq k \\\\\n  \\end{matrix}}\n\\]\nwhere \\(\\mathbf{e}_{j}^{*}\\) denotes the row vector, which \\(k^{th}\\) component is the complex conjugate of the \\(k^{th}\\) component of \\(\\mathbf{e}_{j}\\).7 These vectors are a basis of \\(F_{n}\\), so that any \\(\\mathbf{x}\\in\\mathbb{C}^{n}\\) can be expressed as a sum of \\(n\\) components:\n\\[\n\\mathbf{x} = \\sum_{j = - \\lbrack\\frac{n - 1}{2}\\rbrack}^{\\lbrack\\frac{n}{2}\\rbrack}{a_{j}\\mathbf{e}_{j}}\n\\]\nwhere the coefficients \\(a_{j} = \\mathbf{e}_{j}^{*}\\mathbf{x}=n^{-\\frac{1}{2}}\\sum_{t = 1}^{n}x_{t}e^{-it\\omega_{j}}\\) are derived from [3] by multiplying the equation on the left by \\(\\mathbf{e}_{j}^{*}\\) and using [1].\nThe sequence of \\(\\{a_{j},j\\in F_{n}\\}\\) is referred as a discrete Fourier transform of \\(\\mathbf{x}\\mathbb{\\in C}^{n}\\) and the periodogram \\(I(\\omega_{j})\\) of \\(\\mathbf{x}\\) at Fourier frequency \\(\\omega_{j} = \\frac{2\\pi j}{n}\\) is defined as the square of the Fourier transform \\(\\{a_{j}\\}\\) of \\(\\mathbf{x}\\):\n\\[\n{I(\\omega_{j})\\mathbf{=}{| a_{j} |^{2}}_{\\ } = n^{- \\ 1}| \\sum_{t = 1}^{n}x_{t}e^{- it\\omega_{j}} |^{2}}_{\\mathbf{\\ }}\n\\]\nFrom [2] and [3] it can be shown that in fact the periodogram decomposes the total sum of squares \\(\\sum_{t = 1}^{n}| x_{t} |^{2}\\) into a sums of components associated with the Fourier frequencies \\[ω_{j}\\]:\n\\[\n  \\sum_{t=1}^{n}{|x_{t}|}^{2} = \\sum_{j = - \\lbrack\\frac{n - 1}{2}\\rbrack}^{\\lbrack\\frac{n}{2}\\rbrack}|a_{j}|^{2} = \\sum_{j = - \\lbrack\\frac{n - 1}{2}\\rbrack}^{\\lbrack\\frac{n}{2}\\rbrack}{I(\\omega_{j})}\n\\]\nIf \\(\\ \\mathbf{x\\  \\in}\\ {R}^{n}\\), \\(\\omega_{j}\\) and \\({-\\omega}_{j}\\) are both in \\(\\lbrack- \\pi, -\\pi \\rbrack\\) and \\(a_{j}\\) is presented in its polar form (i.e. \\(a_{j} = r_{j}\\exp( i\\theta_{j})\\)), where \\(r_{j}\\) is the modulus of \\(a_{j}\\), then [3] can be rewritten in the form:\n\\[\n\\mathbf{x} = a_{0}\\mathbf{e}_{0} + \\sum_{j = 1}^{\\lbrack\\frac{n - 1}{2}\\rbrack}{ {2^{1/2}r}_{j}{(\\mathbf{c}}_{j}\\cos\\theta_{j}{- \\mathbf{s}}_{j}\\sin\\theta_{j}) + a_{n/2}\\mathbf{e}_{n/2}}\n\\]\nThe orthonormal basis for \\({R}^{n}\\) is \\(\\{\\mathbf{e}_{0},\\mathbf{c}_{1},\\mathbf{s}_{1},\\ldots,\\mathbf{c}_{\\lbrack\\frac{n - 1}{2}\\rbrack},\\mathbf{s}_{\\lbrack\\frac{n - 1}{2}\\rbrack},\\mathbf{e}_{\\frac{n}{2}(excluded\\ if\\ n\\ is\\ odd)}\\}\\), where:\n\\(\\mathbf{e}_{0}\\) is a vector composed of n elements equal to \\(n^{- 1/2}\\), which implies that \\(\\mathbf{a}_{0}\\mathbf{e}_{0} = {(n^{-1}\\sum_{t = 1}^{n}x_{t},\\ldots,n^{- 1}\\sum_{t=1}^{n}x_{t})}^{'}\\);\n\\[\n\\mathbf{c}_{j}=(\\frac{n}{2})^{- 1/2}{(\\cos\\omega_{j},\\cos{2\\omega}_{j},\\ldots,\\cos{n\\omega_{j}})}^{'}, for 1 \\leq j \\leq \\lbrack \\frac{(n - 1)}{2}\\rbrack\n\\] ;\n\\[\n\\mathbf{s}_{j} = {(\\frac{n}{2})}^{-1/2}{(\\sin{\\omega_{j}},\\sin{2\\omega_{j}},\\ldots,\\sin{n\\omega_{j}})}^{'},\\ for\\ 1 \\leq j \\leq \\lbrack \\frac{(n - 1)}{2} \\rbrack\n\\];\n\\[\n\\mathbf{e}_{n/2} = {(- (n^{-\\frac{1}{2}}),n^{- \\frac{1}{2}},\\ldots,{-(n)}^{- \\frac{1}{2}}),n^{-\\frac{1}{2}})}^{'}\n\\].\nEquation [5] can be seen as an OLS regression of \\(x_{t}\\) on a constant and the trigonometric terms. As the vector of explanatory variables includes \\(n\\) elements, the number of explanatory variables in [5] is equal to the number of observations. HAMILTON, J.D. (1994) shows that the explanatory variables are linearly independent, which implies that an OLS regression yields a perfect fit (i.e. without an error term). The coefficients have the form of a simple OLS projection of the data on the orthonormal basis:\n\\[\n  {\\widehat{a}}_{0}=\\frac{1}{\\sqrt{n}}\\sum_{t=1}^{n}x_{t}\n\\] [7] \n\\[\n  {\\widehat{a}}_{n/2}=\\frac{1}{\\sqrt{n}}\\sum_{t=1}^{n}{(-1)}^{t}x_{t}(   \\text{only when n is even})\n\\] [8] \n\\[\n  {\\widehat{a}}_{0}=\\frac{1}{\\sqrt{n}}\\sum_{t=1}^{n}x_{t}\n\\] [9] \n\\[\n  {\\widehat{\\alpha}}_{j} = 2^{1/2}r_{j}\\cos{\\theta_{j}} = {(\\frac{n}{2})}^{- 1/2}\\sum_{t = 1}^{n}x_{t}\\cos{(t\\frac{2\\pi j}{n})}, j   = 1,\\ldots,\\lbrack\\frac{n - 1}{2}\\rbrack\n\\] [10] \n\\[\n  {\\widehat{\\beta}}_{j} = 2^{1/2}r_{j}\\sin{\\theta_{j}} = {(\\frac{n}{2})}^{-1/2}\\sum_{t = 1}^{n}x_{t}\\sin{(t\\frac{2\\pi j}{n})}, j = 1,\\ldots,\\lbrack\\frac{n - 1}{2}\\rbrack\n\\] [11] \nWith [5] the total sum of squares \\(\\sum_{t = 1}^{n}| x_{t} |^{2}\\) can be decomposed into \\(2 \\times \\lbrack\\frac{n - 1}{2}\\rbrack\\) components corresponding to \\(\\mathbf{c}_{j}\\) and \\(\\mathbf{s}_{j}\\), which are grouped to produce the “frequency \\(ω_{j}\\)” component for \\(1 \\geq j \\geq \\lbrack\\frac{n - 1}{2}\\rbrack\\). As it is shown in the table below, the value of the periodogram at the frequency \\(\\omega_{j}\\) is the contribution of the \\(j\\^{\\text{th}}\\) harmonic to the total sum of squares \\(\\sum_{t = 1}^{n}| x_{t} |^{2}\\).\nDecomposition of sum of squares into components corresponding to the harmonics\n{: .table .table-style} |Frequency |Degrees of freedom |Sum of squares decomposition| |———————————————– |———————— |————————————————————-| |\\(\\omega_{0}\\)(mean) |1 |\\({a_{0}^{2}}_{\\ }=n^{- 1}(\\sum_{t=1}^{n}x_{t})^{2} = I( 0)\\)| |\\(\\omega_{1}\\) |2 |\\({2r_{1}^{2}}_{\\ } = 2{\\|a_{1}\\|}^{2} = 2I(\\omega_{1})\\)| |\\(\\vdots\\) |\\(\\vdots\\) |\\(\\vdots\\)| |\\(\\omega_{k}\\) |2 |\\({2r_{k}^{2}}_{\\ } = 2{\\|a_{k}\\|}^{2} = 2I(\\omega_{k})\\)| |\\[\\vdots\\] |\\(\\vdots\\) |\\(\\vdots\\)| |\\(\\omega_{n/2} = \\pi\\) (excluded if \\(n\\) is odd) |1 |\\(a_{n/2}^{2} = I(\\pi)\\)| |Total |\\(\\mathbf{n}\\) |\\(\\sum_{\\mathbf{t = 1}}^{\\mathbf{n}}\\mathbf{x}_{\\mathbf{t}}^{\\mathbf{2}}\\)|\nSource: DE ANTONIO, D., and PALATE, J. (2015).\nObviously, if series were random then each component \\(I(\\omega\\_{j})\\) would have the same expectation. On the contrary, when the series contains a systematic sine component having a frequency \\(j\\) and amplitude \\(A\\) then the sum of squares \\(I(\\omega_{j})\\) increases with \\(A\\). In practice, it is unlikely that the frequency \\(j\\) of an unknown systematic sine component would exacly match any of the frequencies, for which peridogram have been calcuated. Therefore, the periodogram would show an increase in intensities in the immediate vicinity of \\(j\\).8\nNote that in JDemetra+ the periodogram object corresponds exactly to the contribution to the sum of squares of the standardised data, since the series are divided by their standard deviation for computational reasons.\nUsing the decomposition presented in table above the periodogram can be expressed as:\n\\[\nI(\\omega_{j})\\mathbf{=}\\begin{matrix}                                                                                r_{j}^{2} = \\frac{1}{2}{(\\alpha}_{j}^{2} + \\beta_{j}^{2}) = \\ {\\frac{1}{n}(\\sum_{t = 1}^{n}{x_{t}\\cos{( {t\\frac{2\\pi j}{n}}_{\\ })\\ }})}^{2} + \\frac{1}{n}(\\sum_{t = 1}^{n}{x_{t}\\sin( t\\frac{2\\pi j}{n})_{\\ }})^{2} \\\\\n\\end{matrix}\n\\] [12] \nwhere \\(j = 0,\\ldots,lbrack \\frac{n}{2} rbrack\\).\nSince \\(\\mathbf{x} - \\overline{\\mathbf{x}}\\) are generated by an orthonormal basis, and \\(\\overline{\\mathbf{x}}\\mathbf{=}a_{0}\\mathbf{e}_{0}\\) [5] can be rearranged to show that the sum of squares is equal to the sum of the squared coefficients:\n\\[\n\\mathbf{x} - a_{0}\\mathbf{e}_{0} =\\sum_{j=1}^{\\lbrack(n - 1)/2\\rbrack}(\\alpha_{j}\\mathbf{c}_{j}+\\beta_{j}\\mathbf{s}_{j}) + a_{n/2}\\mathbf{e}_{n/2}\n\\]. [13] \nThus the sample variance of \\[x_{t}\\] can be expressed as:\n\\[\nn^{- 1}\\sum_{t=1}^{n}{(x_{t}-\\overline{x})}^{2}=n^{-1}(\\sum_{k=1}^{\\lbrack(n - 1)/2\\rbrack}2{r_{j}}^{2}\n+{a_{n/2}}^{2})\n\\], [14] \nwhere \\(a_{n/2}^{2}\\) is excluded if \\(n\\) is odd.\nThe term \\(2{r_{j}}^{2}\\) in [14] is then the contribution of the \\(j^{\\text{th}}\\) harmonic to the variance and [14] shows then how the total variance is partitioned.\nThe periodogram ordinate \\(I(\\omega_{j})\\) and the autocovariance coefficient \\(\\gamma(k)\\) are both quadratic forms of \\(x_{t}\\). It can be shown that the periodogram and autocovarinace function are related and the periodogram can be written in terms of the sample autocovariance function for any non-zero Fourier frequency \\(ω_{j}\\) :9\n\\[\nI(\\omega_{j}) = \\sum_{| k | < n}^{\\ }{\\widehat{\\gamma}( k)}_{\\ }e^{- ik\\omega_{j}} = {\\widehat{\\gamma}( 0)}_{\\ } + 2\\sum_{k = 1}^{n - 1}{\\widehat{\\gamma}( k)\\cos{(k\\omega_{j})}}_{\\ }\n\\]\nand for the zero frequency \\(\\ I( 0) = n| \\overline{x} |^{2}\\).\nOnce comparing [15] with an expression for the spectral density of a stationary process:\n\\[\nf(\\omega_{\\ }) = \\frac{1}{2\\pi}\\sum_{k < - \\infty}^{\\infty}{\\gamma( k)}_{\\ }e^{- ik\\omega_{\\ }} = \\frac{1}{2\\pi}lbrack {\\gamma( 0)}_{\\ } + 2(\\sum_{k = 1}^{\\infty}{\\gamma( k)\\cos{(k\\omega_{\\ })}}) rbrack\n\\]\nIt can be noticed that the periodogram is a sample analog of the population spectrum. In fact, it can be shown that the periodogram is asymptotically unbiased but inconsistent estimator of the population spectrum \\(f(\\omega)\\).[^75] Therefore, the periodogram is a wildly fluctuating, with high variance, estimate of the spectrum. However, the consistent estimator can be achieved by applying the different linear smoothing filters to the periodogram, called lag-window estimators. The lag-window estimators implemented in JDemetra+ includes square, Welch, Tukey, Barlett, Hanning and Parzen. They are described in DE ANTONIO, D., and PALATE, J. (2015). Alternatively, the model-based consistent estimation procedure, resulting in autoregressive spectrum estimator, can be applied.\ncomment2: end part theory>spectral analysis>periodogram\n\n\nMethod 2: Autoregressive spectrum estimation\nBROCKWELL, P.J., and DAVIS, R.A. (2006) point out that for any real-valued stationary process \\((x_{t})\\) with continuous spectral density \\(f(\\omega)\\) it is possible to find both \\(AR(p)\\) and \\(MA(q)\\) processes which spectral densities are arbitrarily close to \\(f(\\omega)\\). For this reason, in some sense, \\((x_{t})\\) can be approximated by either \\(AR(p)\\) or \\(MA(q)\\) process. This fact is a basis of one of the methods of achieving a consistent estimator of the spectrum, which is called an autoregressive spectrum estimation. It is based on the approximation of the stochastic process \\((x_{t})\\) by an autoregressive process of sufficiently high order \\(p\\):\n\\[\nx_{t} = \\mu + (\\phi_{1}B + \\ldots + \\phi_{p}B^{p})x_{t} + \\varepsilon_{t}\n\\]\nwhere \\(\\varepsilon_{t}\\) is a white-noise variable with mean zero and a constant variance.\nThe autoregressive spectrum estimator for the series \\(x_{t}\\) is defined as: 10\n\\[\n\\widehat{s}(\\omega) = 10\\operatorname{\\times}{\\log_{10}\\frac{\\sigma_{x}^{2}}{2\\pi{|1 - \\sum_{k = 1}^{p}{\\widehat{\\phi}}_{k}e^{- ik\\omega}|}^{2}}}\n\\]\nwhere:\n\\(\\omega\\)– frequency, \\(0 \\leq \\omega \\leq \\pi\\);\n\\(\\sigma_{x}^{2}\\) – the innovation variance of the sample residuals;\n\\({\\widehat{\\phi}}_{k}\\) – \\(\\text{AR}(k)\\) coefficient estimates of the linear regression of \\(x_{t} - \\overline{x}\\) on \\(x_{t - k} - \\overline{x}\\), \\(1 \\leq k \\leq p\\).\nThe autoregressive spectrum estimator is used in the visual spectral analysis tool for detecting significant peaks in the spectrum. The criterion of visual significance, implemented in JDemetra+, is based on the range \\({\\widehat{s}}^{\\max} - {\\widehat{s}}^{\\min}\\) of the \\(\\widehat{s}(\\omega)\\) values, where \\({\\widehat{s}}^{\\max} = \\max_{k}\\widehat{s}(\\omega_{k})\\); \\({\\widehat{s}}^{\\min} = \\min_{k}\\widehat{s}(\\omega_{k});\\) and \\(\\widehat{s}(\\omega\\_{k})\\) is \\(k^{\\text{th}}\\) value of autoregressive spectrum estimator.\nThe particular value is considered to be visually significant if, at a trading day or at a seasonal frequency \\(\\omega_{k}\\) (other than the seasonal frequency \\(\\omega_{60} = \\pi\\)), \\(\\widehat{s}(\\omega\\_{k})\\) is above the median of the plotted values of \\(\\widehat{s}(\\omega_{k})\\) and is larger than both neighbouring values \\(\\widehat{s}(\\omega_{k - 1})\\) and \\(\\widehat{s}(\\omega_{k + 1})\\) by at least \\(\\frac{6}{52}\\) times the range \\({\\widehat{s}}^{\\max} - {\\widehat{s}}^{\\min}\\).\nFollowing the suggestion of SOUKUP, R.J., and FINDLEY, D.F. (1999), JDemetra+ uses an autoregressive model spectral estimator of model order 30. This order yields high resolution of strong components, meaning peaks that are sharply defined in the plot of \\(\\widehat{s}(\\omega)\\) with 61 frequencies. The minimum number of observations needed to compute the spectrum is set to \\(n=80\\) for monthly data and to \\(n=60\\) for quarterly series while the maximum number of observations considered for the estimation is 121. Consequently, with these settings it is possible to identify up to 30 peaks in the plot of 61 frequencies. By choosing \\(\\omega_{k} = \\frac{\\text{πk}}{60}\\) for \\(k=0,1,...,60\\) the density estimates are calculated at exact seasonal frequencies (1, 2, 3, 4, 5 and 6 cycles per year).\nThe model order can also be selected based on the AIC criterion (in practice it is much lower than 30). A lower order produces the smoother spectrum, but the contrast between the spectral amplitudes at the trading day frequencies and neighbouring frequencies is weaker, and therefore not as suitable for automatic detection.\nSOUKUP, R.J., and FINDLEY, D.F. (1999) also explain that the periodogram can be used in the visual significance test as it has as good as those of the AR(30) spectrum abilities to detect trading day effect, but also has a greater false alarm rate11.\ncomment2: end part theory>spectral analysis>auto-regressive spectrum\n\n\nMethod 3: Tukey spectrum"
  },
  {
    "objectID": "M-spectral-analysis.html#identification-of-spectral-peaks",
    "href": "M-spectral-analysis.html#identification-of-spectral-peaks",
    "title": "Spectral Analysis Principles and Tools",
    "section": "Identification of spectral peaks",
    "text": "Identification of spectral peaks\ncomment3: start part theory>spectral analysis>identification of spectral peaks\nIdentification of seasonal peaks in a Tukey periodogram and in an autoregressive spectrum\nIn order to decide whether a series has a seasonal component that is predictable (stable) enough, these tests use visual criteria and formal tests for the periodogram. The periodogram is calculated using complete years, so that the set of Fourier frequencies contains exactly all seasonal frequencies12.\nThe tests rely on two basic principles:\n\nThe peaks associated with seasonal frequencies should be larger than the median spectrum for all frequencies and;\nThe peaks should exceed the spectrum of the two adjacent values by more than a critical value.\n\n\nJDemetra+ performs this test on the original series. If these two requirements are met, the test results are displayed in green. The statistical significance of each of the seasonal peaks (i.e. frequencies \\(\\frac{\\pi}{6},\\ \\frac{\\pi}{3},\\ \\frac{\\pi}{2},\\ \\frac{2\\pi}{3}\\) and \\(\\frac{5\\pi}{6}\\) corresponding to 1, 2, 3, 4 and 5 cycles per year) is also displayed. The seasonal and trading days frequencies depends on the frequency of time series. They are shown in the table below. The symbol \\(d\\) denotes a default frequency and is described below the table.\n\nThe seasonal and trading day frequencies by time series frequency\n{: .table .table-style} |Number of months per full period | Seasonal frequency | Trading day frequency (radians)| |————————————–| ————————————————————————————-| ————————————| |12 | \\(\\frac{\\pi}{6},\\frac{\\pi}{3},\\ \\frac{\\pi}{2},\\frac{2\\pi}{3},\\ \\frac{5\\pi}{6},\\ \\pi\\) | \\(d\\), 2.714| |6 | \\(\\frac{\\pi}{3},\\frac{2\\pi}{3}\\), \\(\\pi\\) | \\(d\\) |4 | \\(\\frac{\\pi}{2}\\), \\(\\pi\\) | \\(d\\), 1.292, 1.850, 2.128| |3 | \\(\\pi\\) | \\(d\\)| |2 | \\(\\pi\\) | \\(d\\)|\nThe calendar (trading day or working day) effects, related to the variation in the number of different days of the week per period, can induce periodic patterns in the data that can be similar to those resulting from pure seasonal effects. From the theoretical point of view, trading day variability is mainly due to the fact that the average number of days in the months or quarters is not equal to a multiple of 7 (the average number of days of a month in the year of 365.25 days is equal to \\(\\frac{365.25}{12} = 30.4375 days\\)). This effect occurs \\(\\frac{365.25}{12} \\times \\frac{1}{7} = 4.3482 times per month\\): one time for each one of the four complete weeks of each month, and a residual of 0.3482 cycles per month, i.e. \\(0.3482 \\times 2\\pi = 2.1878 radians\\). This turns out to be a fundamental frequency for the effects associated with monthly data. In JDemetra+ the fundamental frequency corresponding to 0.3482 cycles per month is used in place of the closest frequency \\(\\frac{\\text{πk}}{60}\\). Thus, the quantity \\(\\frac{\\pi \\times 42}{60}\\) is replaced by \\(\\omega_{42} = 0.3482 \\times 2\\pi = 2.1878\\). The frequencies neighbouring \\(\\omega_{42}\\), i.e. \\(\\omega_{41}\\) and \\(\\omega_{43}\\) are set to, respectively, \\(2.1865 - \\frac{1}{60}\\) and \\(2.1865 + \\frac{1}{60}\\).\nThe default frequencies (\\(d\\)) for calendar effect are: 2.188 (monthly series) and 0.280 (quarterly series). They are computed as:\n\\[\n\\omega_{\\text{ce}} = \\frac{2\\pi}{7}\\left( n - 7 \\times \\left\\lbrack \\frac{n}{7} \\right\\rbrack \\right)\n\\], [1] \nwhere:\n\\(n = \\frac{365.25}{s}\\), \\(s = 4\\) for quarterly series and \\(s = 12\\) for monthly series.\nOther frequencies that correspond to trading day frequencies are: 2.714 (monthly series) and 1.292, 1.850, 2.128 (quarterly series).\nIn particular, the calendar frequency in monthly data (marked in red on the figure below) is very close to the seasonal frequency corresponding to 4 cycles per year \\(\\text{ω}_{40} = \\frac{2}{3}\\pi = 2.0944\\).\n\n\n\nText\n\n\nPeriodogram with seasonal (grey) and calendar (red) frequencies highlighted\nThis implies that it may be hard to disentangle both effects using the frequency domain techniques.\ncomment3: end part theory>spectral analysis>identification of spectral peaks\n\nin Tukey spectrum\ncomes from Identification of seasonal peaks in a Tukey spectrum\n\n\nTukey Spectrum definition\nThe Tukey spectrum belongs to the class of lag-window estimators. A lag window estimator of the spectral density \\(f(\\omega)=\\frac{1}{2\\pi}\\sum_{k<-\\infty}^{\\infty}\\gamma(k)e^{i k \\omega}\\) is defined as follows:\n\\[\n\\hat{f}_{L}(\\omega)=\\frac{1}{2\\pi}\\sum_{\\left| h \\right| \\leq r } w(h/r)\\hat{\\gamma}(h)e^{i h \\omega}\n\\]\nwhere \\(\\hat{\\gamma}(.)\\) is the sample autocovariance function, \\(w(.)\\) is the lag window, and \\(r\\) is the truncation lag. \\(\\left| w(x)\\right|\\) is always less than or equal to one, \\(w(0)=1\\) and \\(w(x)=0\\) for \\(\\left| x \\right| > 1\\). The simple idea behind this formula is to down-weight the autocovariance function for high lags where \\(\\hat{\\gamma}(h)\\) is more unreliable. This estimator requires choosing \\(r\\) as a function of the sample size such that \\(r/n \\rightarrow 0\\) and \\(r\\rightarrow \\infty\\) when \\(n \\rightarrow \\infty\\) . These conditions guarantee that the estimator converges to the true density.\nJDemetra+ implements the so-called Blackman-Tukey (or Tukey-Hanning) estimator, which is given by \\(w(h/r)=0.5(1+cos(\\pi h/r))\\) if \\(\\left| h/r \\right| \\leq 1\\) and \\(0\\) otherwise.\nThe choice of large truncation lags \\(r\\) decreases the bias, of course, but it also increases the variance of the spectral estimate and decreases the bandwidth.\nJDemetra+ allows the user to modify all the parameters of this estimator, including the window function.\n\n\nGraphical Test\nThe current JDemetra+ implementation of the seasonality test is based on a \\(F(d_{1},d_{2})\\) approximation that has been originally proposed by Maravall (2012) for TRAMO-SEATS. This test is has been designed for a Blackman-Tukey window based on a particular choices of the truncation lag \\(r\\) and sample size. Following this approach, we determine visually significant peaks for a frequency \\(\\omega_{j}\\) when\n\\[\n\\frac{2 f_{x}(\\omega_{j})}{\\left[ f_{x}(\\omega_{j+1})+ f_{x}(\\omega_{j-1}) \\right]} \\ge CV(\\omega_{j})\n\\]\nwhere \\(CV(\\omega_{j})\\) is the critical value of a \\(F(d_{1},d_{2})\\) distribution, where the degrees of freedom are determined using simulations. For \\(\\omega_{j}= \\pi\\), we have a significant peak when \\(\\frac{f_{x}(\\omega_{[n/2]})}{\\left[ f_{x}(\\omega_{[(n-1)/2]})\\right]} \\ge CV(\\omega_{j})\\)\nTwo significant levels for this test are considered: \\(\\alpha=0.05\\) (code “t”) and \\(\\alpha=0.01\\) (code “T”).\nAs opposed to the AR spectrum, which is computed on the basis of the last \\(120\\) data points, we will use here all available observations. Those critical values have been calculated given the recommended truncation lag \\(r=79\\) for a sample size within the interval \\(\\in [80,119]\\) and \\(r=112\\) for \\(n \\in [120,300]\\) . The \\(F\\) approximation is less accurate for sample sizes larger than \\(300\\). For quarterly data, \\(r=44\\), but there are no recommendations regarding the required sample size.\n\n\nUse\nThe test can be applied directly to any series by selecting the option Statistical Methods >> Seasonal Adjustment >> Tools >> Seasonality Tests. This is an example of how results are displayed for the case of a monthly series:\n\n\n\ntktest\n\n\nJDemetra+ considers critical values for \\(\\alpha=1\\%\\) (code “T”) and \\(\\alpha=5\\%\\) (code “t”) at each one of the seasonal frequencies represented in the table below, e.g. frequencies \\(\\frac{\\pi}{6}, \\frac{\\pi}{3}, \\frac{\\pi}{2}, \\frac{2\\pi}{3}\\text{ and } \\frac{5\\pi}{6}\\) corresponding to 1, 2, 3, 4, 5 and 6 cycles per year in this example, since we are dealing with monthly data. The codes “a” and “A” correpond to the so-called AR spectrum, so ignore them for the moment.\nThe seasonal and trading day frequencies by time series frequency\n\n\n\n\n\n\n\n\nNumber of months per full period\nSeasonal frequency\nTrading day frequency (radians)\n\n\n\n\n12\n\\(\\frac{\\pi}{6},\\frac{\\pi}{3},\\ \\frac{\\pi}{2},\\frac{2\\pi}{3},\\ \\frac{5\\pi}{6},\\ \\pi\\)\n\\(d\\), 2.714\n\n\n6\n\\(\\frac{\\pi}{3},\\frac{2\\pi}{3}\\), \\(\\pi\\)\n\\(d\\)\n\n\n4\n\\(\\frac{\\pi}{2}\\), \\(\\pi\\)\n\\(d\\), 1.292, 1.850, 2.128\n\n\n3\n\\(\\pi\\)\n\\(d\\)\n\n\n2\n\\(\\pi\\)\n\\(d\\)\n\n\n\nCurrently, only seasonal frequencies are tested, but the program allows you to manually plot the Tukey spectrum and focus your attention on both seasonal and trading day frequencies.\n\n\nReferences\n\nTukey, J. (1949). The sampling theory of power spectrum estimates., Proceedings Symposium on Applications of Autocorrelation Analysis to Physical Problems, NAVEXOS-P-735, Office of Naval Research, Washington, 47-69\n\n\n\nin AR Spectrum definition\ncomes from: “Identification of seasonal peaks in autoregressive spectrum”\nThe estimator of the spectral density at frequency \\(\\lambda \\in [0,\\pi]\\) will be given by the assumption that the series will follow an AR(p) process with large \\(p\\). The spectral density of such model, with an innovation variance \\(var(x_{t})=\\sigma^2_x\\), is expressed as follows:\n\\[\n10\\times log_{10} f_x(\\lambda)=10\\times log_{10} \\frac{\\sigma^2_x}{2\\pi \\left|\\phi(e^{i\\lambda}) \\right|^2 }=10\\times log_{10} \\frac{\\sigma^2_x}{2\\pi \\left|1-\\sum_{k=1}^{p}\\phi_k e^{i k \\lambda}) \\right|^2 }\n\\]\nwhere \\(\\phi_k\\) denotes the AR(k) coefficient, and \\(e^{-ik\\lambda}=cos⁡(-ik\\lambda)+i sin⁡(-ik\\lambda)\\).\nSoukup and Findely (1999) suggest the use of p=30, which in practice much larger than the order that would result from the AIC criterion. The minimum number of observations needed to compute the spectrum is set to n=80 for monthly data (or n=60) for quarterly series. In turn, the maximum number of observations considered for the estimation is n=121. This choice offers enough resolution, being able to identify a maximum of 30 peaks in a plot of 61 frequencies: by choosing \\(\\lambda_j=\\pi j/60\\),for \\(j=0,1,…,60\\), we are able to calculate our density estimates at exact seasonal frequencies (1, 2, 3, 4, 5 and 6 cycles per year). Note that \\(x\\) cycles per year can be converted into cycles per month by simply dividing by twelve, \\(x/12\\), and to radians by applying the transformation \\(2\\pi(x/12)\\).\nThe traditional trading day frequency corresponding to 0.348 cycles per month is used in place of the closest frequency \\(\\pi j/60\\). Thus, we replace \\(\\pi 42/60\\) by \\(\\lambda_{42}=0.348\\times 2 \\pi = 2.1865\\). The frequencies neighbouring \\(\\lambda_{42}\\) are set to \\(\\lambda_{41}= 2.1865-1/60\\) and \\(\\lambda_{43}= 2.1865+1/60\\). The periodogram below illustrates the proximity of this trading day frequency \\(\\lambda_{42}\\) (red shade) and the frequency corresponding to 4 cycles per year \\(\\lambda_{40}=2.0944\\). This proximity is precisely what poses the identification problems: the AR spectrum boils down to a smoothed version of the periodogram and the contribution of the of the trading day frequency may be obscured by the leakage resulting from the potential seasonal peak at \\(\\lambda_{40}\\), and vice-versa.\n\n\n\nText\n\n\nPeriodogram with seasonal (grey) and calendar (red) frequencies highlighted\nJDemetra+ allows the user to modify the number of lags of this estimator and to change the number of observations used to determine the AR parameters. These two options can improve the resolution of this estimator.\n\n\nGraphical Test\nThe statistical significance of the peaks associated to a given frequency can be informally tested using a visual criterion, which has proved to perform well in simulation experiments. Visually significant peaks for a frequency \\(\\lambda_{j}\\) satisfy both conditions:\n\n\\(\\frac{f_{x}(\\lambda_{j})- \\max \\left\\{f_{x}(\\lambda_{j+1}),f_{x}(\\lambda_{j-1}) \\right\\}}{\\left[ \\max_{k}f_{x}(\\lambda_{k})-\\min_{i}f_{x}(\\lambda_{i}) \\right]}\\ge CV(\\lambda_{j})\\), where \\(CV(\\lambda_{j})\\) can be set equal to \\(6/52\\) for all \\(j\\)\n\\(f_{x}(\\lambda_{j})> median_{j} \\left\\{ f_{x}(\\lambda_{j}) \\right\\}\\), which guarantees \\(f_{x}(\\lambda_{j})\\) it is not a local peak.\n\nThe first condition implies that if we divide the range \\(\\max_{k}f_{x}(\\lambda_{k})-\\min_{i}f_{x}(\\lambda_{i})\\) in 52 parts (traditionally represented by stars) the height of each pick should be at least 6 stars.\n\n\nUse\nThe test can be applied directly to any series by selecting the option Statistical Methods >> Seasonal Adjustment >> Tools >> Seasonality Tests. This is an example of how results are displayed for the case of a monthly series:\n\n\n\nartest\n\n\nJDemetra+ considers critical values for \\(\\alpha=1\\%\\) (code “A”) and \\(\\alpha=5\\%\\) (code “a”) at each one of the seasonal frequencies represented in the table below, e.g. frequencies \\(\\frac{\\pi}{6}, \\frac{\\pi}{3}, \\frac{\\pi}{2}, \\frac{2\\pi}{3}\\text{ and } \\frac{5\\pi}{6}\\) corresponding to 1, 2, 3, 4, 5 and 6 cycles per year in this example, since we are dealing with monthly data. The codes “t” and “T” correpond to the so-called Tukey spectrum, so ignore them for the moment.\nThe seasonal and trading day frequencies by time series frequency\n{: .table .table-style} |Number of months per full period | Seasonal frequency | Trading day frequency (radians)| |————————————–| ————————————————————————————-| ————————————| |12 | \\(\\frac{\\pi}{6},\\frac{\\pi}{3},\\ \\frac{\\pi}{2},\\frac{2\\pi}{3},\\ \\frac{5\\pi}{6},\\ \\pi\\) | \\(d\\), 2.714| |6 | \\(\\frac{\\pi}{3},\\frac{2\\pi}{3}\\), \\(\\pi\\) | \\(d\\) |4 | \\(\\frac{\\pi}{2}\\), \\(\\pi\\) | \\(d\\), 1.292, 1.850, 2.128| |3 | \\(\\pi\\) | \\(d\\)| |2 | \\(\\pi\\) | \\(d\\)|\nCurrently, only seasonal frequencies are tested, but the program allows you to manually plot the AR spectrum and focus your attention on both seasonal and trading day frequencies. Agustin Maravall has conducted a simulation experiment to calculate \\(CV(\\lambda_{42})\\) (trading day frequency) and proposes to set for all \\(j\\) equal to the critical value associated to the trading frequency, but this is currently not part of the current automatic testing procedure of JDemetra+.\n\n\nReferences\n\nSoukup, R.J., and D.F. Findley (1999) On the Spectrum Diagnosis used by X12-ARIMA to Indicate the Presence of Trading Day Effects After Modeling or Adjustment. In Proceedengs of the American Statistical Association. Business and Economic Statistics Section, 144-149, Alexandria, VA.\n\n\n\nin a Periodogram\ncomes from: Identification of seasonal peaks in periodogram\nThe periodogram \\(I(\\omega_j)\\) of \\(\\mathbf{X} \\in \\mathbb{C}^n\\) is defined as the squared of the Fourier transform\n\\[\nI(\\omega_{j})=a_{j}^{2}=n^{-1}\\left| \\sum_{t=1}^{n}\\mathbf{X_t} e^{-it\\omega_j} \\right|^{2},\n\\]\nwhere the Fourier frequencies \\(\\omega_{j}\\) are given by multiples of the fundamental frequency \\(\\frac{2\\pi}{n}\\):\n\\[\n\\omega_{j}= \\frac{2\\pi j}{n}, -\\pi < \\omega_{j} \\leq \\pi\n\\]\nAn orthonormal basis in \\(\\mathbb{R}^n\\):\n\\[\n\\left\\{ e_0, ~~~~~~c_1, s_1, ~~~~~\\ldots~~~~~\\ , ~~~~c_{[(n-1)/2]}, s_{[(n-1)/2]}~~~~,~~~~~~ e_{n/2}  \\right\\},\n\\] where \\(e_{n/2}\\) is excluded if \\(n\\) is odd,\ncan be used to project the data and obtain the spectral decomposition\nThus, the periodogram is given by the projection coefficients and represents the contribution of the jth harmonic to the total sum of squares, as illustrated by Brockwell and Davis (1991):\n\n\n\nSource\nDegrees of freedom\n\n\n\n\nFrequency \\(\\omega_{0}\\)\n1\n\n\nFrequency \\(\\omega_{1}\\)\n2\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\nFrequency \\(\\omega_{k}\\)\n2\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\nFrequency \\(\\omega_{n/2}=\\pi\\)\n1\n\n\n(excluded if \\(n\\) is odd)\n\n\n\n\\(=========\\)\n\\(======\\)\n\n\nTotal\nn\n\n\n\n\\[\n~~~~\n\\]\nIn JDemetra+, the periodogram of \\(\\mathbf{X} \\in \\mathbb{R}^n\\) is computed for the standardized time series.\n\n\nDefining a F-test\nBrockwell and Davis (1991, section 10.2) exploit the fact that the periodogram can be expressed as the projection on the orthonormal basis defined above to derive a test. Thus, under the null hypothesis:\n\n\\(2I(\\omega_{k})= \\| P_{\\bar{sp}_{\\left\\{ c_{k},s_{k} \\right\\}}} \\mathbf{X} \\|^{2} \\sim \\sigma^{2} \\chi^{2}(2)\\), for Fourier frequencies \\(0 < \\omega_{k}=2\\pi k/n < \\pi\\)\n\\(I(\\pi)= \\| P_{\\bar{sp}_{\\left\\{ e_{n/2} \\right\\}}} \\mathbf{X} \\|^{2} \\sim \\sigma^{2} \\chi^{2}(1)\\), for \\(\\pi\\)\n\nBecause \\(I(\\omega_{k})\\) is independent from the projection error sum of squares, we can define our F-test statistic as follows:\n\n\\(\\frac{ 2I(\\omega_{k})}{\\|\\mathbf{X}-P_{\\bar{sp}_{\\left\\{ e_0,c_{k},s_{k} \\right\\}}} \\mathbf{X}\\|^2} \\frac{n-3}{2} \\sim F(2,n-3)\\), for Fourier frequencies \\(0 < \\omega_{k}=2\\pi k/n < \\pi\\)\n\\(\\frac{ I(\\pi)}{\\|\\mathbf{X}-P_{\\bar{sp}_{\\left\\{ e_0,e_{n/2} \\right\\}}} \\mathbf{X}\\|^2} \\frac{n-2}{1} \\sim F(1,n-2)\\), for \\(\\pi\\)\n\nwhere - \\(\\|\\mathbf{X}-P_{\\bar{sp}_{\\left\\{ e_0,c_{k},s_{k} \\right\\}}} \\mathbf{X}\\|^2 = \\sum_{i=1}^{n}\\mathbf{X^2_i}-I(0)-2I(\\omega_{k}) \\sim \\sigma^{2} \\chi^{2}(n-3)\\) for Fourier frequencies \\(0 < \\omega_{k}=2\\pi k/n < \\pi\\) - \\(\\|\\mathbf{X}-P_{\\bar{sp}_{\\left\\{ e_0,e_{n/2} \\right\\}}} \\mathbf{X}\\|^2 = \\sum_{i=1}^{n}\\mathbf{X^2_i}-I(0)-I(\\pi) \\sim \\sigma^{2} \\chi^{2}(n-2)\\) for \\(\\pi\\)\nThus, we reject the null if our F-test statistic computed at a given seasonal frequency (different from \\(\\pi\\)) is larger than \\(F_{1-α}(2,n-3)\\). If we consider \\(\\pi\\), our test statistic follows a \\(F_{1-α}(1,n-2)\\) distribution.\n\n\nSeasonality test\nThe implementation of JDemetra+ considers simultaneously the whole set of seasonal frequencies (1, 2, 3, 4, 5 and 6 cycles per year). Thus, the resulting test-statistic is:\n\\[\n\\frac{ 2I(\\pi/6)+ 2I(\\pi/3)+ 2I(2\\pi/3)+ 2I(5\\pi/6)+ \\delta I(\\pi)}{\\left\\|\\mathbf{X}-P_{\\bar{sp}_{\\left\\{ e_0,c_{1},s_{1},c_{2},s_{2},c_{3},s_{3},c_{4},s_{4},c_{5},s_{5}, \\delta e_{n/2} \\right\\}}} \\mathbf{X} \\right\\|^2} \\frac{n-12}{11} \\sim F(11-\\delta,n-12+\\delta)\n\\] where \\(\\delta=1\\) if \\(n\\) is even and 0 otherwise.\nIn small samples, the test performs better when the periodogram is evaluated as the exact seasonal frequencies. JDemetra+ modifies the sample size to ensure the seasonal frequencies belong to the set of Fourier frequencies. This strategy provides a very simple and effective way to eliminate the leakage problem.\nExample of how results are displayed:\n\n\n\nperiodtest\n\n\n\n\nReferences\nBrockwell, P.J., and R.A. Davis (1991). Times Series: Theory and Methods. Springer Series in Statistics."
  },
  {
    "objectID": "M-spectral-analysis.html#spectral-graphs",
    "href": "M-spectral-analysis.html#spectral-graphs",
    "title": "Spectral Analysis Principles and Tools",
    "section": "Spectral graphs",
    "text": "Spectral graphs\nprobably moove this part to GUI (Tools), just leave a link\ncomment3: start part case studies > spectral graphs\nThis scenario is designed for advanced users interested in an in-depth analysis of time series in the frequency domain using three spectral graphs. Those graphs can also be used as a complementary analysis for a better understanding of the results obtained with some of the tests described above.\nEconomic time series are usually presented in a time domain (X-axis). However, for analytical purposes it is convenient to convert the series to a frequency domain due to the fact that any stationary time series can be expressed as a combination of cosine (or sine) functions. These functions are characterized with different periods (amount of time to complete a full cycle) and amplitudes (maximum/minimum value during the cycle).\nThe tool used for the analysis of a time series in a frequency domain is called a spectrum. The peaks in the spectrum indicate the presence of cyclical movements with periodicity between two months and one year. A seasonal series should have peaks at the seasonal frequencies. Calendar adjusted data are not expected to have peak at with a calendar frequency.\nThe periodicity of the phenomenon at frequency f is \\(\\frac{2\\pi}{f}\\). It means that for a monthly time series the seasonal frequencies \\(\\frac{\\pi}{6}, \\frac{\\pi}{3}, \\frac{\\pi}{2}, \\frac{2\\pi}{3}, \\frac{5\\pi}{6}\\) and \\(\\pi\\) correspond to 1, 2, 3, 4, 5 and 6 cycles per year. For example, the frequency \\(\\frac{\\pi}{3}\\) corresponds to a periodicity of 6 months (2 cycles per year are completed). For the quarterly series there are two seasonal frequencies: \\(\\frac{\\pi}{2}\\) (one cycle per year) and \\(\\pi\\) (two cycles per year). A peak at the zero frequency always corresponds to the trend component of the series. Seasonal frequencies are marked as grey vertical lines, while violet vertical lines represent the trading-days frequencies. The trading day frequency is 0.348 and derives from the fact that a daily component which repeats every seven days goes through 4.348 cycles in a month of average length 30.4375 days. It is therefore seen to advance 0.348 cycles per month when the data are obtained at twelve equally spaced times in 365.25 days (the average length of a year).\nThe interpretation of the spectral graph is rather straightforward. When the values of a spectral graph for low frequencies (i.e. one year and more) are large in relation to its other values it means that the long-term movements dominate in the series. When the values of a spectral graph for high frequencies (i.e. below one year) are large in relation to its other values it means that the series are rather trendless and contains a lot of noise. When the values of a spectral graph are distributed randomly around a constant without any visible peaks, then it is highly probable that the series is a random process. The presence of seasonality in a time series is manifested in a spectral graph by the peaks on the seasonal frequencies.\nSpectral graphs in GUI\n\n\n\nText\n\n\nAuto-regressive spectrum’s properties\n\nThe spectral graphs are available from: Tools → Spectral analysis.\n\n\n\nText\n\n\nTools for spectral analysis\nWhen the first option is chosen JDemetra+ displays an empty Auto-regressive spectrum window. To start an analysis drag a single time series from the Providers window and drop it into the Drop data here area.\n\n\n\nText\n\n\nLaunching an auto-regressive spectrum\nAn auto-regressive spectrum graph available in JDemetra+ is based on the relevant tool from the X-13ARIMA-SEATS program. It shows the spectral density (spectrum) function, which reformulates the content of the stationary time series’ autocovariances in terms of amplitudes at frequencies of half a cycle per month or less. The number of observations, data transformations and other options such as the specification of the frequency grid and the order of the autoregressive polynomial (30 by default) can be specified by opening the Window → Properties from the main menu.\nThe Auto-regressive - Properties window contains the following options:\n\nLog - a log transformation of a time series;\nDifferencing - transforms a data by calculating a regular (order 1,2..) or seasonal (order 4, 12, depending on the time series frequency) differences;\nDifferencing lag - the number of lags that the program will use to take differences. For example, if Differencing lag = 3 then the differencing filter does not apply to the first lag (default) but to the third lag.\nLast years - a number of years at the end of the time series taken to produce autoregresive spectrum. By default, it is 0, which means that the whole time series is considered.\nAuto-regressive polynomial order - the number of lags in the AR model that is used to estimate the spectral density. By default, the order of the autoregressive polynomial is set to 30 lags.\nResolution - the value 1 plots the spectral density estimate for the frequencies \\(\\omega_{j} = \\frac{2\\pi j}{n}\\), where \\(n \\in ( - \\pi;\\pi)\\) is the size of the sample used to estimate the AR model. Increasing this value, which is set to 5 by default, will increase the precision of this grid.\n\nThe seasonality test described above uses an empirical criterion to check whether the series has a seasonal component that is predictable (stable) enough that it can be estimated with reasonable success. The peak in the auto-regressive spectrum has to be greater than the median of the 61 spectrum ordinates and has to exceed the two adjacent spectral values by more than a critical value. When such a case is detected, the test results are displayed in green.\n\n\n\nText\n\n\nAn example of an-auto-regressive spectrum\nThe second spectral graph is a periodogram. To perform the analysis of a single time series using this tool, choose Tools →Spectral analysis → Periodogram and drag and drop a series from the Providers window to the empty Periodogram window.\n\n\n\nText\n\n\nLaunching a periodogram\nThe sample size and data transformations can be specified by opening the Window → Properties, in the main menu. The Periodogram - Properties window contains the following options:\n\nLog - a log transformation of a time series;\nDifferencing - transforms the data by calculating regular (order 1,2..) or seasonal (order 4, 12, depending on the time series frequency) differences;\nDifferencing lag - the number of lags that you will use to take differences. For example, if Differencing lag = 3 then the differencing filter does not apply to the first lag (default) but to the third lag.\nLast years - the number of years at the end of the time series taken to produce periodogram. By default it is 0, which means that the whole time series is considered.\n\n\n\n\nText\n\n\nPeriodogram’s properties\nThe periodogram was one of the earliest tools used for the analysis of time series in the frequency domain. It enables the user to identify the dominant periods (or frequencies) of a time series. In general, the periodogram is a wildly fluctuating estimate of the spectrum with a high variance and is less stable than an auto-regressive spectrum.\n\n\n\nText\n\n\nAn example of a periodogram\nThe third spectral graph is the Tukey spectrum. To perform the analysis of time series using this tool, choose Tools → Spectral analysis → Tukey spectrum and drag and drop a single series from the Providers window to the empty Periodogram window.\n\n\n\nText\n\n\nLaunching a Tukey spectrum\nThe Tukey spectrum estimates the spectral density by smoothing the periodogram.\n\n\n\nText\n\n\nAn example of a Tukey spectrum\nThe options for the Tuckey window can be specified by opening the Window → Properties from the main menu. The Periodogram - Properties window contains the following options:\n\nLog - a log transformation of a time series.\nDifferencing - transforms the data by calculating regular (order 1, 2..) or seasonal (order 4, 12, depending on the time series frequency) differences.\nDifferencing lag - the number of lags that you will use to take differences. For example, if Differencing lag = 3 then the differencing filter does not apply to the first lag (default) but to the third lag.\nTaper part – parameter larger than 0 and smaller or equal to one that shapes the curvature of the smoothing function that is applied to the auto-covariance function.\nWindow length – the size of the window that is used to smooth the auto-covariance function. A value of zero includes the whole series.\nWindow type – it refers to the weighting scheme that it is used to smooth the auto-covariance function. The available windows types (Square, Welch, Tukey, Barlett, Hamming, Parzen) are suitable to estimate the spectral density.\n\n\n\n\nText\n\n\nTukey spectrum’s properties\n\ncomment3: end part case studies > spectral graphs"
  },
  {
    "objectID": "M-reg-arima-modelling.html",
    "href": "M-reg-arima-modelling.html",
    "title": "Reg-Arima models",
    "section": "",
    "text": "The primary aim of seasonal adjustment is to remove the unobservable seasonal component from the observed series. The decomposition routines implemented in the seasonal adjustment methods make specific assumptions concerning the input series. One of the crucial assumptions is that the input series is stochastic, i.e. it is clean of deterministic effects. Another important limitation derives from the symmetric linear filter used in TRAMO-SEATS and X-13ARIMA-SEATS. A symmetric linear filter cannot be applied to the first and last observations with the same set of weights as for the central observations[^1]. Therefore, for the most recent observations these filters provide estimates that are subject to revisions.\nTo overcome these constrains both seasonal adjustment methods discussed here include a modelling step that aims to analyse the time series development and provide a better input for decomposition purposes. The tool that is frequently used for this purpose is the ARIMA model, as discussed by BOX, G.E.P., and JENKINS, G.M. (1970). However, time series are often affected by the outliers, other deterministic effects and missing observations. The presence of these effects is not in line with the ARIMA model assumptions. The presence of outliers and other deterministic effects impede the identification of an optimal ARIMA model due to the important bias in the estimation of parameters of sample autocorrelation functions (both global and partial)[^3]. Therefore, the original series need to be corrected for any deterministic effects and missing observations. This process is called linearisation and results in the stochastic series that can be modelled by ARIMA.\nFor this purpose both TRAMO and RegARIMA use regression models with ARIMA errors. With these models TRAMO and RegARIMA also produce forecasts."
  },
  {
    "objectID": "M-X11-decomposition.html",
    "href": "M-X11-decomposition.html",
    "title": "X-11 decomposition",
    "section": "",
    "text": "When the series are non-stationary differentiation is performed before the seasonality tests.↩︎\nSee section Combined seasonality tests.↩︎\nFor the multiplicative decomposition the following formula is used: \\[\n\\text{Component}_{d} = \\frac{1}{n - d}\\sum_{t = d + 1}^{n}{|\\frac{\\text{Tabl}e_{t}}{\\text{Table}_{t - d}} - 1|}\n\\].↩︎\nThe component is estimated by extracting a linear trend from the trend component presented in Table D12.↩︎\nFor the additive decomposition the formula is: \\[\nCorr_{k}I_{t} = \\frac{\\sum_{t = k + 1}^{N}{(I_{t} \\times I_{t - k})}}{\\sum_{t = 1}^{N}{(I_{t})}^{2}}\n\\]↩︎"
  },
  {
    "objectID": "M-SEATS-decomposition.html",
    "href": "M-SEATS-decomposition.html",
    "title": "SEATS decomposition",
    "section": "",
    "text": "SEATS is a program for estimating unobserved components in a time series. It follows the ARIMA-model-based (AMB) method, developed from the work of CLEVELAND, W.P., and TIAO, G.C. (1976), BURMAN, J.P. (1980), HILLMER, S.C., and TIAO, G.C. (1982), BELL, W.R., and HILLMER, S.C. (1984) and MARAVALL, A., and PIERCE, D.A. (1987).\nIn JDemetra+ the input for the model based signal extraction procedure is always provided by TRAMO and includes the original series \\(y_{t}\\), the linearized series \\(x_{t}\\) (i.e. the original series \\(y_{t}\\ \\)with the deterministic effects removed), the ARIMA model for the stochastic (linearized) time series \\(x_{t}\\) and the deterministic effects (calendar effects, outliers and other regression variable effects)1. SEATS decomposes the linearized series (and the ARIMA model) into trend, seasonal, transitory and irregular components, provides forecasts for these components, together with the associated standard errors, and finally assign the deterministic effects to each component yielding the final components2. The Minimum Mean Square Error (MMSE) estimators of the components are computed with a Wiener-Kolmogorov filter applied to the finite series extended with forecasts and backcasts3."
  },
  {
    "objectID": "M-SEATS-decomposition.html#arima-modellling-of-the-input-series",
    "href": "M-SEATS-decomposition.html#arima-modellling-of-the-input-series",
    "title": "SEATS decomposition",
    "section": "ARIMA modellling of the input series",
    "text": "ARIMA modellling of the input series\nOne of the fundamental assumptions made by SEATS is that the linearized time series \\(x_{t}\\) follows the ARIMA model\n\\[\\phi(B)\\delta\\left( B \\right)x_{t} = \\theta(B)a_{t}\\] [1] \nwhere:\n\\(B\\) – the backshift operator \\((Bx_{t} = x_{t - 1})\\);\n\\(\\delta\\left( B \\right)\\) – a non-stationary autoregressive (AR) polynomial in \\(B\\) (unit roots);\n\\(\\theta\\left( B \\right)\\) – an invertible moving average (MA) polynomial in \\(B\\) and in \\(B^{S}\\), which can be expressed in the multiplicative form\n\\(\\left( 1 + \\vartheta_{1}B + \\ldots{+ \\ \\vartheta}{q}B^{q} \\right)\\left( \\ 1 + \\Theta{1}B^{s} + \\ldots{+ \\ \\Theta}_{Q}B^{\\text{sQ}} \\right)\\) ;\n\\(\\phi(B)\\) – a stationary autoregressive (AR) polynomial in \\(B\\) and in \\(B^{S}\\ \\)containing regular and seasonal unit roots, with s representing the number of observations per year;\n\\(a_{t}\\) – a white-noise variable with the variance\\(\\ V(a)\\).\nIt should be noted that the stochastic time series can be predicted using its past observations and making an error. The variable \\(a_{t}\\), which is assumed to be white noise, is the fundamental innovation to the series at time t, that is the part that cannot be predicted based on the past history of the series.\nDenoting \\(\\varphi\\left( B \\right) = \\phi\\left( B \\right)\\delta\\left( B \\right),\\ \\) [1]  can be written in a more concise form as\n\\[\\varphi\\left( B \\right)x_{t} = \\theta(B)a_{t}\\], [2] \nwhere \\(\\varphi\\left( B \\right)\\) contains both the stationary and the nonstationary roots."
  },
  {
    "objectID": "M-SEATS-decomposition.html#derivation-of-the-models-for-the-components",
    "href": "M-SEATS-decomposition.html#derivation-of-the-models-for-the-components",
    "title": "SEATS decomposition",
    "section": "Derivation of the models for the components",
    "text": "Derivation of the models for the components\nLet us consider the additive decomposition model\n\\[x_{t} = \\sum_{i = 1}^{k}x_{\\text{it}}\\], [3] \nwhere i refers to the orthogonal components: trend, seasonal, transitory or irregular. Apart from the irregular component, supposed to be a white noise, it is assumed that each component follows the ARIMA model which can be represented, using the notation of [2] , as:\n\\[\\varphi_{i}\\left( B \\right)\\ x_{\\text{it}} = \\theta_{i}(B)a_{\\text{it}}\\], [4] \nwhere \\(\\varphi_{i}\\left( B \\right) = \\phi_{i}\\left( B \\right)\\delta_{i}\\left( B \\right),\\ \\ x_{\\text{it}}\\) is the i-th unobserved component, \\(\\varphi_{i}\\left( B \\right)\\) and \\(\\theta_{i}\\left( B \\right)\\) are finite polynomials of order \\(p_{i}\\) and \\(q_{i}\\), respectively, and \\(a_{\\text{it}},\\) the disturbance associated with such component, is a white noise process with zero mean and constant variance \\(V(a_{i})\\) and \\(a_{\\text{it}}\\) and \\(a_{\\text{jt}}\\ \\)are not correlated for \\(i \\neq j\\ \\)and for any \\(t\\).. These disturbances are functions of the innovations in the series and are called \"pseudo-innovations\" in the literature concerning the AMB decomposition as they refer to the components that are never observed 4. In the JDemetra+ documentation the term \"innovations\" is used to refer to the \"pseudo-innovations\".\nThe following assumptions hold for [4] . For each \\(\\text{i}\\) the polynomials \\(\\phi_{i}\\left( B \\right)\\), \\(\\delta_{i}\\left( B \\right)\\) and \\(\\theta_{i}(B)\\) are prime and of finite order. The roots of \\(\\delta_{i}\\left( B \\right)\\) lies on the unit circle; those of \\(\\phi_{i}\\left( B \\right)\\) lie outside, while all the roots of \\(\\theta_{i}\\left( B \\right)\\ \\)are on or outside the unit circle. This means that nonstationary and noninvertible components are allowed. Since different roots of the AR polynomial induce peaks in the spectrum5 of the series at different frequencies, and given that different components are associated with the spectral peaks for different frequencies, it is assumed that for \\(i \\neq j\\) the polynomials\\(\\ \\phi_{i}\\left( B \\right)\\) and \\(\\phi_{j}\\left( B \\right)\\) do not share any common root (they are coprime). Finally, it is assumed that the polynomials \\(\\theta_{i}\\left( B \\right),\\ i = 1,\\ldots,k\\) are prime share no unit root in common, guaranteeing the invertibility of the overall series. In fact, since the unit root of \\(\\theta_{i}\\left( B \\right)\\) induce a spectral zero, when the polynomials \\(\\theta_{i}\\left( B \\right),\\ i = 1,\\ldots,k\\) share no unit root in common, there is no frequency for which all component spectra become zero6.\nSince aggregation of ARIMA models yields ARIMA models, the series \\(x_{t}\\ \\)will also follow an ARIMA model, as in [2] , and consequently the following identity can be derived:\n\\[\\frac{\\theta(B)}{\\varphi(B)}a_{t} = \\sum_{i = 1}^{k}{\\frac{\\theta_{i}(B)}{\\varphi_{i}(B)}a_{\\text{it}}}\\]. [5] \nIn the ARIMA model based approach implemented in SEATS, the ARIMA model identified and estimated for the observed series \\(x_{t}\\) is decomposed to derive the models for the components. In particular, the AR polynomials for the components, \\(\\varphi_{i}\\left( B \\right),\\) are easily derived through the factorization of the AR polynomial \\(\\varphi\\left( B \\right)\\):\n\\[\\varphi\\left( B \\right) = \\prod_{i = 1}^{k}{\\varphi_{i}\\left( B \\right)}\\], [6] \nwhile the MA polynomials for the components, together with the innovation variances \\(V(a_{i})\\), cannot simply be obtained through the relationship:\n\\[\\theta(B)a_{t} = \\sum_{i = 1}^{k}{\\varphi_{\\text{ni}}\\left( B \\right)}\\theta_{i}(B)a_{\\text{it}}\\], [7] \nwhere \\(\\varphi_{\\text{ni}}\\left( B \\right)\\) is the product of all \\(\\varphi_{j}\\left( B \\right),\\ j = 1,\\ldots,k\\), except from \\(\\varphi_{i}\\left( B \\right)\\). Further assumptions are therefore needed to cope with the underidentification problem: i) \\(p_{i} \\geq q_{i}\\) and ii) the canonical decomposition, i.e. the decomposition that allocate all additive white noise to the irregular component (yielding noninvertible components except the irregular).\nTo understand how SEATS factorizes the AR polynomials, first a concept of a root will be explored7.\nThe equation [2]  can be expressed as:\n\\[\\psi^{- 1}(B)x_{t} = a_{t}(1 + \\varphi_{1}B + \\ldots\\varphi_{p}B^{p})x_{t} =(1 + \\theta_{1}B + \\ldots\\theta_{q}B^{q})a_{t}\\], [8] \nLet us now consider [2]  in the inverted form:\n\\[\\theta\\left( B \\right)y_{t} = \\varphi(B)a_{t}\\], [9] \nIf both sides of [8]  are multiplied by \\(x_{t - k}\\) with \\(k > q\\), and expectations are taken, the right hand side of the equation vanishes and the left hand side becomes:\n\\[\\varphi(B)\\gamma_{k} = \\gamma_{k} + \\varphi_{1}\\gamma_{k - 1} + \\ldots\\varphi_{p}\\gamma_{k - p} = 0 \\], [10] \nwhere \\(B\\) operates on the subindex \\(k\\).\nThe autocorrelation function \\(\\gamma_{k}\\) is a solution of [10]  with the characteristic equation:\n\\[z^{p} + \\varphi_{1}z^{p - 1} + \\ldots\\varphi_{p - 1}z + \\varphi_{p} = 0\\]. [11] \nIf \\(z_{1}\\),…,\\(\\ z_{p}\\) are the roots of [11] , the solutions of [10]  can be expressed as:\n\\(\\gamma_{k} = \\sum_{i = 1}^{p}z_{i}^{k}\\), [12] \nand will converge to zero as \\(k \\rightarrow \\infty\\) when \\(\\left| r_{i} \\right| < 1,\\ i = 1,\\ldots,p\\). From [10]  and [12]  it can be noticed that \\(z_{1} = B_{i}^{- 1}\\), meaning that \\(z_{1}\\),…,\\(\\ z_{p}\\) are the inverses of the roots \\(B_{1},\\ldots,B_{p}\\) of the polynomial \\(\\varphi(B)\\). The convergence of \\(\\gamma_{k}\\) implies that the roots of the \\(\\varphi(B)\\) are larger than 1 in modulus (lie outside the unit circle). Therefore, from the equation\n\\[\n  {\\varphi(B)}^{- 1} = \\frac{1}{(1 - z_{1})\\ldots(1 - z_{1})}\n  \\] [13] \nit can be derived that \\({\\varphi(B)}^{- 1}\\) is convergent and all its inverse roots are less than 1 in modulus.\nEquation [11]  has real and complex roots (solutions). Complex number \\(x = a + bi\\), with \\(a\\) and \\(\\text{b}\\) both real numbers, can be represented as \\(x = r\\left( cos(\\omega) + i\\ sin(\\omega \\right))\\), where \\(i\\) is the imaginary unit\\({\\ (i}^{2} = - 1)\\), \\(r\\) is the modulus of \\(x\\), that is \\(\\ r = \\left| x \\right| = \\sqrt{a^{2} + b^{2}}\\) and \\(\\omega\\) is the argument (frequency). When roots are complex, they are always in pairs of complex conjugates. The representation of the complex number \\(x = a + bi\\) has a geometric interpretation in the complex plane established by the real axis and the orthogonal imaginary axis.\n\n\n\nText\n\n\nGeometric representation of a complex number and of its conjugate\nRepresenting the roots of the characteristic equation [11]  in the complex plane enhances understanding how they are allocated to the components. When the modulus \\(r\\) of the roots in \\(\\text{z}\\) are greater than 1 (i.e. modulus of the roots in \\(\\varphi(B)\\  < 1\\)), the solution of the characteristic equation has a systematic explosive process, which means that the impact of the given impulse on the time series is more and more pronounced in time. This behaviour is not in line with the developments that can be identified in actual economic series. Therefore, the models estimated by TRAMO-SEATS (and X-13ARIMA-SEATS) have never inverse roots in \\(B\\) with modulus greater than 1.\nThe characteristic equations associated with the regular and the seasonal differences have roots in \\(\\varphi(B)\\) with modulus \\(r = 1\\). They are called non-stationary roots and can be represented on the unit circle. Let us consider the seasonal differencing operator applied to a quarterly time series \\((1 - B^{4})\\). Its characteristic equation is \\({(z}^{4} - 1) = 0\\) with solutions given by\\(\\ z = \\sqrt[4]{1}\\), i.e. \\(z_{1,2} = \\pm 1\\) and \\(z_{3,4} = \\pm i1\\). The first two solutions are real and the last two are complex conjugates. They are represented by the black points on the unit circle on the figure below.\n\n\n\nText\n\n\nUnit roots on the unit circle\nFor the seasonal differencing operator \\((1 - B^{12})\\) applied to the monthly time series the characteristic equation \\({\\ (z}^{12} - 1) = 0\\) has twelve non-stationary solutions given by\\(\\ z = \\sqrt[12]{1}:\\) two real and ten complex conjugates, represented by the white circles in unit roots figure above.\nThe complex conjugates roots generate the periodic movements of the type:\n\\[z_{t} = A^{t}\\cos\\left( \\omega t + W \\right).\\] [14] \nwhere:\n\\(A\\) – amplitude;\n\\(\\omega\\) – angular frequency (in radians);\n\\(W\\) – phase (angle at \\(t = 0)\\).\nThe frequency \\(f\\), i.e. the number of cycles per unit time, is \\(\\frac{\\omega}{2\\pi}\\). If it is multiplied by s, the number of observations per year, the number of cycles completed in one year is derived. The period of function [14] , denoted by \\(\\tau\\), is the number of units of time (months/quarters) it takes for a full circle to be completed.\nFor quarterly series the seasonal movements are produced by complex conjugates roots with angular frequencies at \\(\\frac{\\pi}{2}\\) (one cycle per year) and \\(\\pi\\) (two cycles per year). The corresponding number of cycles per year and the length of the movements are presented in the table below.\nSeasonal frequencies for a quarterly time series\n{: .table .table-style} | Angular frequency (\\(\\omega\\)) | Frequency (cycles per unit time) (\\(f\\)) | Cycles per year | Length of the movement measured in quarters (\\(\\tau\\)) | |—————–|—————–|—————–|—————–| | \\(\\frac{\\pi}{2}\\) | 0.25 | 1 | 4 | | \\(\\pi\\) | 0.5 | 2 | 2 |\nFor monthly time series the seasonal movements are produced by complex conjugates roots at the angular frequencies: \\(\\ \\frac{\\pi}{6},\\frac{\\pi}{3},\\ \\frac{\\pi}{2},\\ \\frac{2\\pi}{3},\\ \\frac{5\\pi}{6}\\ \\)and \\(\\pi\\). The corresponding number of cycles per year and the length of the movements are presented in the table below: Seasonal frequencies for a monthly time series.\nSeasonal frequencies for a monthly time series\n{: .table .table-style} | Angular frequency (\\(\\omega\\)) | Frequency (cycles per unit time) (\\(f\\)) | Cycles per year | Length of the movement measured in months (\\(\\tau\\)) | |—————–|—————–|—————–|—————–| | \\(\\frac{\\pi}{6}\\) | 0.083 | 1 | 12 | | \\(\\frac{\\pi}{3}\\) | 0.167 | 2 | 6 | | \\(\\frac{\\pi}{2}\\) | 0.250 | 3 | 4 | | \\(\\frac{2\\pi}{3}\\) | 0.333 | 4 | 3 | | \\(\\frac{5\\pi}{6}\\) | 0.417 | 5 | 2.4 | | \\[\\pi\\] | 0.500 | 6 | 2 |\nIn JDemetra+ SEATS assigns the roots of the AR full polynomial to the components according to their associated modulus and frequency, i.e.:8\n\nRoots of \\(\\left( 1 - B \\right)^{d}\\) are assigned to trend component.\nRoots of \\(\\ \\left( 1 - B^{s} \\right)^{d_{s}} = {((1 - B)(1 + B + \\ldots + B^{s - 1}))}^{d_{s}}\\ \\)are assigned to the trend component (root of\\({\\ \\left( 1 - B \\right)}^{d_{s}}\\)) and to the seasonal component (roots of\\({\\ (1 + B + \\ldots + B^{s - 1})}^{d_{s}}\\)).\nWhen the modulus of the inverse of a real positive root of \\(\\varphi(B)\\) is greater than \\(k\\) or equal to \\(k\\), where \\(k\\) is the threshold value controlled by the Trend boundary parameter(in the original SEATS it is controlled by rmod)9, then the root is assigned to the trend component. Otherwise it is assigned to the transitory component.\nReal negative inverse roots of \\(\\text{ ϕ}_{p}\\left( B \\right)\\ \\)associated with the seasonal two-period cycle are assigned to the seasonal component if their modulus is greater than k, where \\(k\\) is the threshold value controlled by the Seasonal boundary and the Seas. boundary (unique) parameters. Otherwise they are assigned to the transitory component.\nComplex roots, for which the argument (angular frequency) is close enough to the seasonal frequency are assigned to the seasonal component. Closeness is controlled by the Seasonal tolerance and Seasonal tolerance (unique) parameters (in the original SEATS it is controlled by epsphi). Otherwise they are assigned to the transitory component.\nIf \\(d_{s}\\ \\)(seasonal differencing order) is present\\(\\ \\)and \\(\\text{Bphi} < 0\\) (\\(\\text{Bphi}\\) is the estimate of the seasonal autoregressive parameter), the real positive inverse root is assigned to the trend component and the other (\\(s - 1\\)) inverse roots are assigned to the seasonal component. When \\(d_{s} = 0\\), the root is assigned to the seasonal when \\(\\text{Bphi} < - 0.2\\) and/or the overall test for seasonality indicates presence of seasonality. Otherwise it goes to the transitory component. Also, when \\(\\text{Bphi} > 0\\), roots are assigned to the transitory component.\n\nFor further details about JDemetra+ parameters see section TramoSeats.\nIt should be highlighted that when\\(\\ Q > P\\), where \\(Q\\) and \\(P\\) denote the orders of the polynomials \\(\\varphi\\left( B \\right)\\) and \\(\\theta(B)\\), the SEATS decomposition yields a pure MA \\((Q - P)\\) component (hence transitory). In this case the transitory component will appear even when there is no AR factor allocated to it.\nOnce these rules are applied, the factorization of the AR polynomial presented by [2]  yields to the identification of the AR polynomials for the components which contain, respectively, the AR roots associated with the trend component, the seasonal component and the transitory component.10\nThen with the partial fraction expansion the spectrum of the final components are obtained.\nFor example, the Airline model for a monthly time series:\n\\[(1 - B)(1 - B^{12})x_{t} = (1 + \\theta_{1}B)(1 + \\Theta_{1}B^{12})\\ a_{t}\\], [15] \nis decomposed by SEATS into the model for the trend component:\n\\[(1 - B)(1 - B)c_{t} = (1 + \\theta_{c,1}B + \\theta_{c,2}B^{2})a_{c,t}\\], [16] \nand the model for the seasonal component:\n\\[\\left( 1 + B + \\ldots + B^{11} \\right)s_{t} = \\left( 1 + \\theta_{s,1}B + \\ldots + {\\theta_{s,11}B}^{11} \\right)a_{s,t},\\] [17] \nAs a result, the Airline model is decomposed as follows:\n\\[\\frac{(1 + \\theta_{1}B)(1 + \\Theta_{1}B^{12})}{(1 - B)(1 - B)}a_{t} = \\frac{\\left( 1 + \\theta_{s,1}B + \\ldots + {\\theta_{s,11}B}^{11} \\right)}{\\left( 1 + B + \\ldots + B^{11} \\right)}a_{s,t} + \\frac{(1 + \\theta_{c,1}B + \\theta_{c,2}B^{2})}{(1 - B)(1 - B)}a_{c,t} + u_{t}\\]. [18] \nThe transitory component is not present in this case and the irregular component is the white noise.\nThe partial fractions decomposition is performed in a frequency domain. In essence, it consists in portioning of the pseudo-spectrum11 of \\(x_{t}\\) into additive spectra of the components. When the AMB decomposition of the ARIMA model results in the non-negative spectra for all components, the decomposition is called admissible12. In such case an infinite number of admissible decompositions exists, i.e. decompositions that yield the non-negative spectra of all components. Therefore, the MA polynomials and the innovation variances cannot be yet identified from the model of \\(x_{t}\\). As sketched above, to solve this underidentification problem and identify a unique decomposition, it is assumed that for each component the order of the MA polynomial is no greater than the order of the AR polynomial and the canonical solution of S.C. Hillmer and G.C. Tiao is applied13, i.e. all additive white noise is added to the irregular component As a consequence all components derived from the canonical decomposition, except from the irregular, have a spectral minimum of zero and are thus noninvertible14. Given the stochastic features of the series, it can be shown by that the canonical decomposition produces as stable as possible trend and seasonal components since it maximizes the variance of the irregular and minimizes the variance of the other components15. However, there is a price to be paid as canonical components can produce larger revisions in the preliminary estimators of the component16 than any other admissible decomposition.\nThe figure below represents the pseudo-spectrum for the canonical trend and an admissible trend.\n\n\n\nText\n\n\nA comparison of canonical trend and admissible trend\nA pseudo-spectrum is denoted by\\(\\ g_{i}(\\omega)\\), where \\(\\omega\\) represents the angular frequency. The pseudo-spectrum of \\(x_{\\text{it}}\\) is defined as the Fourier transform of ACGF of\\(\\ x_{t}\\) which is expressed as:\n\\[\\frac{\\psi_{i}\\left( B \\right)\\psi_{i}\\left( F \\right)}{\\delta_{i}\\left( B \\right)\\delta_{i}\\left( F \\right)}V(a_{i})\\], [19] \nwhere:\n\\(\\psi_{i}\\left( F \\right) = \\frac{\\theta_{i}\\left( F \\right)}{\\phi_{i}\\left( F \\right)}\\)\n\\(\\psi_{i}\\left( B \\right) = \\frac{\\theta_{i}\\left( B \\right)}{\\phi_{i}\\left( B \\right)}\\)\n\\(B\\) is the backward operator,\n\\(F\\) is the forward operator.\nA pseudo-spectrum for a monthly time series \\(x_{t}\\ \\)is presented in the figure below: The pseudo-spectrum for a monthly series. The frequency \\(\\omega = 0\\) is associated with the trend, frequencies in the range [\\(0 + \\epsilon_{1},\\ \\frac{\\pi}{6} - \\epsilon_{2}\\)] with \\(\\left[0 + \\epsilon_{1},\\ \\frac{\\pi}{6} - \\epsilon_{2}\\right]\\) \\(\\epsilon_{1},\\ \\epsilon_{2} > 0\\) and \\(\\epsilon_{1} < \\ \\frac{\\pi}{6} - \\epsilon_{2}\\ \\) are usually associated with the business-cycle and correspond to a period longer than a year and bounded17. The frequencies in the range [\\(\\frac{\\pi}{6},\\ \\pi\\)] are associated with the short term movements, whose cycle is completed in less than a year. If a series contains an important periodic component, its spectrum reveals a peak around the corresponding frequency and in the ARIMA model it is captured by an AR root. In the example below spectral peaks occur at the frequency \\(\\omega = 0\\) and at the seasonal frequencies ( \\(\\frac{\\pi}{6}\\), \\(\\frac{2\\pi}{6},\\ \\frac{3\\pi}{6},\\ \\frac{4\\pi}{6},\\frac{5\\pi}{6},\\pi\\)). 18\n\n\n\nText\n\n\nThe pseudo-spectrum for a monthly series\nIn the decomposition procedure, the pseudo-spectrum of the time series \\(x_{t}\\) is divided into the spectra of its components (in the example figure below, four components were obtained).\n\n\n\nText\n\n\nThe pseudo-spectra for the components"
  },
  {
    "objectID": "M-SEATS-decomposition.html#estimation-of-the-components-with-the-wiener-kolmogorow-filter",
    "href": "M-SEATS-decomposition.html#estimation-of-the-components-with-the-wiener-kolmogorow-filter",
    "title": "SEATS decomposition",
    "section": "Estimation of the components with the Wiener-Kolmogorow filter",
    "text": "Estimation of the components with the Wiener-Kolmogorow filter\nThe various components are estimated using Wiener-Kolmogorow (WK) filters. JDemetra+ includes three options to estimate the WK filter, namely Burman, KalmanSmoother and MCElroyMatrix19. Here the first of abovementioned options, proposed by BURMAN, J.P. (1980) will be explained.\nThe estimation procedure and the properties of the WK filter are easier to explain with a two-component model. Let the seasonally adjusted series (\\(s_{t}\\)) be the signal of interest and the seasonal component (\\(n_{t}\\)) be the remainder, \"the noise\". The series is given by the model [2]  and from [4]  the models for theoretical components are:\n\\[\\varphi_{s}(B)s_{t} = \\theta_{s}(B)a_{\\text{st}}\\] [20] \nand\n\\[\\varphi_{n}(B)n_{t} = \\theta_{n}(B)a_{\\text{nt}}\\]. [21] \nFrom [6]  and [7]  it is clear that \\(\\varphi\\left( B \\right) = \\varphi_{s}(B)\\varphi_{n}(B)\\) and \\(\\theta\\left( B \\right)a_{t} = \\theta_{s}(B)a_{\\text{st}}+\\theta_{n}(B)a_{\\text{nt}}\\).\nAs the time series components are never observed, their estimators have to be used. Let us note \\(X_{T}\\) an infinite realization of the time series \\(x_{t}\\). SEATS computes the Minimum Mean Square Error (MMSE) estimator of \\(s_{t}\\), e.g. the estimator \\[\\widehat{s}_{t}\\] that minimizes \\[E\\lbrack\\left({s_{t}-{\\widehat{s}}_{t})}^{2}|X_{T} \\right)\\rbrack\\]. Under the normality assumption \\[{\\widehat{s}}_{t|T}\\] is also equal to the conditional expectation \\[E\\left(s_{t}|X_{T}\\right)\\], so it can be presented as a linear function of the elements in \\[X_{T}\\].20 WHITTLE (1963) shows that the MMSE estimator of \\[{\\widehat{s}}_{t}\\] is:\n\\[{\\widehat{s}}_{t} = k_{s}\\frac{\\psi_{s}(B)\\psi_{s}(F)}{\\psi(B)\\psi(F)}x_{t}\\], [22] \nwhere \\[\\psi(B)= \\frac{\\theta(B)}{\\phi(B)}\\],\n\\[F = B^{- 1}\\] and \\[k_{s}=\\frac{V(a_{s})}{V(a)}\\],\n\\[V(a_{s})\\] is the variance of \\[a_{st}\\] and \\[V(a)\\] is the variance of \\[a_{t}\\].\nExpressing the \\[\\psi\\left(B\\right)\\] polynomials as functions of the AR and MA polynomials, after cancelation of roots, the estimator of \\[s_{t}\\] can be expressed as:\n\\[{\\widehat{s}}_{t} = k_{s}\\frac{\\theta_{s}\\left(B\\right)\\theta_{s}\\left(F\\right)\\varphi_{n}\\left(B \\right)\\delta_{n}\\left(B\\right)\\varphi_{n}\\left(F\\right)\\delta_{n}\\left(F\\right)}{\\theta\\left(B\\right)\\theta\\left(F \\right)}x_{t}\\], [23] \nwhere:\n\\[\\nu_{s}\\left( B,F \\right) = k_{s}\\frac{\\theta_{s}\\left( B \\right)\\theta_{s}\\left( F \\right)\\varphi_{n}\\left( B \\right)\\delta_{n}\\left( B \\right)\\varphi_{n}\\left( F \\right)\\delta_{n}\\left( F \\right)}{\\theta\\left( B \\right)\\theta\\left( F \\right)}\\] [24] \nis a WK filter.\nEquation [24]  shows that the WK filter is two-sided (uses observations both from the past and from the future), centered (the number of points in the past is the same as in the future) and symmetric (for any \\(k\\) the weight applied to \\(x_{t - k}\\) and \\(x_{t + k}\\) is the same), which allows the phase effect to be avoided. Due to invertibility of \\(\\theta\\left( B \\right)\\) (and \\(\\theta\\left( F \\right)\\)) the filter is convergent in the past and in the future.\nThe estimator can be presented as\n\\[{\\widehat{s}}_{t} = \\nu_{i}\\left(B,F\\right)x_{t}\\], [25] \nwhere\n\\[\\nu_{i}\\left(B,F\\right)=\\nu_{0}+ \\sum_{j = 1}^{\\infty}\\nu_{ij}(B^{j}+F^{j})\\]\nis the WK filter.\nThe example of the WK filters obtained for the pseudo-spectra of the series illustrated above is shown on the figure below: WK filters for components.\n\n\n\nText\n\n\nWK filters for components\nThe WK filter from [24]  can also be expressed as a ratio of two pseudo-autocovariance generating functions (p-ACGF). The p-ACGF function summarizes the sequence of absolutely summable autocovariances of a stationary process \\(x_{t}\\) (see section section Spectral Analysis).\nThe ACGF function of an ARIMA process is expressed as:\n\\[acgf(B) = \\frac{\\theta\\left( B \\right)\\theta\\left( F \\right)}{\\phi\\left( B \\right)\\delta\\left( B \\right)\\phi\\left( F \\right)\\delta\\left( F \\right)}V(a)\\] [26] \nAnd, the WK filter can be rewritten as:\n\\[\\nu_{s}\\left( B,F \\right) = \\frac{\\gamma_{s}(B,F)}{\\gamma(B,F)}\\], [27] \nwhere:\n\\[\\gamma_{s}\\left( B,F \\right) = \\frac{\\theta_{s}\\left( B \\right)\\theta_{s}\\left( F \\right)}{\\phi_{s}\\left( B \\right)\\delta_{s}\\left( B \\right)\\phi_{s}\\left( F \\right)\\delta_{s}\\left( F \\right)}V(a_{s})\\] is the p-ACGF of \\[s_{t}\\];\n\\(\\gamma\\left( B,F \\right) = \\frac{\\theta\\left( B \\right)\\theta\\left( F \\right)}{\\phi\\left( B \\right)\\delta\\left( B \\right)\\phi\\left( F \\right)\\delta\\left( F \\right)}V(a)\\) is the p-ACGF of \\(x_{t}\\).\nFrom [24]  it can be seen that the WK filter depends on both the component and the series models. Consequently, the estimator of the component and the WK filter reflect the characteristic of data and by construction, the WK filter adapts itself to the series under consideration. Therefore, the ARIMA model is of particular importance for the SEATS method. Its misspecification results in an incorrect decomposition.\nThis adaptability, if the model has been correctly determined, avoids the dangers of under and overestimation with an ad-hoc filtering. For example, for the series with a highly stochastic seasonal component the filter adapts to the width of the seasonal peaks and the seasonally adjusted series does not display any spurious seasonality21. Examples of WK filters for stochastic and stable seasonal components are presented on the figure below.\n\n\n\nText\n\n\nWK filters for stable and stochastic seasonal components\nThe derivation of the components requires an infinite realization of \\(x_{t}\\) in the direction of the past and of the future. However, the convergence of the WK filter guarantees that, in practice, it could be approximated by a truncated (finite) filter and, in most applications, for large \\[k\\] the estimator for the central periods of the series can be safely seen as generated by the WK filter22:\n\\[{\\widehat{s}}_{t}=\\nu_{k}x_{t-k} + \\ldots + \\nu_{0}x_{t} + \\ldots + \\nu_{k}x_{t+k}\\]. [28] \nWhen \\(T > 2L + 1\\), where \\(T\\) is the last observed period, and \\(L\\) is an a priori number that typically expands between 3 and 5 years, the estimator expressed by [23]  can be assumed as the final (historical) estimator for the central observations of the series23. In practice, the Wiener-Kolmogorov filter is applied to \\(x_{t}\\) extended with forecasts and backcasts from the ARIMA model. The final or historical estimator of \\[{\\widehat{s}}_{t}\\], is obtained with a doubly infinite filter, and therefore contains an error \\[e_{st}\\] called final estimation error, which is equal \\[e_{st}=s_{t}-{\\widehat{s}}_{t}\\].\nIn the frequency domain, the Wiener-Kolmogorov filter\\(\\ \\nu(B,F)\\) that provides the final estimator of \\(s_{t}\\ \\)is expressed as the ratio of the \\(s_{t}\\ \\)and \\(x_{t}\\) pseudo-spectra:\n\\[\\widetilde{\\nu}\\left( \\omega \\right) = \\frac{g_{s}(\\omega)}{g_{x}(\\omega)}\\]. [29] \nThe function \\(\\widetilde{\\nu}\\left( \\omega \\right)\\ \\)is also referred as the gain of the filter.24 GÓMEZ, V., and MARAVALL, A. (2001a) show that when for some frequency the signal (the seasonally adjusted series) dominates the noise (seasonal fluctuations) the gain \\(\\widetilde{\\nu}\\left( \\omega \\right)\\) approaches 1. On the contrary, when for some frequency the noise dominates the gain \\(\\widetilde{\\nu}\\left( \\omega \\right)\\ \\)approaches 0.\nThe spectrum of the estimator of the seasonal component is expressed as:\n\\[g_{\\widehat{s}}\\left( \\omega \\right) = \\left\\lbrack \\frac{g_{s}(\\omega)}{g_{x}(\\omega)} \\right\\rbrack^{2}g_{x}(\\omega)\\], [30] \nwhere\\(\\ \\left\\lbrack \\widetilde{\\nu}\\left( \\omega \\right) \\right\\rbrack^{2} = \\left\\lbrack \\frac{g_{s}(\\omega)}{g_{x}(\\omega)} \\right\\rbrack^{2} = \\left\\lbrack \\frac{g_{s}(\\omega)}{g_{s}(\\omega) + g_{n}(\\omega)} \\right\\rbrack^{2} = \\left\\lbrack \\frac{1}{1 + \\frac{1}{r(\\omega)}} \\right\\rbrack^{2}\\) is the squared gain of the filter and \\(r\\left( \\omega \\right) = \\frac{g_{s}(\\omega)}{g_{n}(\\omega)}\\) represents the signal-to-noise ratio.\nFor each \\(\\omega\\), the MMSE estimation gives the signal-to-noise ratio. If this ratio is high, then the contribution of that frequency to the estimation of the signal will be also high. Assume that the trend is a signal that needs to be extracted from a seasonal time series. Then \\(R\\left( 0 \\right) = 1\\ \\)and the frequency \\(\\omega = 0\\) will only be used for trend estimations. For seasonal frequencies \\(R\\left( \\omega \\right) = 0,\\) so that these frequencies are ignored in computing the trend resulting in spectral zeros in \\(g_{\\widehat{s}}\\left( \\omega \\right)\\). For this reason, unlike the spectrum of the component, the component spectrum contains dips as it can be seen on the figure below: Component spectrum and estimator spectrum for trend.\n\n\n\nText\n\n\nComponent spectrum and estimator spectrum for trend\nFrom the equation [29]  it is clear that the squared gain of the filter determines how the variance of the series contributes to the variance of the seasonal component for the different frequencies. When \\(\\widetilde{\\nu}\\left( \\omega \\right) = 1\\), the full variation of \\(x_{t}\\) for that frequency is passed to \\[{\\widehat{s}}_{t}\\], while if \\[\\widetilde{\\nu}\\left(\\omega\\right) = 0 \\] the variation of \\(x_{t}\\) for that frequency is fully ignored in the computation of \\[{\\widehat{s}}_{t}\\]. These two cases are well illustrated by the figure below that shows the square gain of the WK filter for two series already analysed in the figure above (Figure: WK filters for stable and stochastic seasonal components).\n\n\n\nText\n\n\nThe squared gain of the WK filter for stable and stochastic seasonal components.\nSince \\(r\\left( \\omega \\right) \\geq 0\\), then \\(\\widetilde{\\nu}\\left( \\omega \\right) \\leq 1\\) and from [29]  it can be derived that \\(g_{\\widehat{s}}\\left( \\omega \\right) = \\widetilde{\\nu}\\left( \\omega \\right)g_{s}(\\omega)\\). As a result, the estimator will always underestimate the component, i.e. it will be always more stable that the component.25\nSince \\(g_{\\widehat{n}}\\left( \\omega \\right) < g_{n}\\left( \\omega \\right)\\) and\\(\\ g_{\\widehat{s}}\\left( \\omega \\right) < g_{s}\\left( \\omega \\right)\\) the expression: \\(g_{x}\\left( \\omega \\right) - \\left\\lbrack g_{\\widehat{n}}\\left( \\omega \\right) + g_{\\widehat{s}}\\left( \\omega \\right) \\right\\rbrack \\geq 0\\) is the cross-spectrum. As it is positive, the MMSE yields correlated estimators. This effect emerges since variance of estimator is smaller than the variance of component. Nevertheless, if at least one non-stationary component exists, cross-correlations estimated by TRAMO-SEATS will tend to zero as cross-covariances between estimators of the components are finite. In practice, the inconvenience caused by this property will likely be of little relevance.\nPreliminary estimators for the components\nGÓMEZ, V., and MARAVALL, A. (2001a) point out that the properties of the estimators have been derived for the final (or historical) estimators. For a finite (long enough) realization, they can be assumed to characterize the estimators for the central observations of the series, but for periods close to the beginning of the end the filter cannot be completed and some preliminary estimator has to be used. Indeed, the historical estimator shown in [28]  is obtained for the central periods of the series. However, when \\(t\\) approaches \\(T\\) (last observation), the WK filter requires observations, which are not available yet. For this reason a preliminary estimator needs to be used.\nTo introduce preliminary estimators let us consider a semi-finite realization \\(\\lbrack x_{- \\infty}\\),…\\(\\ x_{T}\\)], where \\(T\\) is the last observed period. The preliminary estimator of \\[x_{\\text{it}}\\] obtained at \\(T\\) \\[(T - t = k \\geq 0)\\] can be expressed as\n\\[\n{\\widehat{x}}_{it|t + k}=\\nu_{i}\\left(B,F\\right)x_{t|T}^{e}\n\\], [31] \nwhere \\[\\nu_{i}\\left(B,F \\right)\\] is the WK filter and \\[x_{t|T}^{e}\\] is the extended series, such that \\(x_{t|T}^{e} = x_{t}\\) for \\(t \\leq T\\) and \\[x_{t|T}^{e}={\\widehat{x}}_{t|T}\\] for \\[t>T\\], where \\[{\\widehat{x}}_{t|T}\\] denotes the forecast of \\(x_{t}\\) obtained at period \\(T\\).\nThe future \\(k\\) values necessary to apply the filter are not yet available and are replaced by their optimal forecasts from the ARIMA model on \\[x_{t}\\]. When \\[k=0\\] the preliminary estimator becomes the concurrent estimator. As the forecasts are linear functions of present and past observations of \\[x_{t}\\], the preliminary estimator \\[{\\widehat{x}}_{it}\\] will be a truncated asymmetric filter applied to \\[x_{t}\\] that generates a phase effect26.\nWhen a new observation \\[x_{T + 1}\\] becomes available the forecast \\[{\\widehat{x}}_{T + 1|T}\\] is replaced by the observation and the forecast \\[{\\widehat{x}}_{iT + j|T}\\], \\[j > 1\\] are updated to \\[x_{T + j|T + 1}\\] resulting in the revision error27. The total error in the preliminary estimator \\[d_{it|t + k}\\] is expressed as a sum of the final estimation error (\\[e_{it}\\]) and the revision error (\\[r_{it|t + k}\\]), i.e.:\n\\[\nd_{it|t + k} = x_{it}-{\\widehat{x}}_{it|t + k} = \\left(x_{it} - {\\widehat{x}}_{it}\\right) + \\left(          {\\widehat{x}}_{it} - {\\widehat{x}}_{it|t + k} \\right) = e_{it} + r_{it|t + k}\n\\], [32] \nwhere:\n\\[x_{it}-i^{th}\\] component;\n\\[{\\widehat{x}}_{it|t + k}\\]- the estimator of \\[x_{it}\\] when the last observation is \\[x_{t + k}\\].\nTherefore the preliminary estimator is subject not only to the final error but also to a revision error, which are orthogonal to each other28. The revision error decreases as \\[k\\] increases, until it can be assumed equal to 0 for large enough \\[k\\].\nIt’s worth remembering that SEATS estimates the unobservable components of the time series so the \"true\" components are never observed. Therefore, MARAVALL, A. (2009) stresses that the error in the historical estimator is more of academic rather than practical interest. In practice, interest centres on revisions. (…) the revision standard deviation will be an indicator of how far we can expect to be from the optimal estimator that will be eventually attained, and the speed of convergence of \\({\\theta\\left( B \\right)\\ }^{- 1}\\) will dictate the speed of convergence of the preliminary estimator to the historical one. The analysis of an error is therefore useful for making decision concerning the revision policy, including the policy for revisions and horizon of revisions."
  },
  {
    "objectID": "M-SEATS-decomposition.html#psie-weights",
    "href": "M-SEATS-decomposition.html#psie-weights",
    "title": "SEATS decomposition",
    "section": "PsiE-weights",
    "text": "PsiE-weights\nThe estimator of the component is calculated as \\[{\\widehat{x}}_{it} = \\nu_{s}\\left(B,F\\right)x_{t}\\]. By replacing \\[x_{it}=\\frac{\\theta(B)}{\\gamma(B)\\delta(B)}a_{t}\\], the component estimator can be expressed as29:\n\\[\n  {\\widehat{x}}_{it} = \\xi_{s}\\left(B,F\\right)a_{t}\n  \\], [33] \nwhere \\(\\xi_{s}\\left( B,F \\right) = \\ldots + \\xi_{j}B^{j} + \\ldots + \\xi_{1}B + \\xi_{0} + \\xi_{- 1}F\\ldots\\xi_{- j}F^{j} + \\ldots\\).\nThis representation shows the estimator as a filter applied to the innovation \\[a_{t}\\], rather than on the series \\[x_{t}\\]30. Hence, the filter from [32]  can be divided into two components: the first one, i.e. \\[\\ldots + \\xi_{j}B^{j}+ \\ldots+ \\xi_{1}B + \\xi_{0}\\], applies to prior and concurrent innovations, the second one, i.e. \\[\\xi_{- 1}F + \\ldots + \\xi_{- j}F^{j}\\] applies to future (i.e. posterior to \\[t\\]) innovations. Consequently, \\[\\xi_{j}\\] determines the contribution of \\[a_{t - j}\\] to \\[{\\widehat{s}}_{t}\\] while \\[\\xi_{- j}\\] determines the contribution of \\[a_{t + j}\\] to \\[{\\widehat{s}}_{t}\\]. Finally, the estimator of the component can be expressed as:\n\\[\n  {\\widehat{x}}_{it} =\\xi_{i}(B)^{-}a_{t} + \\xi_{i}(F)^{+}a_{t + 1}\n  \\], [34] \nwhere:\n\\(\\xi_{i}{(B)}^{-}a_{t}\\) is an effect of starting conditions, present and past innovations in series;\n\\(\\xi_{i}{(F)}^{+}a_{t + 1}\\) is an effect of future innovations.\nFor the two cases already presented in figure WK filters for stable and stochastic seasonal components and figure The squared gain of the WK filter for stable and stochastic seasonal components above, the psi-weights are shown in the figure below.\n\n\n\nText\n\n\nIt can be shown that \\[{\\xi}_{- 1},\\ldots,\\xi_{- j}\\] are convergent and \\[\\xi_{j},\\ldots, {\\xi}_{1},\\xi_{0}\\] are divergent. From [33] , the concurrent estimator is equal to \\[\n{\\widehat{x}}_{it|t} = E_{t}x_{it}=E_{t}{\\widehat{x}}_{it} = {\\xi}_{i}(B)^{-}a_{t}\n\\], [35] \nso that the revision \\[\nr_{it} = {\\widehat{x}}_{it} - {\\widehat{x}}_{it|t} = \\xi_{i}(F)^{+}a_{t + 1}\n\\] [36] \nis a zero-mean stationary MA process. As a result, historical and preliminary estimators are cointegrated. From expression [25]  the relative size of the full revision and the speed of convergence can be obtained."
  },
  {
    "objectID": "M-Trend-Estimation-Local-Polynomials.html",
    "href": "M-Trend-Estimation-Local-Polynomials.html",
    "title": "Local Polynomials Methods for Trend Estimation",
    "section": "",
    "text": "This chapter provides details on the the statistical methods whereas practical estimation steps are described “here” (Chap On trend estimation) and are run with the rjfilters or rjd3highfreq packages."
  },
  {
    "objectID": "M-tests.html",
    "href": "M-tests.html",
    "title": "Tests",
    "section": "",
    "text": "This chapter describes all the tests available in JDemetra+, via Graphical User interface and/or R packages. An outline of the underlying theoretical principles of each test is provided.\nThe procedure to apply these tests in context is described when their use is relevant in the chapters dedicated to algorithms description, mainly on seasonal adjustment.(not sure) Links to use in context are available below."
  },
  {
    "objectID": "M-tests.html#tests-on-residuals",
    "href": "M-tests.html#tests-on-residuals",
    "title": "Tests",
    "section": "Tests on residuals",
    "text": "Tests on residuals\ntable with all tests by purpose and accessibility\n\nTests on Residuals\n\n\nTest\nPurpose\nGUI\nR package\n\n\n\n\nLjung-Box\nautocorrelation\n\n\n\n\nBox-Pierce\nautocorrelation\n\n\n\n\nDoornik-Hansen\nnormality\n\n\n\n\n\n\n\n\n\n\n\n\nLjung-Box\nThe Ljung-Box Q-statistics are given by:\n\\[\n  \\text{LB}\\left( k \\right) = n \\times (n + 2) \\times \\sum_{k = 1}^{K}\\frac{\\rho_{a,k}^{2}}{n - k}\n  \\], [1]\nwhere \\(\\rho_{a,k}^{2}\\) is the autocorrelation coefficient at lag \\(k\\) of the residuals \\({\\widehat{a}}_{t}\\), \\(n\\) is the number of terms in differenced (? differenciated ?) series, \\(K\\) is the maximum lag being considered, set in JDemetra+ to \\(24\\) (monthly series) or \\(8\\) (quarterly series).\nIf the residuals are random (which is the case for residuals from a well specified model), they will be distributed as \\(\\chi_{(K - m)}^{2}\\), where \\(m\\) is the number of parameters in the model which has been fitted to the data. (edit: not the residuals, but \\(\\widehat{\\rho}\\) )\nThe Ljung-Box and Box-Pierce tests sometimes fail to reject a poorly fitting model. Therefore, care should be taken not to accept a model on a basis of their results. For the description of autocorrelation concept see section Autocorrelation function and partial autocorrelation function.\n\n\nBox-Pierce\nThe Box-Pierce Q-statistics are given by:\n\\[\\text{BP}\\left( k \\right) = n\\sum_{k = 1}^{K}\\rho_{a,k}^{2}\\]\nwhere:\n\\(\\rho_{a,k}^{2}\\) is the autocorrelation coefficient at lag \\(k\\) of the residuals \\({\\widehat{a}}_{t}\\).\n\\(n\\) is the number of terms in differenced (differenciated?) series;\n\\(K\\) is the maximum lag being considered, set in JDemetra+ to \\(24\\) (monthly series) or \\(8\\) (quarterly series).\nIf the residuals are random (which is the case for residuals from a well specified model), they will be distributed as \\(\\chi_{(K - m)}^{2}\\) degrees of freedom, where \\(m\\) is the number of parameters in the model which has been fitted to the data.(edit: same as above)\n\n\nDornik-Hansen\nThe Doornik-Hansen test for multivariate normality (DOORNIK, J.A., and HANSEN, H. (2008)) is based on the skewness and kurtosis of multivariate data that is transformed to ensure independence. It is more powerful than the Shapiro-Wilk test for most tested multivariate distributions1.\nThe skewness and kurtosis are defined, respectively, as: \\[s = \\frac{m_{3}}{\\sqrt{m_{2}}^{3}}\\] and $k = \\frac{m_{4}}{m_{2}^{2}},\\ $where: \\(m_{i} = \\frac{1}{n}\\sum_{i = 1}^{n}{(x_{i}}{- \\overline{x})}^{i}\\) \\(\\overline{x} = \\frac{1}{n}\\sum_{i = 1}^{n}x_{i}\\) and \\(n\\) is a number of (non-missing) residuals.\nThe Doornik-Hansen test statistic derives from SHENTON, L.R., and BOWMAN, K.O. (1977) and uses transformed versions of skewness and kurtosis.\nThe transformation for the skewness \\(s\\) into \\(\\text{z}_{1}\\) is as in D’AGOSTINO, R.B. (1970):\n\\[\n  \\beta = \\frac{3(n^{2} + 27n - 70)(n + 1)(n + 3)}{(n - 2)(n + 5)(n + 7)(n + 9)}\n  \\]\n\\[\n  \\omega^{2} = - 1 + \\sqrt{2(\\beta - 1)}\n  \\]\n\\[\n  \\delta = \\frac{1}{\\sqrt{\\log{(\\omega}^{2})}}\n  \\]\n\\[\n  y = s\\sqrt{\\frac{(\\omega^{2} - 1)(n + 1)(n + 3)}{12(n - 2)}}\n  \\]\n\\[\n  z_{1} = \\delta log(y + \\sqrt{y^{2} - 1})\n  \\]\nThe kurtosis \\(k\\) is transformed from a gamma distribution to \\(\\chi^{2}\\), which is then transformed into standard normal \\(z_{2}\\) using the Wilson-Hilferty cubed root transformation:\n\\[\n  \\delta = (n - 3)(n + 1)(n^{2} + 15n - 4)\n  \\]\n\\[\n  a = \\frac{(n - 2)(n + 5)(n + 7)(n^{2} + 27n - 70)}{6\\delta}\n  \\]\n\\[\n  c = \\frac{(n - 7)(n + 5)(n + 7)(n^{2} + 2n - 5)}{6\\delta}\n  \\]\n\\[\n  l= \\frac{(n + 5)(n + 7)({n^{3} + 37n}^{2} + 11n - 313)}{12\\delta}\n  \\]\n\\[\n  \\alpha = a + c \\times s^{2}\n  \\]\n\\[\n  \\chi = 2l(k - 1 - s^{2})\n  \\]\n\\[\n  z_{2} = \\sqrt{9\\alpha}\\left( \\frac{1}{9\\alpha} - 1 + \\sqrt[3]{\\frac{\\chi}{2\\alpha}} \\right)\n  \\]\nFinally, the Doornik-Hansen test statistic is defined as the sum of squared transformations of the skewness and kurtosis. Approximately, the test statistic follows a \\(\\chi^{2}\\)distribution, i.e.:\n\\[DH = z_{1}^{2} + z_{2}^{2}\\sim\\chi^{2}(2)\\]"
  },
  {
    "objectID": "M-tests.html#seasonality-tests",
    "href": "M-tests.html#seasonality-tests",
    "title": "Tests",
    "section": "Seasonality tests",
    "text": "Seasonality tests\ntable with all tests by purpose and accessibility\n\nSeasonality tests\n\n\n\n\n\n\n\n\nTest\nPurpose\nGUI\nR package\n\n\n\n\nQS test\nAutocorrelation at seasonal lags\n\n\n\n\nF-test with seasonal dummies\nStable seasonality\n\n\n\n\nIdentification of spectral peaks\nSeasonal frequencies\n\n\n\n\nFriedman test\nStable seasonality\n\n\n\n\nTwo-way variance analysis\nMoving seasonality\n\n\n\n\n\n\nQS Test on autocorrelation at seasonal lags\nThe QS test is a variant of the Ljung-Box test computed on seasonal lags, where we only consider positive auto-correlations\nMore exactly,\n\\[ QS=n \\left(n+2\\right)\\sum_{i=1}^k\\frac{\\left[ \\max  \\left(0, \\hat\\gamma_{i \\cdot l}\\right)\\right]^2}{n-i \\cdot l}\\]\nwhere \\[k=2\\], so only the first and second seasonal lags are considered. Thus, the test would checks the correlation between the actual observation and the observations lagged by one and two years. Note that \\[l=12\\] when dealing with monthly observations, so we consider the autocovariances \\[\\hat\\gamma_{12}\\] and \\[\\hat\\gamma_{24}\\] alone. In turn, \\[k=4\\] in the case of quarterly data.\nUnder H0, which states that the data are independently distributed, the statistics follows a \\[\\chi \\left(k\\right)\\] distribution. However, the elimination of negative correlations makes it a bad approximation. The p-values would be given by \\(P(\\chi^{2}\\left( k \\right) > Q)\\) for \\(k = 2\\). As \\({P(\\chi}^{2}(2)) > 0.05 = 5.99146\\) and \\({P(\\chi}^{2}(2)) > 0.01 = 9.21034\\), \\(QS > 5.99146\\) and \\(QS > 9.21034\\) would suggest rejecting the null hypothesis at \\(95\\%\\) and \\(99\\%\\) significance levels, respectively.\n\n\nModification\nMaravall (2012) proposes approximate the correct distribution (p-values) of the QS statistic using simulation techniques. Using 1000K replications of sample size 240, the correct critical values would be 3.83 and 7.09 with confidence levels of \\(95\\%\\) and \\(99\\%\\), respectively (lower than the 5.99146 and 9.21034 shown above). For each of the simulated series, he obtains the distribution by assuming \\(QS=0\\) when \\[\\hat\\gamma_{12}\\], so in practice this test will detect seasonality only when any of these conditions hold: - Statistically significant positive autocorrelation at lag 12 - Nonnegative sample autocorrelation at lag 12 and statistically significant positive autocorrelation at lag 24\n\n\nUse\nThe test can be applied directly to any series by selecting the option Statistical Methods >> Seasonal Adjustment >> Tools >> Seasonality Tests. This is an example of how results are displayed for the case of a monthly series:\n\n\n\nqs\n\n\nThe test can be applied to the input series before any seasonal adjustment method has been applied. It can also be applied to the seasonally adjusted series or to the irregular component.\n\n\nReferences\n\nLJUNG G. M. and G. E. P. BOX (1978). “On a Measure of a Lack of Fit in Time Series Models”. Biometrika 65 (2): 297–303. doi:10.1093/biomet/65.2.297\nMARAVALL, A. (2011). “Seasonality Tests and Automatic Model Identification in Tramo-Seats”. Manuscript\nMARAVALL, A. (2012). “Update of Seasonality Tests and Automatic Model Identification in TRAMO-SEATS”. Bank of Spain (November 2012)\n\n\n\nF-test on seasonal dummies\nThe F-test on seasonal dummies checks for the presence of deterministic seasonality. The model used here uses seasonal dummies (mean effect and 11 seasonal dummies for monthly data, mean effect and 3 for quarterly data) to describe the (possibly transformed) time series behaviour. The test statistic checks if the seasonal dummies are jointly statistically not significant. When this hypothesis is rejected, it is assumed that the deterministic seasonality is present and the test results are displayed in green.\nThis test refers to Model-Based $\\chi^{2}\\ $and F-tests for Fixed Seasonal Effects proposed by LYTRAS, D.P., FELDPAUSCH, R.M., and BELL, W.R. (2007) that is based on the estimates of the regression dummy variables and the corresponding t-statistics of the RegARIMA model, in which the ARIMA part of the model has a form (0,1,1)(0,0,0). The consequences of a misspecification of a model are discussed in LYTRAS, D.P., FELDPAUSCH, R.M., and BELL, W.R. (2007).\nFor a monthly time series the RegARIMA model structure is as follows:\n\\[\\left( 1 - B \\right)\\left( y_{t} - \\beta_{1}M_{1,t} - \\ldots - \\beta_{11}M_{11,t} - \\gamma X_{t} \\right) = \\mu + (1 - B)a_{t}\n\\], [1] \nwhere:\n\\[\nM_{j,t} =\n\\begin{cases}\n1 & \\text{ in month } j = 1, \\ldots, 11 \\\\\n- 1 & \\text{ in December}\\\\\n0 & \\text{ otherwise}\n\\end{cases} \\text{ - dummy variables;}\n\\]\n\\(y_{t}\\) – the original time series;\n\\(B\\) – a backshift operator;\n\\(X_{t}\\) – other regression variables used in the model (e.g. outliers, calendar effects, user-defined regression variables, intervention variables);\n\\(\\mu\\) – a mean effect;\n\\(a_{t}\\) – a white-noise variable with mean zero and a constant variance.\nIn the case of a quarterly series the estimated model has a form:\n\\[\\left( 1 - B \\right)\\left( y_{t} - \\beta_{1}M_{1,t} - \\ldots - \\beta_{3}M_{3,t} - \\gamma X_{t} \\right) = \\mu + (1 - B)a_{t}\\], [2] \nwhere:\n\\[\nM_{j,t} =\n\\begin{cases}\n1 & \\text{ in quarter} j = 1, \\ldots, 3 \\\\\n- 1 & \\text{ in the fourth quarter}\\\\\n0 & \\text{ otherwise}\n\\end{cases} \\text{ - dummy variables;}\n\\]\nOne can use the individual t-statistics to assess whether seasonality for a given month is significant, or a chi-squared test statistic if the null hypothesis is that the parameters are collectively all zero. The chi-squared test statistic is \\({\\widehat{\\chi}}^{2} = {\\widehat{\\beta}}^{'}{\\lbrack Var(\\widehat{\\beta})}^{\\ })^{- 1}\\rbrack{\\widehat{\\beta}}^{\\ }\\) in this case compared to critical values from a \\(\\chi^{2}\\left( \\text{df} \\right)\\)-distribution, with degrees of freedom $df = 11\\ $(monthly series) or \\(df = 3\\) (quarterly series). Since the \\({Var(\\widehat{\\beta})}^{\\ }\\) computed using the estimated variance of \\(\\alpha_{t}\\) may be very different from the actual variance in small samples, this test is corrected using the proposed \\(\\text{F}\\) statistic:\n\\[\n  F = \\frac{ {\\widehat{\\chi}}^{2}}{s - 1} \\times \\frac{n - d - k}{n - d}\n  \\]\nwhere \\(n\\) is the sample size, \\(d\\) is the degree of differencing, s is time series frequency (12 for a monthly series, 4 for a quarterly series) and \\(k\\) is the total number of regressors in the RegARIMA model (including the seasonal dummies \\(\\text{M}_{j,t}\\) and the intercept).\nThis statistic follows a \\[F_{s - 1,n - d - k}\\] distribution under the null hypothesis.\n\n\nIdentification of spectral peaks\nlink to relevant part in spectral analysis chapter ?\n\n\nFriedman test for stable seasonality test\nThe Friedman test is a non-parametric method for testing that samples are drawn from the same population or from populations with equal medians. The significance of the month (or quarter) effect is tested. The Friedman test requires no distributional assumptions. It uses the rankings of the observations. If the null hypothesis of no stable seasonality is rejected at the 0.10% significance level then the series is considered to be seasonal and the test’s outcome is displayed in green.\nThe test statistic is constructed as follows. Consider first the matrix of data \\[ \\left\\{x_{ij}\\right\\}_{n \\times k} \\] with \\[ n \\] rows (the blocks, i.e. number of years in the sample), \\[ k \\] columns (the treatments, i.e. either 12 months or 4 quarters, depending on the frequency of the data).\nThe data matrix needs to be replaced by a new matrix \\[ \\left\\{r_{ij}\\right\\}_{n \\times k} \\], where the entry \\[ r_{ij} \\] is the rank of \\[ x_{ij} \\] within block \\[ i \\] .\nThe test statistic is given by\n\\[\nQ=\\frac{SS_t}{SS_e}\n\\]\nwhere \\[ SS_t=n \\sum_{j=1}^{k}(\\bar{r}_{.j}-\\bar{r})^2 \\] and \\[ SS_e=\\frac{1}{n(k-1)} \\sum_{i=1}^{n}\\sum_{j=1}^{k}(r_{ij}-\\bar{r})^2 \\] It represents the variance of the average ranking across treatments j relative to the total.\nUnder the hypothesis of no seasonality, all months can be equally treated. For the sake of completeness: - \\[ \\bar{r}_{.j} \\] is the average ranks of each treatment (month) j within each block (year) - The average rank is given by \\[ \\bar{r}= \\frac{1}{nk}\\sum_{i=1}^{n}\\sum_{j=1}^{k}(r_{ij})\\]\nFor large \\[ n \\] or \\[ k \\] , i.e. n > 15 or k > 4, the probability distribution of \\[ Q \\] can be approximated by that of a chi-squared distribution. Thus, the p-value is given by \\[ P( \\chi^2_{k-1}>Q) \\] .\n\nUse\nThe test can be applied directly to any series by selecting the option Statistical Methods >> Seasonal Adjustment >> Tools >> Seasonality Tests. This is an example of how results are displayed for the case of a monthly series:\n\n\n\nfriedman\n\n\nIf the null hypothesis of no stable seasonality is rejected at the 1% significance level, then the series is considered to be seasonal and the outcome of the test is displayed in green.\nThe test can be applied to the input series before any seasonal adjustment method has been applied. It can also be applied to the seasonally adjusted series or to the irreguar component. In the case of X-13ARIMA-SEATS, the test is applied to the preliminary estimate of the unmodified Seasonal-Irregular component2 (time series shown in Table B3). In this estimate, the number of observations is lower than in the final estimate of the unmodified Seasonal-Irregular component. Thus, the number of degrees of freedom in the stable seasonality test is lower than the number of degrees of freedom in the test for the presence of seasonality assuming stability. For example, X-13ARIMA-SEATS uses a centred moving average of order 12 to calculate the preliminary estimation of trend. Consequently, the first six and last six points in the series are not computed at this stage of calculation. The preliminary estimation of the trend is then used for the calculation of the preliminary estimation of the unmodified Seasonal-Irregular.\n\n\nRelated tests\n\nWhen using this kind of design for a binary response, one instead uses the Cochran’s Q test.\nKendall’s W is a normalization of the Friedman statistic between 0 and 1.\nThe Wilcoxon signed-rank test is a nonparametric test of non-independent data from only two groups.\n\n\n\nReferences\n\nFriedman, Milton (December 1937). “The use of ranks to avoid the assumption of normality implicit in the analysis of variance”. Journal of the American Statistical Association (American Statistical Association) 32 (200): 675–701. doi:10.2307/2279372. JSTOR 2279372.\nFriedman, Milton (March 1939). “A correction: The use of ranks to avoid the assumption of normality implicit in the analysis of variance”. Journal of the American Statistical Association (American Statistical Association) 34 (205): 109. doi:10.2307/2279169. JSTOR 2279169.\nFriedman, Milton (March 1940). “A comparison of alternative tests of significance for the problem of m rankings”. The Annals of Mathematical Statistics 11 (1): 86–92. doi:10.1214/aoms/1177731944. JSTOR 2235971.\n\n\n\n\nMoving seasonality test\nThe evolutive seasonality test is based on a two-way analysis of variance model. The model uses the values from complete years only. Depending on the decomposition type for the Seasonal – Irregular component it uses [1] (in the case of a multiplicative model) or [2] (in the case of an additive model):\n\\[\n  \\left|\\text{SI}_{\\text{ij}} - 1 \\right| = X_{\\text{ij}} = b_{i} + m_{j} + e_{\\text{ij}}\n  \\], [1] \n\\[\n  \\left| \\text{SI}_{\\text{ij}} \\right| = X_{\\text{ij}} = b_{i} + m_{j} + e_{\\text{ij}}\n  \\], [2] \nwhere:\n\\(m_{j}\\) – the monthly or quarterly effect for \\(j\\)-th period, \\(j = (1,\\ldots,k)\\), where \\(k = 12\\) for a monthly series and \\(k = 4\\) for a quarterly series;\n\\(b_{j}\\) – the annual effect \\(i\\), \\((i = 1,\\ldots,N)\\) where \\(N\\) is the number of complete years;\n\\(e_{\\text{ij}}\\) – the residual effect.\nThe test is based on the following decomposition:\n\\[S^{2} = S_{A}^{2} + S_{B}^{2} + S_{R}^{2},\\] [3] \nwhere:\n\\[\nS^{2} = \\sum_{j = 1}^{k}{\\sum_{i = 1}^{N}\\left( {\\overline{X}}_{\\text{ij}} - {\\overline{X}}_{\\bullet \\bullet} \\right)^{2}}\\\n\\] –the total sum of squares;\n\\[\nS_{A}^{2} = N\\sum_{j = 1}^{k}\\left( {\\overline{X}}_{\\bullet j} - {\\overline{X}}_{\\bullet \\bullet} \\right)^{2}\n\\] – the inter-month (inter-quarter, respectively) sum of squares, which mainly measures the magnitude of the seasonality;\n\\[\nS_{B}^{2} = k\\sum_{i = 1}^{N}\\left( {\\overline{X}}_{i \\bullet} - {\\overline{X}}_{\\bullet \\bullet} \\right)^{2}\n\\] – the inter-year sum of squares, which mainly measures the year-to-year movement of seasonality;\n\\[\nS_{R}^{2} = \\sum_{i = 1}^{N}{\\sum_{j = 1}^{k}\\left( {\\overline{X}}_{\\text{ij}} - {\\overline{X}}_{i \\bullet} - {\\overline{X}}_{\\bullet j} - {\\overline{X}}_{\\bullet \\bullet} \\right)^{2}}\n\\] – the residual sum of squares.\nThe null hypothesis $H_{0}\\ $is that \\(b_{1} = b_{2} = ... = b_{N}\\) which means that there is no change in seasonality over the years. This hypothesis is verified by the following test statistic:\n\\[\n   F_{M} = \\frac{\\frac{S_{B}^{2}}{(n - 1)}}{\\frac{S_{R}^{2}}{(n - 1)(k - 1)}}\n   \\]\nwhich follows an \\(F\\)-distribution with \\(k - 1\\) and \\(n - k\\) degrees of freedom.\n\n\nCombined seasonality test\nThis test combines the Kruskal-Wallis test along with test for the presence of seasonality assuming stability (\\(F_{S}\\)), and evaluative seasonality test for detecting the presence of identifiable seasonality (\\(F_{M}\\)). Those three tests are calculated using the final unmodified SI component. The main purpose of the combined seasonality test is to check whether the seasonality of the series is identifiable. For example, the identification of the seasonal pattern is problematic if the process is dominated by highly moving seasonality3. The testing procedure is shown in the figure below.\n\n\n\nText\n\n\nCombined seasonality test, source: LADIRAY, D., QUENNEVILLE, B. (2001)"
  },
  {
    "objectID": "M-state-space-framework.html",
    "href": "M-state-space-framework.html",
    "title": "1  State space modelling",
    "section": "",
    "text": "State space forms play a central role in JD+. In a first point, we present the model used throughout the library. In a second point, we give an overview of the current implementation. The specificity of the state space framework of JD+ is stressed and the main modules are shortly presented. We finish the document with two examples based on the state space framework of JD+. The first one explains the basic structural models implemented in the “nbdemetra-sa-advanced” plug-in of JD+. The second one shows how the framework can be used to implement time-varying trading days models."
  },
  {
    "objectID": "M-state-space-framework.html#state-space-forms-ssf",
    "href": "M-state-space-framework.html#state-space-forms-ssf",
    "title": "1  State space modelling",
    "section": "1.2 State space forms (SSF)",
    "text": "1.2 State space forms (SSF)\n\n1.2.1 General form\nThe general linear gaussian state-space model can be written in many different ways. The measurement equation and the state equation considered in JD+ 3.0 are presented below.\n\\[\n\\begin{array}{r l}\ny_t&= Z_t\\alpha_t+\\epsilon_t, \\quad \\epsilon_t \\sim\\mathcal{N}(0,\\sigma^2H_t)\\\\\n\\alpha_{t+1}&= T_t\\alpha_t+\\mu_t,\\quad \\mu_t\\sim\\mathcal{N}(0,\\sigma^2V_t)\n\\end{array}\n\\]\n\\(y_t\\) is the observation at period t, \\(\\alpha_t\\) is the state vector. \\(\\epsilon_t, \\mu_t\\) are assumed to be serially independent at all time points and independent between them at all time points.\nThe residuals of the state equation will be modelled as \\(\\mu_t=S_t\\xi_t, \\quad \\xi_t\\sim\\mathcal{N}(0,\\sigma^2I)\\).\nIn other words, \\(V_t=S_tS_t'\\).\nThe initial conditions of the filter are defined as follows:\n\\[\\alpha_{-1}=a_{-1}+A_{-1}\\delta+\\mu_{-1}\\]\n\\[\\delta\\sim\\mathcal{N}(0,\\sigma^2\\kappa I)\\]\n\\[\\mu_{-1}\\sim\\mathcal{N}(0,\\sigma^2P_*)\\]\nwhere \\(\\kappa\\) is arbitrary large.\n\\(P_*\\) is the variance of the stationary part of the initial state vector and models the diffuse part.\nThe model used in JD+ is quasi-identical to the model discussed in Durbin-Koopman."
  },
  {
    "objectID": "M-Temp-Disagg-Bench.html",
    "href": "M-Temp-Disagg-Bench.html",
    "title": "Methods for Temporal disaggregation and benchmarking",
    "section": "",
    "text": "Benchmarking1 is a procedure widely used when for the same target variable the two or more sources of data with different frequency are available. Generally, the two sources of data rarely agree, as an aggregate of higher-frequency measurements is not necessarily equal to the less-aggregated measurement. Moreover, the sources of data may have different reliability. Usually it is thought that less frequent data are more trustworthy as they are based on larger samples and compiled more precisely. The more reliable measurement is considered as a benchmark.\nBenchmarking also occurs in the context of seasonal adjustment. Seasonal adjustment causes discrepancies between the annual totals of the seasonally unadjusted (raw) and the corresponding annual totals of the seasonally adjusted series. Therefore, seasonally adjusted series are benchmarked to the annual totals of the raw time series2. Therefore, in such a case benchmarking means the procedure that ensures the consistency over the year between adjusted and non-seasonally adjusted data. It should be noted that the ‘ESS Guidelines on Seasonal Adjustment’ (2015) do not recommend benchmarking as it introduces a bias in the seasonally adjusted data. Also the U.S. Census Bureau points out that: Forcing the seasonal adjustment totals to be the same as the original series annual totals can degrade the quality of the seasonal adjustment, especially when the seasonal pattern is undergoing change. It is not natural if trading day adjustment is performed because the aggregate trading day effect over a year is variable and moderately different from zero.3 Nevertheless, some users may prefer the annual totals for the seasonally adjusted series to match the annual totals for the original, non-seasonally adjusted series4. According to the ‘ESS Guidelines on Seasonal Adjustment’ (2015), the only benefit of this approach is that there is consistency over the year between adjusted and non-seasonally adjusted data; this can be of particular interest when low-frequency (e.g. annual) benchmarking figures officially exist (e.g. National Accounts, Balance of Payments, External Trade, etc.) where user needs for time consistency are stronger.\nThe benchmarking procedure in JDemetra+ is available for a single seasonally adjusted series and for an indirect seasonal adjustment of an aggregated series. In the first case, univariate benchmarking ensures consistency between the raw and seasonally adjusted series. In the second case, the multivariate benchmarking aims for consistency between the seasonally adjusted aggregate and its seasonally adjusted components.\nGiven a set of initial time series \\[\\left\\{ z_{i,t} \\right\\}_{i \\in I}\\], the aim of the benchmarking procedure is to find the corresponding \\[\\left\\{ x_{i,t} \\right\\}_{i \\in I}\\] that respect temporal aggregation constraints, represented by \\[X_{i,T} = \\sum_{t \\in T}^{}x_{i,t}\\] and contemporaneous constraints given by \\[q_{k,t} = \\sum_{j \\in J_{k}}^{}{w_{\\text{kj}}x_{j,t}}\\] or, in matrix form: \\[q_{k,t} = w_{k}x_{t}\\].\nThe underlying benchmarking method implemented in JDemetra+ is an extension of Cholette’s5 method, which generalises, amongst others, the additive and the multiplicative Denton procedure as well as simple proportional benchmarking.\nThe JDemetra+ solution uses the following routines that are described in DURBIN, J., and KOOPMAN, S.J. (2001):\n\nThe multivariate model is handled through its univariate transformation,\nThe smoothed states are computed by means of the disturbance smoother.\n\nThe performance of the resulting algorithm is highly dependent on the number of variables involved in the model (\\(\\propto \\ n^{3}\\)). The other components of the problem (number of constraints, frequency of the series, and length of the series) are much less important (\\(\\propto \\ n\\)).\nFrom a theoretical point of view, it should be noted that this approach may handle any set of linear restrictions (equalities), endogenous (between variables) or exogenous (related to external values), provided that they don’t contain incompatible equations. The restrictions can also be relaxed for any period by considering their “observation” as missing. However, in practice, it appears that several kinds of contemporaneous constraints yield unstable results. This is more especially true for constraints that contain differences (which is the case for non-binding constraints). The use of a special square root initializer improves in a significant way the stability of the algorithm."
  },
  {
    "objectID": "M-Temp-Disagg-Bench.html#temporal-disaggragation-underlying-theory",
    "href": "M-Temp-Disagg-Bench.html#temporal-disaggragation-underlying-theory",
    "title": "Methods for Temporal disaggregation and benchmarking",
    "section": "Temporal Disaggragation underlying Theory",
    "text": "Temporal Disaggragation underlying Theory"
  }
]