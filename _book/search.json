[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "JDemetra+ online documentation",
    "section": "",
    "text": "Welcome to the JDemetra+ online documentation.\nJDemetra+ is a software for seasonal adjustment and other time series functions, developed in Eurostat’s “Centre of Excellence on Statistical Methods and Tools”.\nTo learn more about this project https://ec.europa.eu/eurostat/cros/content/centre-excellence-statistical-methods-and-tools."
  },
  {
    "objectID": "G-what-is-jd.html",
    "href": "G-what-is-jd.html",
    "title": "1  JDemetra+ Software",
    "section": "",
    "text": "ressources for descrption - R tools WP - desp in esp"
  },
  {
    "objectID": "G-what-is-jd.html#a-library-of-algorithms-for-time-series-related-functions-needs",
    "href": "G-what-is-jd.html#a-library-of-algorithms-for-time-series-related-functions-needs",
    "title": "1  JDemetra+ Software",
    "section": "1.1 A library of algorithms for time series related functions ? needs ?",
    "text": "1.1 A library of algorithms for time series related functions ? needs ?\nYou can learn more about the history of the project here (link to below)"
  },
  {
    "objectID": "G-what-is-jd.html#structure-of-this-book",
    "href": "G-what-is-jd.html#structure-of-this-book",
    "title": "1  JDemetra+ Software",
    "section": "1.2 Structure of this book",
    "text": "1.2 Structure of this book\nThis book is divided in four parts, each being an entry point for the user.\n\n1.2.1 Algorithms\nThis part provides a step by step description of all the algorithms featured in JD+, grouped by purpose - seasonal adjstement - benchmarking - temporal disaggregation - … links\n\n\n1.2.2 Tools\nJdemetra+ offers 3 kind of tools\n\n\n1.2.3 Underlying Statistical Methods\nThis part gives details about the underlying statistical methods to foster a more in-depth understanding of the algorithms.Those methods are described in the light and spirit of their use as buliding blocks of the algorithms presented above, not aiming at all at their comprehensive coverage."
  },
  {
    "objectID": "G-what-is-jd.html#how-to-use-this-book",
    "href": "G-what-is-jd.html#how-to-use-this-book",
    "title": "1  JDemetra+ Software",
    "section": "1.3 How to use this book",
    "text": "1.3 How to use this book\naudience: book targets the beginner as well as seasoned (pun moethodlogist. The beginner is advised to use the quick start chapter as an etrey point, it’s presneted like a decsion tree which will point directly to the the part it’s useful to review.\nthe seasoned methodologist will benefit from the detailed chapter ad part strucutre to quickly find the needed information."
  },
  {
    "objectID": "G-quick-start.html",
    "href": "G-quick-start.html",
    "title": "2  Quick start with…",
    "section": "",
    "text": "objective: describe key steps + provide useful liks to relevant code"
  },
  {
    "objectID": "G-quick-start.html#seasonal-adjustment",
    "href": "G-quick-start.html#seasonal-adjustment",
    "title": "2  Quick start with…",
    "section": "2.1 Seasonal Adjustment",
    "text": "2.1 Seasonal Adjustment"
  },
  {
    "objectID": "G-quick-start.html#seasonal-adjustment-of-high-frequency-data",
    "href": "G-quick-start.html#seasonal-adjustment-of-high-frequency-data",
    "title": "2  Quick start with…",
    "section": "2.2 Seasonal Adjustment of High-Frequency Data",
    "text": "2.2 Seasonal Adjustment of High-Frequency Data"
  },
  {
    "objectID": "G-quick-start.html#use-of-jd-algorithms-in-r",
    "href": "G-quick-start.html#use-of-jd-algorithms-in-r",
    "title": "2  Quick start with…",
    "section": "2.3 Use of JD+ algorithms in R",
    "text": "2.3 Use of JD+ algorithms in R"
  },
  {
    "objectID": "G-quick-start.html#use-of-jd-graphical-interface",
    "href": "G-quick-start.html#use-of-jd-graphical-interface",
    "title": "2  Quick start with…",
    "section": "2.4 Use of JD+ graphical interface",
    "text": "2.4 Use of JD+ graphical interface"
  },
  {
    "objectID": "G-main-features.html",
    "href": "G-main-features.html",
    "title": "3  Main functions overview",
    "section": "",
    "text": "link to key references * 2 handbooks * sets of guidelines\nObjective: present JDemetra+ capabilities by category"
  },
  {
    "objectID": "G-main-features.html#seasonal-adjustment-algorithms",
    "href": "G-main-features.html#seasonal-adjustment-algorithms",
    "title": "3  Main functions overview",
    "section": "3.1 Seasonal adjustment algorithms",
    "text": "3.1 Seasonal adjustment algorithms\nbelow: pieces of old pages to edit and update\n\n3.1.1 Data frequencies\nThe seasonal adjustment methods available in JDemetra+ aim to decompose a time series into components and remove seasonal fluctuations from the observed time series. The X-11 method considers monthly and quarterly series while SEATS is able to decompose series with 2, 3, 4, 6 and 12 observations per year.\n\n\n3.1.2 X-13\nX-13ARIMA is a seasonal adjustment program developed and supported by the U.S. Census Bureau. It is based on the U.S. Census Bureau’s earlier X-11 program, the X-11-ARIMA program developed at Statistics Canada, the X-12-ARIMA program developed by the U.S. Census Bureau, and the SEATS program developed at the Banco de España. The program is now used by the U.S. Census Bureau for a seasonal adjustment of time series.\n\n\n3.1.3 Tramo-Seats\n\n\n3.1.4 STL\n\n\n3.1.5 Basic Structural Models"
  },
  {
    "objectID": "G-main-features.html#trend-cycle-estimation",
    "href": "G-main-features.html#trend-cycle-estimation",
    "title": "3  Main functions overview",
    "section": "3.2 Trend-cycle estimation",
    "text": "3.2 Trend-cycle estimation"
  },
  {
    "objectID": "G-main-features.html#nowacsting",
    "href": "G-main-features.html#nowacsting",
    "title": "3  Main functions overview",
    "section": "3.3 Nowacsting",
    "text": "3.3 Nowacsting"
  },
  {
    "objectID": "G-main-features.html#temporal-disaggregation",
    "href": "G-main-features.html#temporal-disaggregation",
    "title": "3  Main functions overview",
    "section": "3.4 Temporal Disaggregation",
    "text": "3.4 Temporal Disaggregation"
  },
  {
    "objectID": "A-sa.html",
    "href": "A-sa.html",
    "title": "4  Seasonal Adjustment",
    "section": "",
    "text": "much less text than current doc (too long)\nmethod details -> to method chapters\ntools details in tools: GUI or R"
  },
  {
    "objectID": "A-sa.html#motivation",
    "href": "A-sa.html#motivation",
    "title": "4  Seasonal Adjustment",
    "section": "4.2 Motivation",
    "text": "4.2 Motivation\nThe primary aim of the seasonal adjustment process is to remove seasonal fluctuations from the time series. To achieve this goal, seasonal adjustment methods decompose the original time series into components that capture specific movements. These components are: trend-cycle, seasonality and irregularity. The trend-cycle component includes long-term and medium-term movements in the data. For seasonal adjustment purposes there is no need to divide this component into two parts. JDemetra+ refers to the trend-cycle as trend and consequently this convention is used here.\nThis section presents the options of the seasonal adjustment processes performed by the methods implemented in JDemetra+ (X-12-ARIMA/X-13ARIMA-SEATS and TRAMO/SEATS) and discusses the output displayed by JDemetra+. As these seasonal adjustment methods use different approach to the decomposition, the output produced for both of them has different structure and content. Therefore, the results for both methods are discussed separately. However, in contrast to the original programs, in JDemetra+ some quality indicators have been implemented for both methods, allowing for an easier comparison of the results."
  },
  {
    "objectID": "A-sa.html#unobserved-components-uc",
    "href": "A-sa.html#unobserved-components-uc",
    "title": "4  Seasonal Adjustment",
    "section": "4.3 Unobserved Components (UC)",
    "text": "4.3 Unobserved Components (UC)\nThe main components, each representing the impact of certain types of phenomena on the time series (\\(X_{t}\\)), are:\n\nThe trend (\\(T_{t}\\)) that captures long-term and medium-term behaviour;\nThe seasonal component (\\(S_{t}\\)) representing intra-year fluctuations, monthly or quarterly, that are repeated more or less regularly year after year;\nThe irregular component (\\(I_{t}\\)) combining all the other more or less erratic fluctuations not covered by the previous components.\n\nIn general, the trend consists of 2 sub-components:\n\nThe long-term evolution of the series;\nThe cycle, that represents the smooth, almost periodic movement around the long-term evolution of the series. It reveals a succession of phases of growth and recession.\n\nFor seasonal adjustment purposes both TRAMO-SEATS and X-13ARIMA-SEATS do not separate the long-term trend from the cycle as these two components are usually too short to perform their reliable estimation. Consequently, hereafter TRAMO-SEATS and X-13ARIMA-SEATS estimate the trend component. However, the original TRAMO-SEATS may separate the long-term trend from the cycle through the Hodrick-Precsott filter using the output of the standard decomposition. It should be remembered that JDemetra+ refers to the trend-cycle as trend (\\(T_{t}\\)), and consequently this convention is used in this document.\nTRAMO-SEATS considers two decomposition models:\n\nThe additive model: \\(X_{t} = T_{t} + S_{t} + I_{t}\\);\nThe log additive model: \\(log(X_{t}) = log(T_{t}) + log(S_{t}) + log(I_{t})\\).\n\nApart from these two decomposition types X-13ARIMA-SEATS allows the user to apply also the multiplicative model: \\(X_{t} = T_{t} \\times S_{t} \\times I_{t}\\).\nA time series \\(x_{t}\\), which is a subject to a decomposition, is assumed to be a realisation of a discrete-time stochastic, covariance-stationary linear process, which is a collection of random variables \\(x_{t}\\), where \\(t\\) denotes time. It can be shown that any stochastic, covariance-stationary process can be presented in the form:\n\\(x_{t} = \\mu_{t} + {\\widetilde{x}}_{t}\\), \\[1\\]\nwhere \\(\\mu_{t}\\) is a linearly deterministic component and \\({\\widetilde{x}}_{t}\\) is a linearly interderministic component, such as:\n\\[{\\widetilde{x}}_{t} = {\\sum_{j = 0}^{\\infty}\\psi_{j}a}_{t - j}\n  \\], \\[2\\]\nwhere \\(\\sum_{j = 0}^{\\infty}\\psi_{i}^{2} < \\infty\\) (coefficients \\(\\psi_{j}\\) are absolutely summable), \\(\\psi_{0} = 1\\) and \\(a_{t}\\) is the white noise error with zero mean and constant variance \\(V_{a}\\). The error term \\(a_{t}\\) represents the one-period ahead forecast error of \\(x_{t}\\), that is:\n\\[\n  a_{t} = {\\widetilde{x}}_{t} - {\\widehat{x}}_{t|t - 1}\n  \\], \\[3\\]\nwhere \\[{\\widehat{x}}_{t|t - 1}\\] is the forecast of \\[{\\widetilde{x}}_{t}\\] made at period \\(t - 1\\). As \\(a_{t}\\) represents what is new in \\[{\\widetilde{x}}_{t}\\] in point \\(t\\), i.e., not contained in the past values of \\[{\\widetilde{x}}_{t}\\], it is also called innovation of the process. From \\[3\\] \\[{\\widetilde{x}}_{t}\\] can be viewed as a linear filter applied to the innovations.\nThe equation 7.1 is called a Wold representation. It presents a process as a sum of linearly deterministic component \\(\\mu_{t}\\) and linearly interderministic component \\(\\sum_{j = 0}^{\\infty}\\psi_{j}a_{t - j}\\), the first one is perfectly predictable once the history of the process \\(x_{t - 1}\\) is known and the second one is impossible to predict perfectly. This explains why the stochastic process cannot be perfectly predicted.\nUnder suitable conditions \\[{\\widetilde{x}}_{t}\\] can be presented as a weighted sum of its past values and \\(a_{t}\\), i.e.:\n\\[\n  { {\\widetilde{x}}_{t} = \\sum_{j = 0}^{\\infty}\\pi_{j}{\\widetilde{x}}_{t - j} + a}_{t}\n  \\], \\[4\\]\nIn general, for the observed time series, the assumptions concerning the nature of the process \\[1\\] do not hold for various reasons. Firstly, most observed time series display a mean that cannot be assumed to be constant due to the presence of a trend and the seasonal movements. Secondly, the variance of the time series may vary in time. Finally, the observed time series usually contain outliers, calendar effects and regression effects, which are treated as deterministic. Therefore, in practice a prior transformation and an adjustment need to be applied to the time series. The constant variance is usually achieved through taking a logarithmic transformation and the correction for the deterministic effects, while stationarity of the mean is achieved by applying regular and seasonal differencing. These processes, jointly referred to as preadjustment or linearization, can be performed with the TRAMO or RegARIMA models. Besides the linearisation, forecasts and backcasts of stochastic time series are estimated with the ARIMA model, allowing for later application of linear filters at both ends of time series. The estimation performed with these models delivers the stochastic part of the time series, called the linearised series, which is assumed to be an output of a linear stochastic process.1 The deterministic effects are removed from the time series and used to form the final components.\nIn the next step the linearised series is decomposed into its components. There is a fundamental difference in how this process is performed in TRAMO-SEATS and X-13ARIMA-SEATS. In TRAMO-SEATS the decomposition is performed by the SEATS procedure, which follows a so called ARIMA model based approach. In principle, it aims to derive the components with statistical models. More information is given in the SEATS section. X-13ARIMA-SEATS offers two algorithms for decomposition: SEATS and X-11. The X-11 algorithm, which is described in the X-11 section section, decomposes a series by means of linear filters. Finally, in both methods the final components are derived by the assignment of the deterministic effects to the stochastic components. Consequently, the role of the ARIMA models is different in each method. TRAMO-SEATS applies the ARIMA models both in the preadjustment step and in the decomposition procedure. On the contrary, when the X-11 algorithm is used for decomposition, X-13ARIMA-SEATS uses the ARIMA model only in the preadjustment step. In summary, the decomposition procedure that results in an estimation of the seasonal component requires prior identification of the deterministic effects and their removal from the time series. This is achieved through the linearisation process performed by the TRAMO and the RegARIMA models, shortly discussed in the Linearisation with the TRAMO and RegARIMA models section.The linearised series is then decomposed into the stochastic components with SEATS or X-11 algorithms."
  },
  {
    "objectID": "A-sa.html#detecting-seasonal-patterns",
    "href": "A-sa.html#detecting-seasonal-patterns",
    "title": "4  Seasonal Adjustment",
    "section": "4.4 Detecting seasonal patterns",
    "text": "4.4 Detecting seasonal patterns"
  },
  {
    "objectID": "A-sa.html#pre-treatment",
    "href": "A-sa.html#pre-treatment",
    "title": "4  Seasonal Adjustment",
    "section": "4.5 Pre-treatment",
    "text": "4.5 Pre-treatment\n\n4.5.1 Calendar correction\ndetails of regressor building in calendar chapter\n\n4.5.1.1 rationale\n\n\n4.5.1.2 method\n\n\n4.5.1.3 tools\n\n\n\n4.5.2 Outliers\n\n4.5.2.1 rationale\n\n\n4.5.2.2 method\n\n\n4.5.2.3 tools\n\n\n\n4.5.3 Reg-Arima Model\nTramo and Reg-Arima are very similar…details in M chapter\n\n\n4.5.4 Model evaluation\ngoodness-of-fit"
  },
  {
    "objectID": "A-sa.html#x-11-decompostion",
    "href": "A-sa.html#x-11-decompostion",
    "title": "4  Seasonal Adjustment",
    "section": "4.6 X-11 Decompostion",
    "text": "4.6 X-11 Decompostion\nthis part should allow to use x-11 via RJDemetra as well as via GUI\n\n4.6.1 Quick launch with default specifications\n\n\n4.6.2 Output 1: series\n\n\n4.6.3 Output 2: diagnostics\n\n\n4.6.4 Specifications / parameters\n\n4.6.4.1 List\n\n4.6.4.1.1 General settings\n\nMode\n\ncheck if this option still works, if so add and edit instructions from old page)\nif not but button present : explain that the mode is determined in pre-adjustment (function)\n\nSeasonal component\n\ncheck if still relevant, idem as above\nin v.2.3 if not ticked, S estimated but options on seasonal filter not available\n\nForecasts horizon\n\nLength of the forecasts generated by the RegARIMA model - in months (positive values) - years (negative values) - if set to is set to 0, the X-11 procedure does not use any model-based forecasts but the original X-11 type forecasts for one year. - default value: -1, thus one year from the Arima model\n\nBackcasts horizon\n\nLength of the backcasts generated by the RegARIMA model - in months (positive values) - years (negative values) - default value: 0\n\n\n4.6.4.1.2 Irregular correction\n\nLSigma\n\nsets lower sigma (standard deviation) limit used to down-weight the extreme irregular values in the internal seasonal adjustment iterations, learn more here (LINK to M_ chapter)\nvalues in \\([0,Usigma]\\)\ndefault value is 1.5\n\nUSigma\n\nsets upper sigma (standard deviation)\nvalues in \\([Lsigma,+\\infty]\\)\ndefault value is 2.5\n\nCalendarsigma\n\nallows to set different LSigma and USigma for each period\nvalues\n\nNone (default)\nAll: standard errors used for the extreme values detection and adjustment computed separately for each calendar month/quarter\nSignif: groups determined by cochran test (check)\nSigmavec: set two customized groups of periods\n\n\nExcludeforecasts\n\nticked : forecasts and backcasts from the RegARIMA model not used in Irregular Correction\nunticked (default): forecasts and backcasts used\n\n\n\n\n4.6.4.1.3 Seasonality extraction filters\n\nSeasonal filter choice\n\nSpecifies which be used to estimate the seasonal factors for the entire series (link to relevant part in M chapter)\n\nS3 × 1 – 3 × 1 moving average.\nS3 × 3 – 3 × 3 moving average.\nS3 × 5 – 3 × 5 moving average.\nS3 × 9 – 3 × 9 moving average.\nS3 × 15 – 3 × 15 moving average.\nStable – a single seasonal factor for each calendar period is generated by calculating a simple average over all values for each period (taken after detrending and outlier correction).\nX11Default – 3 × 3 moving average is used to calculate the initial seasonal factors and a 3 × 5 moving average to calculate the final seasonal factors.\nMsr – automatic choice of a seasonal filter. The seasonal filters can be selected for the entire series, or for a particular month or quarter.\ndefault value: Msr\n\nCheck: will user choice be applied to all steps or only to final phase D step\n\nDetails on seasonal filters\n\nSets different seasonal filters by period in order to account for seasonal heteroskedasticity (link to M chapter)\n\ndefault value: empty\n\n\n\n4.6.4.1.4 Trend estimation filters\n\nAutomatic Henderson filter our user-defined\n\ndefault: lentgh 13\nunticked: user defined length choice\n\nHenderson filter length choice\n\nvalues: odd number in in \\([3,101]\\)\ndefault value: 13\n\n\nCheck: will user choice be applied to all steps or only to final phase D step\n\n\n\n4.6.4.2 Parameter setting in GUI\nhere v2, adjust to v3 asap\n\n\n4.6.4.3 Parameter setting in R packages\nextensive help on functions available in package help pages Rcode snippet\nIn R, to implement any param change, it is required to retrieve current spec, modify it and apply it again (see T R packages chapter for details). (specific link)\nhere example changing all the settings (just remove irrelevant changes)\nRjdemetra (v2) Edit : here static R code and link to a “worked example” wih dynamic code\n\n\n\n\ncustomizing decomposition filters Henderson moving average (ma) length for trend Seasonal moving average (ma) length for seasonal component’s extraction\nstarting from spec_1 spec_6 <- x13_spec(spec_1,x11.trendma = 23,x11.seasonalma = “S3X9”)\nnew sa processing model_sa_6<-x13(serie_brute,spec_5)\nto access the current parameters s_decomp ?\n\nrjd3sa ?"
  },
  {
    "objectID": "A-sa.html#stl",
    "href": "A-sa.html#stl",
    "title": "4  Seasonal Adjustment",
    "section": "4.7 STL",
    "text": "4.7 STL"
  },
  {
    "objectID": "A-sa.html#seats",
    "href": "A-sa.html#seats",
    "title": "4  Seasonal Adjustment",
    "section": "4.8 SEATS",
    "text": "4.8 SEATS"
  },
  {
    "objectID": "A-sa.html#ssf",
    "href": "A-sa.html#ssf",
    "title": "4  Seasonal Adjustment",
    "section": "4.9 SSF",
    "text": "4.9 SSF"
  },
  {
    "objectID": "A-sa.html#quality-assessment",
    "href": "A-sa.html#quality-assessment",
    "title": "4  Seasonal Adjustment",
    "section": "4.10 Quality assessment",
    "text": "4.10 Quality assessment\n\n4.10.1 Residual seasonality\n\n\n4.10.2 Residual calendar effects"
  },
  {
    "objectID": "A-outlier-detection.html",
    "href": "A-outlier-detection.html",
    "title": "5  Outlier detection",
    "section": "",
    "text": "in or outside a seasonal adjustment process"
  },
  {
    "objectID": "A-outlier-detection.html#motivation",
    "href": "A-outlier-detection.html#motivation",
    "title": "5  Outlier detection",
    "section": "5.1 Motivation",
    "text": "5.1 Motivation"
  },
  {
    "objectID": "A-outlier-detection.html#with-reg-arima-models",
    "href": "A-outlier-detection.html#with-reg-arima-models",
    "title": "5  Outlier detection",
    "section": "5.2 With Reg Arima models",
    "text": "5.2 With Reg Arima models\n\n5.2.1 Part of preadjustment\n\n\n5.2.2 Specific TERROR tool"
  },
  {
    "objectID": "A-outlier-detection.html#with-structural-models",
    "href": "A-outlier-detection.html#with-structural-models",
    "title": "5  Outlier detection",
    "section": "5.3 With structural models",
    "text": "5.3 With structural models"
  },
  {
    "objectID": "A-calendar-and-input.html",
    "href": "A-calendar-and-input.html",
    "title": "6  Calendar and user-defined corrections",
    "section": "",
    "text": "generating Calendar regressors and other input variables"
  },
  {
    "objectID": "A-calendar-and-input.html#chapter-building-process",
    "href": "A-calendar-and-input.html#chapter-building-process",
    "title": "6  Calendar and user-defined corrections",
    "section": "6.1 Chapter building process",
    "text": "6.1 Chapter building process\n\n6.1.1 Edit content\n\nless dense content, less text more tables, bullet points\ncheck and add documents new v3 features, rjd3 modelling\n\n\n\n6.1.2 Overview of Calendar effects in JDemetra+\nThe following description of the calendar effects in JDemetra+ is strictly based on PALATE, J. (2014).\nA natural way for modelling calendar effects consists of distributing the days of each period into different groups. The regression variable corresponding to a type of day (a group) is simply defined by the number of days it contains for each period. Usual classifications are:\n\nTrading days (7 groups): each day of the week defines a group (Mondays,...,Sundays);\nWorking days (2 groups): week days and weekends.\n\nThe definition of a group could involve partial days. For instance, we could consider that one half of Saturdays belong to week days and the second half to weekends.\nUsually, specific holidays are handled as Sundays and they are included in the group corresponding to \"non-working days\". This approach assumes that the economic activity on national holidays is the same (or very close to) the level of activity that is typical for Sundays. Alternatively, specific holidays can be considered separately, e.g. by the specification that divided days into three groups:\n\nWorking days (Mondays to Fridays, except for specific holidays),\nNon-working days (Saturdays and Sundays, except for specific holidays),\nSpecific holidays.\n\n\n\n6.1.3 Summary of the method used in JDemetra+ to compute trading day and working day effects\nThe computation of trading day and working days effects is performed in four steps:\n\nComputation of the number of each weekday performed for all periods.\nCalculation of the usual contrast variables for trading day and working day.\nCorrection of the contrast variables with specific holidays (for each holiday add +1 to the number of Sundays and subtract 1 from the number of days of the holiday). The correction is not performed if the holiday falls on a Sunday, taking into account the validity period of the holiday.\nCorrection of the constant variables for long term mean effects, > taking into account the validity period of the holiday; see below > for the different cases.\n\nThe corrections of the constant variables may receive a weight corresponding to the part of the holiday considered as a Sunday.\nAn example below illustrates the application of the above algorithm for the hypothetical country in which three holidays are celebrated:\n\nNew Year (a fixed holiday, celebrated on 01 January);\nShrove Tuesday (a moving holiday, which falls 47 days before Easter Sunday, celebrated until the end of 2012);\nFreedom day (a fixed holiday, celebrated on 25 April).\n\nThe consecutive steps in calculation of the calendar for 2012 and 2013 years are explained below.\nFirst, the number of each day of the week in the given month is calculated as it is shown in table below.\nNumber of each weekday in different months\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonth\nMon\nTue\nWed\nThu\nFri\nSat\nSun\n\n\n\n\nJan-12\n5\n5\n4\n4\n4\n4\n5\n\n\nFeb-12\n4\n4\n5\n4\n4\n4\n4\n\n\nMar-12\n4\n4\n4\n5\n5\n5\n4\n\n\nApr-12\n5\n4\n4\n4\n4\n4\n5\n\n\nMay-12\n4\n5\n5\n5\n4\n4\n4\n\n\nJun-12\n4\n4\n4\n4\n5\n5\n4\n\n\nJul-12\n5\n5\n4\n4\n4\n4\n5\n\n\nAug-12\n4\n4\n5\n5\n5\n4\n4\n\n\nSep-12\n4\n4\n4\n4\n4\n5\n5\n\n\nOct-12\n5\n5\n5\n4\n4\n4\n4\n\n\nNov-12\n4\n4\n4\n5\n5\n4\n4\n\n\nDec-12\n5\n4\n4\n4\n4\n5\n5\n\n\nJan-13\n4\n5\n5\n5\n4\n4\n4\n\n\nFeb-13\n4\n4\n4\n4\n4\n4\n4\n\n\nMar-13\n4\n4\n4\n4\n5\n5\n5\n\n\nApr-13\n5\n5\n4\n4\n4\n4\n4\n\n\nMay-13\n4\n4\n5\n5\n5\n4\n4\n\n\nJun-13\n4\n4\n4\n4\n4\n5\n5\n\n\nJul-13\n5\n5\n5\n4\n4\n4\n4\n\n\nAug-13\n4\n4\n4\n5\n5\n5\n4\n\n\nSep-13\n5\n4\n4\n4\n4\n4\n5\n\n\nOct-13\n4\n5\n5\n5\n4\n4\n4\n\n\nNov-13\n4\n4\n4\n4\n5\n5\n4\n\n\nDec-13\n5\n5\n4\n4\n4\n4\n5\n\n\n\nNext, the contrast variables are calculated (table below) as a result of the linear transformation applied to the variables presented in table below.\nContrast variables (series corrected for leap year effects)\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonth\nMon\nTue\nWed\nThu\nFri\nSat\nLength\n\n\n\n\nJan-12\n0\n0\n-1\n-1\n-1\n-1\n0\n\n\nFeb-12\n0\n0\n1\n0\n0\n0\n0.75\n\n\nMar-12\n0\n0\n0\n1\n1\n1\n0\n\n\nApr-12\n0\n-1\n-1\n-1\n-1\n-1\n0\n\n\nMay-12\n0\n1\n1\n1\n0\n0\n0\n\n\nJun-12\n0\n0\n0\n0\n1\n1\n0\n\n\nJul-12\n0\n0\n-1\n-1\n-1\n-1\n0\n\n\nAug-12\n0\n0\n1\n1\n1\n0\n0\n\n\nSep-12\n-1\n-1\n-1\n-1\n-1\n0\n0\n\n\nOct-12\n1\n1\n1\n0\n0\n0\n0\n\n\nNov-12\n0\n0\n0\n1\n1\n0\n0\n\n\nDec-12\n0\n-1\n-1\n-1\n-1\n0\n0\n\n\nJan-13\n0\n1\n1\n1\n0\n0\n0\n\n\nFeb-13\n0\n0\n0\n0\n0\n0\n-0.25\n\n\nMar-13\n-1\n-1\n-1\n-1\n0\n0\n0\n\n\nApr-13\n1\n1\n0\n0\n0\n0\n0\n\n\nMay-13\n0\n0\n1\n1\n1\n0\n0\n\n\nJun-13\n-1\n-1\n-1\n-1\n-1\n0\n0\n\n\nJul-13\n1\n1\n1\n0\n0\n0\n0\n\n\nAug-13\n0\n0\n0\n1\n1\n1\n0\n\n\nSep-13\n0\n-1\n-1\n-1\n-1\n-1\n0\n\n\nOct-13\n0\n1\n1\n1\n0\n0\n0\n\n\nNov-13\n0\n0\n0\n0\n1\n1\n0\n\n\nDec-13\n5\n5\n4\n4\n4\n4\n0\n\n\n\nIn the next step the corrections for holidays is done in the following way:\n\nNew Year: In 2012 it falls on a Sunday. Therefore no correction is applied. In 2013 it falls on a Tuesday. Consequently, the following corrections are applied to the number of each weekday in January: Tuesday -1, Sunday +1, so the following corrections are applied to the contrast variables: -2 for Tuesday and -1 for the other contrast variables.\nShrove Tuesday: It is a fixed day of the week holiday that always falls on Tuesday. For this reason in 2012 the following corrections are applied to the number of each weekday in February: Tuesday -1, Sunday +1, so the following corrections are applied to the contrast variables: -2 for the contrast variable associated with Tuesday, and -1 for the other contrast variables. The holiday expires at the end of 2012. Therefore no corrections are made for 2013.\nFreedom Day: In 2012 it falls on a Wednesday. Consequently, the following corrections are applied to the number of each weekday in April: Wednesday -1, Sunday +1, so the following corrections are applied to the contrast variables: -2 for Wednesday and -1 for the other contrast variables. In 2013 it falls on Thursday. Therefore, the following corrections are applied to the number of each weekday in April: Thursday -1, Sunday +1, so the following corrections are applied to the contrast variables: -2 for Thursday, and -1 for the other contrast variables.\n\nThe result of these corrections is presented in table below.\nContrast variables corrected for holidays\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonth\nMon\nTue\nWed\nThu\nFri\nSat\nLength\n\n\n\n\nJan-12\n0\n0\n-1\n-1\n-1\n-1\n0\n\n\nFeb-12\n-1\n-2\n0\n-1\n-1\n-1\n0.75\n\n\nMar-12\n0\n0\n0\n1\n1\n1\n0\n\n\nApr-12\n-1\n-2\n-3\n-2\n-2\n-2\n0\n\n\nMay-12\n0\n1\n1\n1\n0\n0\n0\n\n\nJun-12\n0\n0\n0\n0\n1\n1\n0\n\n\nJul-12\n0\n0\n-1\n-1\n-1\n-1\n0\n\n\nAug-12\n0\n0\n1\n1\n1\n0\n0\n\n\nSep-12\n-1\n-1\n-1\n-1\n-1\n0\n0\n\n\nOct-12\n1\n1\n1\n0\n0\n0\n0\n\n\nNov-12\n0\n0\n0\n1\n1\n0\n0\n\n\nDec-12\n0\n-1\n-1\n-1\n-1\n0\n0\n\n\nJan-13\n-1\n-1\n0\n0\n-1\n-1\n0\n\n\nFeb-13\n0\n0\n0\n0\n0\n0\n-0.25\n\n\nMar-13\n-1\n-1\n-1\n-1\n0\n0\n0\n\n\nApr-13\n0\n0\n-1\n-2\n-1\n-1\n0\n\n\nMay-13\n0\n0\n1\n1\n1\n0\n0\n\n\nJun-13\n-1\n-1\n-1\n-1\n-1\n0\n0\n\n\nJul-13\n1\n1\n1\n0\n0\n0\n0\n\n\nAug-13\n0\n0\n0\n1\n1\n1\n0\n\n\nSep-13\n0\n-1\n-1\n-1\n-1\n-1\n0\n\n\nOct-13\n0\n1\n1\n1\n0\n0\n0\n\n\nNov-13\n0\n0\n0\n0\n1\n1\n0\n\n\nDec-13\n0\n0\n-1\n-1\n-1\n-1\n0\n\n\n\nFinally, the long term corrections are applied on each year of the validity period of the holiday.\n\nNew Year: Correction on the contrasts: +1, to be applied to January of 2012 and 2013.\nShrove Tuesday: It may fall either in February or in March. It will fall in March if Easter is on or after 17 April. Taking into account the theoretical distribution of Easter, it gives: prob(March) = +0.22147, prob(February) = +0.77853. The correction of the contrasts will be +1.55707 for Tuesday in February 2012 and +0.77853 for the other contrast variables. The correction of the contrasts will be +0.44293 for Tuesday in March 2012, +0.22147 for the other contrast variables.\nFreedom Day: Correction on the contrasts: +1, to be applied to April of 2012 and 2013.\n\nThe modifications due to the corrections described above are presented in table below.\nTrading day variables corrected for the long term effects\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonth\nMon\nTue\nWed\nThu\nFri\nSat\nLength\n\n\n\n\nJan-12\n1\n1\n0\n0\n0\n0\n0\n\n\nFeb-12\n-0.22115\n-0.44229\n0.778853\n-0.22115\n-0.22115\n-0.22115\n0.75\n\n\nMar-12\n0.221147\n0.442293\n0.221147\n1.221147\n1.221147\n1.221147\n0\n\n\nApr-12\n0\n-1\n-2\n-1\n-1\n-1\n0\n\n\nMay-12\n0\n1\n1\n1\n0\n0\n0\n\n\nJun-12\n0\n0\n0\n0\n1\n1\n0\n\n\nJul-12\n0\n0\n-1\n-1\n-1\n-1\n0\n\n\nAug-12\n0\n0\n1\n1\n1\n0\n0\n\n\nSep-12\n-1\n-1\n-1\n-1\n-1\n0\n0\n\n\nOct-12\n1\n1\n1\n0\n0\n0\n0\n\n\nNov-12\n0\n0\n0\n1\n1\n0\n0\n\n\nDec-12\n0\n-1\n-1\n-1\n-1\n0\n0\n\n\nJan-13\n0\n0\n1\n1\n0\n0\n0\n\n\nFeb-13\n0\n0\n0\n0\n0\n0\n-0.25\n\n\nMar-13\n-1\n-1\n-1\n-1\n0\n0\n0\n\n\nApr-13\n1\n1\n0\n-1\n0\n0\n0\n\n\nMay-13\n0\n0\n1\n1\n1\n0\n0\n\n\nJun-13\n-1\n-1\n-1\n-1\n-1\n0\n0\n\n\nJul-13\n1\n1\n1\n0\n0\n0\n0\n\n\nAug-13\n0\n0\n0\n1\n1\n1\n0\n\n\nSep-13\n0\n-1\n-1\n-1\n-1\n-1\n0\n\n\nOct-13\n0\n1\n1\n1\n0\n0\n0\n\n\nNov-13\n0\n0\n0\n0\n1\n1\n0\n\n\nDec-13\n0\n0\n-1\n-1\n-1\n-1\n0\n\n\n\n\n\n6.1.4 Mean and seasonal effects of calendar variables\nThe calendar effects produced by the regression variables that fulfil the definition presented above include a mean effect (i.e. an effect that is independent of the period) and a seasonal effect (i.e. an effect that is dependent of the period and on average it is equal to 0). Such an outcome is inappropriate, as in the usual decomposition of a series the mean effect should be allocated to the trend component and the fixed seasonal effect should be affected to the corresponding component. Therefore, the actual calendar effect should only contain effects that don't belong to the other components.\nIn the context of JDemetra+ the mean effect and the seasonal effect are long term theoretical effects rather than the effects computed on the time span of the considered series (which should be continuously revised).\nThe mean effect of a calendar variable is the average number of days in its group. Taking into account that one year has on average 365.25 days, the monthly mean effects for a working days are, as shown in the table below, 21.7411 for week days and 8.696 for weekends.\nMonthly mean effects for the Working day variable\n\n\n\nGroups of Working day effect\nMean effect\n\n\n\n\nWeek days\n365.25/12*5/7 = 21.7411\n\n\nWeekends\n365.25/12*2/7 = 8.696\n\n\nTotal\n365.25/12 = 30.4375\n\n\n\nThe number of days by period is highly seasonal, as apart from February, the length of each month is the same every year. For this reason, any set of calendar variables will contain, at least in some variables, a significant seasonal effect, which is defined as the average number of days by period (Januaries..., first quarters...) outside the mean effect. Removing that fixed seasonal effects consists of removing for each period the long term average of days that belong to it. The calculation of a seasonal effect for the working days classification is presented in the table below.\nThe mean effect and the seasonal effect for the calendar periods\n\n\n\n\n\n\n\n\n\n\nPeriod\nAverage number of days\nAverage number of week days\nMean effect\nSeasonal effect\n\n\n\n\nJanuary\n31\n31*5/7=22.1429\n21.7411\n0.4018\n\n\nFebruary\n28.25\n28.25*5/7=20.1786\n21.7411\n-1.5625\n\n\nMarch\n31\n31*5/7=22.1429\n21.7411\n0.4018\n\n\nApril\n30\n30*5/7=21.4286\n21.7411\n-0.3125\n\n\nMay\n31\n31*5/7=22.1429\n21.7411\n0.4018\n\n\nJune\n30\n30*5/7=21.4286\n21.7411\n-0.3125\n\n\nJuly\n31\n31*5/7=22.1429\n21.7411\n0.4018\n\n\nAugust\n31\n31*5/7=22.1429\n21.7411\n0.4018\n\n\nSeptember\n30\n30*5/7=21.4286\n21.7411\n-0.3125\n\n\nOctober\n31\n31*5/7=22.1429\n21.7411\n0.4018\n\n\nNovember\n30\n30*5/7=21.4286\n21.7411\n-0.3125\n\n\nDecember\n31\n31*5/7=22.1429\n21.7411\n0.4018\n\n\nTotal\n365.25\n260.8929\n260.8929\n0\n\n\n\nFor a given time span, the actual calendar effect for week days can be easily calculated as the difference between the number of week days in a specific period and the sum of the mean effect and the seasonal effect assigned to this period, as it is shown in the table below for the period 01.2013 – 06.2013.\nThe calendar effect for the period 01.2013 - 06.2013\n\n\n\n\n\n\n\n\n\n\nTime period (t)\nWeek days\nMean effect\nSeasonal effect\nCalendar effect\n\n\n\n\nJan-2013\n23\n21.7411\n0.4018\n0.8571\n\n\nFeb-2013\n20\n21.7411\n-1.5625\n-0.1786\n\n\nMar-2013\n21\n21.7411\n0.4018\n-1.1429\n\n\nApr-2013\n22\n21.7411\n-0.3125\n0.5714\n\n\nMay-2013\n23\n21.7411\n0.4018\n0.8571\n\n\nJun-2013\n20\n21.7411\n-0.3125\n-1.4286\n\n\nJul-2013\n23\n21.7411\n0.4018\n0.8571\n\n\n\nThe distinction between the mean effect and the seasonal effect is usually unnecessary. Those effects can be considered together (simply called mean effects) and be computed by removing from each calendar variable its average number of days by period. These global means effect are considered in the next section.\n\n\n6.1.5 Impact of the mean effects on the decomposition\nWhen the ARIMA model contains a seasonal difference – something that should always happen with calendar variables – the mean effects contained in the calendar variables are automatically eliminated, so that they don't modify the estimation. The model is indeed estimated on the series/regression variables after differencing. However, they lead to a different linearised series (\\(y_{\\text{lin}})\\). The impact of other corrections (mean and/or fixed seasonal) on the decomposition is presented in the next paragraph. Such corrections could be obtained, for instance, by applying other solutions for the long term corrections or by computing them on the time span of the series.\nNow the model with \"correct\" calendar effects (denoted as \\(C\\)), i.e. effects without mean and fixed seasonal effects, can be considered. To simplify the problem, the model has no other regression effects.\nFor such a model the following relations hold:\n\\[y_{\\text{lin}} = \\ y - C\\]\n\\[T = \\ F_{T}\\left( y_{\\text{lin}} \\right)\\]\n\\[S = \\ F_{S}\\left( y_{\\text{lin}} \\right) + C\\]\n\\[I = \\ F_{I}\\left( y_{\\text{lin}} \\right)\\]\nwhere:\nT - the trend;\nS - the seasonal component;\nI - the irregular component;\n\\(F_{X}\\) - the linear filter for the component X.\nConsider next other calendar effects (\\(\\widetilde{C}\\)) that contain some mean (\\(\\text{cm}\\), integrated to the final trend) and fixed seasonal effects (\\(\\text{cs}\\), integrated to the final seasonal). The modified equations are now:\n\\[\\widetilde{C} = C + cm + cs\\]\n\\[{\\widetilde{y}}_{\\text{lin}} = \\ y - \\widetilde{C} = \\ y_{\\text{lin}} - cm - cs\\]\n\\[\\widetilde{T} = \\ F_{T}\\left( {\\widetilde{y}}_{\\text{lin}} \\right) + cm\\]\n\\[\\widetilde{S} = \\ F_{S}\\left( {\\widetilde{y}}_{\\text{lin}} \\right) + C + cs\\]\n\\[\\widetilde{I} = \\ F_{I}\\left( {\\widetilde{y}}_{\\text{lin}} \\right)\\]\nTaking into account that \\(F_{X}\\) is a linear transformation and that1\n\\[F_{T}\\left( \\text{cm} \\right) = cm\\]\n\\[F_{T}\\left( \\text{cs} \\right) = 0\\]\n\\[F_{S}\\left( \\text{cm} \\right) = 0\\ \\]\n\\[F_{S}\\left( \\text{cs} \\right) = cs\\]\n\\[F_{I}\\left( \\text{cm} \\right) = 0\\]\n\\[F_{I}\\left( \\text{cs} \\right) = 0\\]\nThe following relationships hold:\n\\[\\widetilde{T} = \\ F_{T}\\left( {\\widetilde{y}}_{\\text{lin}} \\right) + cm = F_{T}\\left( y_{\\text{lin}} \\right) - cm + cm = T\\]\n\\[\\widetilde{S} = \\ F_{S}\\left( {\\widetilde{y}}_{\\text{lin}} \\right) + C + cs = F_{S}\\left( y_{\\text{lin}} \\right) - cs + C + cs = S\\]\n\\[\\widetilde{I} = \\ I\\]\nIf we don’t take into account the effects and apply the same approach as in the “correct” calendar effects, we will get:\n\\[\\breve{T} = \\ F_{T}\\left( {\\widetilde{y}}_{\\text{lin}} \\right) = T - cm\\]\n\\[\\breve{S} = \\ F_{S}\\left( {\\widetilde{y}}_{\\text{lin}} \\right) + \\widetilde{C} = S + cm\\]\n\\[\\breve{I} = \\ F_{I}\\left( {\\widetilde{y}}_{\\text{lin}} \\right) = I\\]\nThe trend, seasonal and seasonally adjusted series will only differ by a (usually small) constant.\nIn summary, the decomposition does not depend on the mean and fixed seasonal effects used for the calendar effects, provided that those effects are integrated in the corresponding final components. If these corrections are not taken into account, the main series of the decomposition will only differ by a constant.\n\n\n6.1.6 Linear transformations of the calendar variables\nAs far as the RegARIMA and the TRAMO models are considered, any non-degenerated linear transformation of the calendar variables can be used. It will produce the same results (likelihood, residuals, parameters, joint effect of the calendar variables, joint F-test on the coefficients of the calendar variables…). The linearised series that will be further decomposed is invariant to any linear transformation of the calendar variables.\nHowever, it should be mentioned that choices of calendar corrections based on the tests on the individual t statistics are dependent on the transformation, which is rather arbitrary. This is the case in old versions of TRAMO-SEATS. That is why the joint F-test (as in the version of TRAMO-SEATS implemented in TSW+) should be preferred.\nAn example of a linear transformation is the calculation of the contrast variables. In the case of the usual trading day variables, they are defined by the following transformation: the 6 contrast variables (\\(\\text{No.}\\left( \\text{Mondays} \\right) - No.\\left( \\text{Sundays} \\right),\\ldots No.\\left( \\text{Saturdays} \\right) - No.(Sundays)\\)) used with the length of period.\n\\[\\begin{bmatrix}                 \n  1 & 0 & 0 & 0 & 0 & 0 & - 1 \\\\    \n  0 & 1 & 0 & 0 & 0 & 0 & - 1 \\\\    \n  0 & 0 & 1 & 0 & 0 & 0 & - 1 \\\\    \n  0 & 0 & 0 & 1 & 0 & 0 & - 1 \\\\    \n  0 & 0 & 0 & 0 & 1 & 0 & - 1 \\\\    \n  0 & 0 & 0 & 0 & 0 & 1 & - 1 \\\\    \n  1 & 1 & 1 & 1 & 1 & 1 & 1 \\\\      \n  \\end{bmatrix}\\begin{bmatrix}      \n  \\text{Mon} \\\\                     \n  \\text{Tue} \\\\                     \n  \\text{Wed} \\\\                     \n  \\text{Thu} \\\\                     \n  \\text{Fri} \\\\                     \n  \\text{Sat} \\\\                     \n  \\text{Sun} \\\\                     \n  \\end{bmatrix} = \\begin{bmatrix}   \n  Mon - Sun \\\\                      \n  Tue - Sun \\\\                      \n  Wed - Sun \\\\                      \n  Thu - Sun \\\\                      \n  Fri - Sun \\\\                      \n  Sat - Sun \\\\                      \n  \\text{Length of period} \\\\      \n  \\end{bmatrix}\\]\nFor the usual working day variables, two variables are used: one contrast variable and the length of period\n\\[\\begin{bmatrix}                  \n  1 & - \\frac{5}{2} \\\\              \n  1 & 1 \\\\                          \n  \\end{bmatrix}\\begin{bmatrix}      \n  \\text{Week} \\\\                    \n  \\text{Weekend} \\\\                 \n  \\end{bmatrix} = \\begin{bmatrix}   \n  \\text{Contrast week} \\\\          \n  \\text{Length of period} \\\\      \n  \\end{bmatrix}\\]\nThe \\(\\text{Length of period}\\) variable is defined as a deviation from the length of the month (in days) and the average month length, which is equal to \\(30.4375.\\) Instead, the leap-year variable can be used here (see Regression sections in RegARIMA or Tramo)2.\nSuch transformations have several advantages. They suppress from the contrast variables the mean and the seasonal effects, which are concentrated in the last variable. So, they lead to fewer correlated variables, which are more appropriate to be included in the regression model. The sum of the effects of each day of the week estimated with the trading (working) day contrast variables cancel out.\n\n\n6.1.7 Handling of specific holidays\ncheck vs GUI (v3) and rjd3 modelling\nThree types of holidays are implemented in JDemetra+:\n\nFixed days, corresponding to the fixed dates in the year (e.g. New Year, Christmas).\nEaster related days, corresponding to the days that are defined in relation to Easter (e.g. Easter +/- n days; example: Ascension, Pentecost).\nFixed week days, corresponding to the fixed days in a given week of a given month (e.g. Labor Day celebrated in the USA on the first Monday of September).\n\nFrom a conceptual point of view, specific holidays are handled in exactly the same way as the other days. It should be decided, however, to which group of days they belong. Usually they are handled as Sundays. This convention is also used in JDemetra+. Therefore, except if the holiday falls on a Sunday, the appearance of a holiday leads to correction in two groups, i.e. in the group that contains the weekday, in which holiday falls, and the group that contains the Sundays.\nCountry specific holidays have an impact on the mean and the seasonal effects of calendar effects. Therefore, the appropriate corrections to the number of particular days (which are usually the basis for the definition of other calendar variables) should be applied, following the kind of holidays. These corrections are applied to the period(s) that may contain the holiday. The long term corrections in JDemetra+ don't take into account the fact that some moving holidays could fall on the same day (for instance the May Day and the Ascension). However, those events are exceptional, and their impact on the final result is usually not significant.\n\n6.1.7.1 Fixed day\nThe probability that the holiday falls on a given day of the week is 1/7. Therefore, the probability to have 1 day more that is treated like Sunday is 6/7. The effect on the means for the period that contains the fixed day is presented in the table below (the correction on the calendar effect has the opposite sign).\nThe effect of the fixed holiday on the period, in which it occurred\n\n\n\nSundays\nOthers days\nContrast variables\n\n\n\n\n+ 6/7\n- 1/7\n1/7 - (+ 6/7)= -1\n\n\n\n\n\n6.1.7.2 Easter related days\nEaster related days always fall the same week day (denoted as Y in the table below: The effects of the Easter Sunday on the seasonal means). However, they can fall during different periods (months or quarters). Suppose that, taking into account the distribution of the dates for Easter and the fact that this holiday falls in one of two periods, the probability that Easter falls during the period \\(m\\) is \\(p\\), which implies that the probability that it falls in the period \\(m + 1\\) is \\(1 - p\\). The effects of Easter on the seasonal means are presented in the table below.\nThe effects of the Easter Sunday on the seasonal means\n|Period | Sundays Days X Others days Contrast Y Other contrasts |————| ————- ———— —————– ——————- ——————— |m |+ p - p 0 - 2p - p |m+1 | + (1-p) - (1-p) 0 - 2\\(\\times\\)(1-p) - (1-p)\nThe distribution of the dates for Easter may be approximated in different ways. One of the solutions consists of using some well-known algorithms for computing Easter on a very long period. JDemetra+ provides the Meeus/Jones/Butcher's and the Ron Mallen's algorithms (they are identical till year 4100, but they slightly differ after that date). Another approach consists in deriving a raw theoretical distribution based on the definition of Easter. It is the solution used for Easter related days. It is shortly explained below.\nThe date of Easter in the given year is the first Sunday after the full moon (the Paschal Full Moon) following the northern hemisphere's vernal equinox. The definition is influenced by the Christian tradition, according to which the equinox is reckoned to be on 21 March3 and the full moon is not necessarily the astronomically correct date. However, when the full moon falls on Sunday, then Easter is delayed by one week. With this definition, the date of Easter Sunday varies between 22 March and 25 April. Taking into account that an average lunar month is \\(29.530595\\) days the approximated distribution of Easter can be derived. These calculations do not take into account the actual ecclesiastical moon calendar.\nFor example, the probability that Easter Sunday falls on 25 March is 0.004838 and results from the facts that the probability that 25 March falls on a Sunday is \\(1/7\\) and the probability that the full moon is on 21 March, 22 March, 23 March or 24 March is \\(5/29.53059\\). The probability that Easter falls on 24 April is 0.01708 and results from the fact that the probability that 24 April is Sunday is \\(1/7\\) and takes into account that 18 April is the last acceptable date for the full moon. Therefore the probability that the full moon is on 16 April or 17 April is \\(1/29.53059\\) and the probability that the full moon is on 18 April is \\(1.53059/29.53059\\).\nThe approximated distribution of Easter dates\n\n\n\nDay\nProbability\n\n\n\n\n22 March\n1/7 * 1/29.53059\n\n\n23 March\n1/7 * 2/29.53059\n\n\n24 March\n1/7 * 3/29.53059\n\n\n25 March\n1/7 * 4/29.53059\n\n\n26 March\n1/7 * 5/29.53059\n\n\n27 March\n1/7 * 6/29.53059\n\n\n28 March\n1/29.53059\n\n\n29 March\n1/29.53059\n\n\n…\n…\n\n\n18 April\n1/29.53059\n\n\n19 April\n1/7 * (6 + 1.53059)/29.53059\n\n\n20 April\n1/7 * (5 + 1.53059)/29.53059\n\n\n21 April\n1/7 * (4 + 1.53059)/29.53059\n\n\n22 April\n1/7 * (3 + 1.53059)/29.53059\n\n\n23 April\n1/7 * (2 + 1.53059)/29.53059\n\n\n24 April\n1/7 * (1 + 1.53059)/29.53059\n\n\n25 April\n1/7 * 1.53059/29.53059\n\n\n\n\n\n6.1.7.3 Fixed week days\nFixed week days always fall on the same week day (denoted as Y in the table below) and in the same period. Their effect on the seasonal means is presented in the table below.\nThe effect of the fixed week holiday on the period, in which it occurred\n\n\n\nSundays\nDay Y\nOthers days\n\n\n\n\n+ 1\n- 1\n0\n\n\n\nThe impact of fixed week days on the regression variables is zero because the effect itself is compensated by the correction for the mean effect.\n\n\n\n6.1.8 Holidays with a validity period\nWhen a holiday is valid only for a given time span, JDemetra+ applies the long term mean corrections only on the corresponding period. However, those corrections are computed in the same way as in the general case.\nIt is important to note that using or not using mean corrections will impact in the estimation of the RegARIMA and TRAMO models. Indeed, the mean corrections do not disappear after differencing. The differences between the SA series computed with or without mean corrections will no longer be constant.\n\n\n6.1.9 Different Kinds of calendars\nsee link with GUI\nThis scenario presents how to define different kinds of calendars. These calendars can be applied to the specifications that take into account country-specific holidays and can be used for detecting and estimating the calendar effects.\nThe calendar effects are those parts of the movements in the time series that are caused by different number of weekdays in calendar months (or quarters). They arise as the number of occurrences of each day of the week in a month (or a quarter) differs from year to year. These differences cause regular effects in some series. In particular, such variation is caused by a leap year effect because of an extra day inserted into February every four years. As with seasonal effects, it is desirable to estimate and remove calendar effects from the time series.\nThe calendar effects can be divided into a mean effect, a seasonal part and a structural part. The mean effect is independent from the period and therefore should be allocated to the trend-cycle. The seasonal part arises from the properties of the calendar that recur each year. For one thing, the number of working days of months with 31 calendar days is on average larger than that of months with 30 calendar days. This effect is part of the seasonal pattern captured by the seasonal component (with the exception of leap year effects). The structural part of the calendar effect remains to be determined by the calendar adjustment. For example, the number of working days of the same month in different years varies from year to year.\nBoth X-12-ARIMA/X-13ARIMA-SEATS and TRAMO/SEATS estimate calendar effects by adding some regressors to the equation estimated in the pre-processing part (RegARIMA or TRAMO, respectively). Regressors mentioned above are generated from the default calendar or the user defined calendar.\nThe calendars of JDemetra+ simply correspond to the usual trading days contrast variables based on the Gregorian calendar, modified to take into account some specific holidays. Those holidays are handled as \"Sundays\" and the variables are properly adjusted to take into account the long term mean effects.\n\n\n6.1.10 Tests for residual trading days\nWe consider below tests on the seasonally adjusted series (\\(sa_t\\)) or on the irregular component (\\(irr_t\\)). When the reasoning applies on both components, we will use \\(y_t\\). The functions \\(stdev\\) stands for “standard deviation” and \\(rms\\) for “root mean squares”\nThe tests are computed on the log-transformed components in the case of multiplicative decomposition.\nTD are the usual contrasts of trading days, 6 variables (no specific calendar).\n\n6.1.10.1 Non significant irregular\nWhen \\(irr_t\\) is not significant, we don’t compute the test on it, to avoid irrelevant results. We consider that \\(irr_t\\) is significant if \\(stdev( irr_t)>0.01\\) (multiplicative case) or if \\(stdev(irr_t)/rms(sa_t) >0.01\\) (additive case).\n\n\n6.1.10.2 F test\nThe test is the usual joint F-test on the TD coefficients, computed on the following models:\n\n6.1.10.2.1 Autoregressive model (AR modelling option)\nWe compute by OLS:\n\\[y_t=\\mu + \\alpha y_{t-1} + \\beta TD_t + \\epsilon_t \\]\n\n\n6.1.10.2.2 Difference model\nWe compute by OLS:\n\\[\\Delta y_t - \\overline{\\Delta y_t}=\\beta TD_t + \\epsilon_t \\]\nSo, the latter model is a restriction of the first one (\\(\\alpha =1, \\mu =μ=\\overline{\\Delta y_t}\\))\nThe tests are the usual joint F-tests on \\(\\beta \\quad (H_0:\\beta=0)\\).\nBy default, we compute the tests on the 8 last years of the components, so that they might highlight moving calendar effects.\nRemark:\nIn Tramo, a similar test is computed on the residuals of the Arima model. More exactly, the F-test is computed on \\(e_t=\\beta TD_t + \\epsilon_t\\), where \\(e_t\\) are the one-step-ahead forecast errors."
  },
  {
    "objectID": "A-sa-hf.html#tools",
    "href": "A-sa-hf.html#tools",
    "title": "7  Seasonal adjustment of high frequency data",
    "section": "7.2 Tools",
    "text": "7.2 Tools\ncode here and/or link to R packages chapter"
  },
  {
    "objectID": "A-sa-hf.html#unobserved-components",
    "href": "A-sa-hf.html#unobserved-components",
    "title": "7  Seasonal adjustment of high frequency data",
    "section": "7.3 Unobserved Components",
    "text": "7.3 Unobserved Components"
  },
  {
    "objectID": "A-sa-hf.html#identifying-seasonal-patterns",
    "href": "A-sa-hf.html#identifying-seasonal-patterns",
    "title": "7  Seasonal adjustment of high frequency data",
    "section": "7.4 Identifying seasonal patterns",
    "text": "7.4 Identifying seasonal patterns\n\n7.4.1 Spectral analysis\n\n\n7.4.2 Seasonality tests"
  },
  {
    "objectID": "A-sa-hf.html#pre-adjustment",
    "href": "A-sa-hf.html#pre-adjustment",
    "title": "7  Seasonal adjustment of high frequency data",
    "section": "7.5 Pre-adjustment",
    "text": "7.5 Pre-adjustment\n\n7.5.1 Calendar correction\n\n\n7.5.2 Outliers and intervention variables\n\n\n7.5.3 Linearization"
  },
  {
    "objectID": "A-sa-hf.html#decomposition",
    "href": "A-sa-hf.html#decomposition",
    "title": "7  Seasonal adjustment of high frequency data",
    "section": "7.6 Decomposition",
    "text": "7.6 Decomposition\n\n7.6.1 Extended X-11\n\n\n7.6.2 STL decomposition\n\n\n7.6.3 Arima Model Based (AMB) Decomposition\n\n\n7.6.4 State Space framework"
  },
  {
    "objectID": "A-sa-hf.html#quality-assessment",
    "href": "A-sa-hf.html#quality-assessment",
    "title": "7  Seasonal adjustment of high frequency data",
    "section": "7.7 Quality assessment",
    "text": "7.7 Quality assessment\n\n7.7.1 Residual seasonality\n\n\n7.7.2 Residual Calendar effects"
  },
  {
    "objectID": "A-benchmarking.html",
    "href": "A-benchmarking.html",
    "title": "8  Benchmarking and temporal disagreggation",
    "section": "",
    "text": "Often one has two (or multiple) datasets of different frequency for the same target variable. Sometimes, however, these data sets are not coherent in the sense that they don’t match up. Benchmarking[^1] is a method to deal with this situation. An aggregate of a higher-frequency measurement variables is not necessarily equal to the corresponding lower-frequency less-aggregated measurement. Moreover, the sources of data may have different reliability levels. Usually, less frequent data are considered more trustworthy as they are based on larger samples and compiled more precisely. The more reliable measurements, hence often the less frequent, will serve as benchmark.\nIn seasonal adjustment methods benchmarking is the procedure that ensures the consistency over the year between adjusted and non-seasonally adjusted data. It should be noted that the [ESS Guidelines on Seasonal Adjustment (2015)] (https://ec.europa.eu/eurostat/documents/3859598/6830795/KS-GQ-15-001-EN-N.pdf/d8f1e5f5-251b-4a69-93e3-079031b74bd3), do not recommend benchmarking as it introduces a bias in the seasonally adjusted data. The U.S. Census Bureau also points out that “forcing the seasonal adjustment totals to be the same as the original series annual totals can degrade the quality of the seasonal adjustment, especially when the seasonal pattern is undergoing change. It is not natural if trading day adjustment is performed because the aggregate trading day effect over a year is variable and moderately different from zero”[^2]. Nevertheless, some users may need that the annual totals of the seasonally adjusted series match the annual totals of the original, non-seasonally adjusted series[^3].\nAccording to the [ESS Guidelines on Seasonal Adjustment (2015)] (https://ec.europa.eu/eurostat/documents/3859598/6830795/KS-GQ-15-001-EN-N.pdf/d8f1e5f5-251b-4a69-93e3-079031b74bd3), the only benefit of this approach is that there is consistency over the year between adjusted and the non-seasonally adjusted data; this can be of particular interest when low-frequency (e.g. annual) benchmarking figures officially exist (e.g. National Accounts, Balance of Payments, External Trade, etc.) and where users’ needs for time consistency are stronger."
  },
  {
    "objectID": "A-benchmarking.html#underlying-theory",
    "href": "A-benchmarking.html#underlying-theory",
    "title": "8  Benchmarking and temporal disagreggation",
    "section": "8.2 Underlying Theory",
    "text": "8.2 Underlying Theory\nBenchmarking1 is a procedure widely used when for the same target variable the two or more sources of data with different frequency are available. Generally, the two sources of data rarely agree, as an aggregate of higher-frequency measurements is not necessarily equal to the less-aggregated measurement. Moreover, the sources of data may have different reliability. Usually it is thought that less frequent data are more trustworthy as they are based on larger samples and compiled more precisely. The more reliable measurement is considered as a benchmark.\nBenchmarking also occurs in the context of seasonal adjustment. Seasonal adjustment causes discrepancies between the annual totals of the seasonally unadjusted (raw) and the corresponding annual totals of the seasonally adjusted series. Therefore, seasonally adjusted series are benchmarked to the annual totals of the raw time series2. Therefore, in such a case benchmarking means the procedure that ensures the consistency over the year between adjusted and non-seasonally adjusted data. It should be noted that the ‘ESS Guidelines on Seasonal Adjustment’ (2015) do not recommend benchmarking as it introduces a bias in the seasonally adjusted data. Also the U.S. Census Bureau points out that: Forcing the seasonal adjustment totals to be the same as the original series annual totals can degrade the quality of the seasonal adjustment, especially when the seasonal pattern is undergoing change. It is not natural if trading day adjustment is performed because the aggregate trading day effect over a year is variable and moderately different from zero.3 Nevertheless, some users may prefer the annual totals for the seasonally adjusted series to match the annual totals for the original, non-seasonally adjusted series4. According to the ‘ESS Guidelines on Seasonal Adjustment’ (2015), the only benefit of this approach is that there is consistency over the year between adjusted and non-seasonally adjusted data; this can be of particular interest when low-frequency (e.g. annual) benchmarking figures officially exist (e.g. National Accounts, Balance of Payments, External Trade, etc.) where user needs for time consistency are stronger.\nThe benchmarking procedure in JDemetra+ is available for a single seasonally adjusted series and for an indirect seasonal adjustment of an aggregated series. In the first case, univariate benchmarking ensures consistency between the raw and seasonally adjusted series. In the second case, the multivariate benchmarking aims for consistency between the seasonally adjusted aggregate and its seasonally adjusted components.\nGiven a set of initial time series \\[\\left\\{ z_{i,t} \\right\\}_{i \\in I}\\], the aim of the benchmarking procedure is to find the corresponding \\[\\left\\{ x_{i,t} \\right\\}_{i \\in I}\\] that respect temporal aggregation constraints, represented by \\[X_{i,T} = \\sum_{t \\in T}^{}x_{i,t}\\] and contemporaneous constraints given by \\[q_{k,t} = \\sum_{j \\in J_{k}}^{}{w_{\\text{kj}}x_{j,t}}\\] or, in matrix form: \\[q_{k,t} = w_{k}x_{t}\\].\nThe underlying benchmarking method implemented in JDemetra+ is an extension of Cholette’s5 method, which generalises, amongst others, the additive and the multiplicative Denton procedure as well as simple proportional benchmarking.\nThe JDemetra+ solution uses the following routines that are described in DURBIN, J., and KOOPMAN, S.J. (2001):\n\nThe multivariate model is handled through its univariate transformation,\nThe smoothed states are computed by means of the disturbance smoother.\n\nThe performance of the resulting algorithm is highly dependent on the number of variables involved in the model (\\(\\propto \\ n^{3}\\)). The other components of the problem (number of constraints, frequency of the series, and length of the series) are much less important (\\(\\propto \\ n\\)).\nFrom a theoretical point of view, it should be noted that this approach may handle any set of linear restrictions (equalities), endogenous (between variables) or exogenous (related to external values), provided that they don’t contain incompatible equations. The restrictions can also be relaxed for any period by considering their “observation” as missing. However, in practice, it appears that several kinds of contemporaneous constraints yield unstable results. This is more especially true for constraints that contain differences (which is the case for non-binding constraints). The use of a special square root initialiser improves in a significant way the stability of the algorithm."
  },
  {
    "objectID": "A-benchmarking.html#tools",
    "href": "A-benchmarking.html#tools",
    "title": "8  Benchmarking and temporal disagreggation",
    "section": "8.3 Tools",
    "text": "8.3 Tools\n\n8.3.1 Benchmarking with GUI\n\nWith the pre-defined specifcations the benchmarking functionality is not applied by default following the ESS Guidelines on Seasonal Adjustment (2015) recommendations. It means that once the user has seasonally adjustd the series with a pre-defined specifcation the Benchmarking node is empty. To execute benchmarking click on the Specifications button and activate the checkbox in the Benchmarking section.\n\n\n\nText\n\n\nBenchmarking option – a default view\nThree parameters can be set here. Target specifies the target variable for the benchmarking procedure. It can be either the Original (the raw time series) or the Calendar Adjusted (the time series adjusted for calendar effects). Rho is a value of the AR(1) parameter (set between 0 and 1). By default it is set to 1. Finally, Lambda is a parameter that relates to the weights in the regression equation. It is typically equal to 0 (for an additive decomposition), 0.5 (for a proportional decomposition) or 1 (for a multiplicative decomposition). The default value is 1.\nTo launch the benchmarking procedure click on the Apply button. The results are displayed in four panels. The top-left one compares the original output from the seasonal adjustment procedure with the result from applying a benchmarking to the seasonal adjustment. The bottom-left panel highlights the differences between these two results. The outcomes are also presented in a table in the top-right panel. The relevant statistics concerning relative differences are presented in the bottom-right panel.\n\n\n\nText\n\n\nThe results of the benchmarking procedure\nBoth pictures and the table can be copied the usual way (see the Simple seasonal adjustment of a single time series scenario).\n\n\n\nText\n\n\nOptions for benchmarking results\nTo export the result of the benchmarking procedure (benchmarking.result) and the target data (benchmarking.target) one needs to once execute the seasonal adjustment with benchmarking using the muli-processing option (see the Simple seasonal adjustment of multiple time series scenario. Once the muli-processing is executed, select the Output item from the SAProcessing menu.\n\n\n\nText\n\n\nThe SAProcessing menu\nExpand the \"+\" menu and choose an appropriate data format (here Excel has been chosen). It is possible to save the results in TXT, XLS, CSV, and CSV matrix formats. Note that the available content of the output depends on the output type.\n\n\n\nText\n\n\nExporting data to an Excel file\nChose the output items that refer to the results from the benchmarking procedure, move them to the window on the right and click OK.\n\n\n\nText\n\n\nExporting the results of the benchmarking procedure\n\n\n\n8.3.2 Benchmarking in R\npackage rjd3bench orga doc - here - in package - example"
  },
  {
    "objectID": "A-benchmarking.html#references",
    "href": "A-benchmarking.html#references",
    "title": "8  Benchmarking and temporal disagreggation",
    "section": "8.4 References",
    "text": "8.4 References"
  },
  {
    "objectID": "A-trend-cycle-estimation.html#underlying-theory",
    "href": "A-trend-cycle-estimation.html#underlying-theory",
    "title": "9  Trend-cycle estimation",
    "section": "9.2 Underlying Theory",
    "text": "9.2 Underlying Theory"
  },
  {
    "objectID": "A-trend-cycle-estimation.html#tools",
    "href": "A-trend-cycle-estimation.html#tools",
    "title": "9  Trend-cycle estimation",
    "section": "9.3 Tools",
    "text": "9.3 Tools\n\n9.3.1 rjdfilters package"
  },
  {
    "objectID": "A-nowcasting.html#underlying-theory",
    "href": "A-nowcasting.html#underlying-theory",
    "title": "10  Nowcasting",
    "section": "10.2 Underlying Theory",
    "text": "10.2 Underlying Theory"
  },
  {
    "objectID": "A-nowcasting.html#tools",
    "href": "A-nowcasting.html#tools",
    "title": "10  Nowcasting",
    "section": "10.3 Tools",
    "text": "10.3 Tools"
  },
  {
    "objectID": "T-graphical-user-interface.html",
    "href": "T-graphical-user-interface.html",
    "title": "11  Graphical User Interface",
    "section": "",
    "text": "why use the graphical user interface ? what is not directly available in R yet?\nobjective: describe the general features (independent of algorithms) - general layout - import data - documents - workspaces - specifications - output\nold content can be recycled but - very heavy (trim from md or txt files) - check version 3 - see if we stick with pasted screen shots"
  },
  {
    "objectID": "T-r-packages.html",
    "href": "T-r-packages.html",
    "title": "12  R packages",
    "section": "",
    "text": "table"
  },
  {
    "objectID": "T-r-packages.html#organisation-overview",
    "href": "T-r-packages.html#organisation-overview",
    "title": "12  R packages",
    "section": "12.2 Organisation overview",
    "text": "12.2 Organisation overview\na suite (order)\ngeneral output organisation"
  },
  {
    "objectID": "T-r-packages.html#installation-procedure",
    "href": "T-r-packages.html#installation-procedure",
    "title": "12  R packages",
    "section": "12.3 Installation procedure",
    "text": "12.3 Installation procedure"
  },
  {
    "objectID": "T-r-packages.html#interaction-with-gui",
    "href": "T-r-packages.html#interaction-with-gui",
    "title": "12  R packages",
    "section": "12.4 Interaction with GUI",
    "text": "12.4 Interaction with GUI"
  },
  {
    "objectID": "T-r-packages.html#full-list",
    "href": "T-r-packages.html#full-list",
    "title": "12  R packages",
    "section": "12.5 Full list",
    "text": "12.5 Full list\n\n12.5.1 rjd3modelling\nmain functions: table"
  },
  {
    "objectID": "T-plug-ins.html",
    "href": "T-plug-ins.html",
    "title": "13  Plug-ins for JDemetra+",
    "section": "",
    "text": "table"
  },
  {
    "objectID": "T-production.html",
    "href": "T-production.html",
    "title": "14  Production",
    "section": "",
    "text": "obj here: general explanations + examples ? here : explain voc discrepancies vs guidelines bbk controlled current link to plug in illustration links on covid\nJDemetra+ offers several options for refreshing the output, which are in line with the ESS Guidelines on Seasonal Adjustment (2015) (link) requirements.\nreprduce table cf. my pdf (xls) doc remark: rjwsa cruncher vignette is not up to date"
  },
  {
    "objectID": "T-tool-selection.html",
    "href": "T-tool-selection.html",
    "title": "15  Tool selection issues",
    "section": "",
    "text": "objective : select wisely between GUI(+ cruncher and plug ins) and R packages"
  },
  {
    "objectID": "M-spectral-analysis.html",
    "href": "M-spectral-analysis.html",
    "title": "16  Spectral Analysis Principles and Tools",
    "section": "",
    "text": "add: - R code or refernces to - rjd3sa (?) references to tests - more explanations on spectral analysis rationale\n\n\n\n\nspectral density\nperiodgram\nspectral density estimator\n“spectrum” is used too ubiquitously"
  },
  {
    "objectID": "M-spectral-analysis.html#spectral-analysis-concepts-and-overview",
    "href": "M-spectral-analysis.html#spectral-analysis-concepts-and-overview",
    "title": "16  Spectral Analysis Principles and Tools",
    "section": "16.2 Spectral analysis concepts and overview",
    "text": "16.2 Spectral analysis concepts and overview\nA time series \\(x_{t}\\) with stationary covariance, mean \\(μ\\) and \\(k^{th}\\) autocovariance \\(E(x_{t}-\\mu)(x_{t- k}\\mu))=\\gamma(k)\\) can be described as a weighted sum of periodic trigonometric functions: \\(sin(\\omega t)\\) and \\(cos(\\omega t)\\), where \\(\\omega=\\frac{2*pi}{T}\\) denotes frequency. Spectral analysis investigates this frequency domain representation of \\(x_{t}\\) to determine how important cycles of different frequencies are in accounting for the behavior of \\(x_{t}\\).\nAssuming that the autocovariances \\(\\gamma(k)\\) are absolutely summable(\\(\\sum_{k =-\\infty}^{\\infty}|\\gamma(k)|<\\infty\\)), the autocovariance generating function, which summarizes these autocovariances through a scalar valued function, is given by equation [1]1.\n\\(acgf(z)=\\sum_{k=-\\infty}^{\\infty}{z^{k}\\gamma(k)}\\),\nwhere \\(z\\) denotes complex scalar.\nOnce the equation [1]is divided by \\(\\pi\\) and evaluated at some \\(z{= e}^{- i\\omega} = cos\\omega - isin\\omega\\), where \\(i = \\sqrt{- 1}\\) and \\(\\omega\\) is a real scalar, \\(\\  - \\infty < \\ \\omega < \\infty\\), the result of this transformation is called a population spectrum \\(f(\\omega)\\)for \\(\\ x_{t}\\), given in equation [2]2.\n\\[\nf(\\omega) = \\frac{1}{\\pi}\\sum_{k=- \\infty}^{\\infty}{e^{- ik\\omega}\\gamma(k)}\n\\]\nTherefore, the analysis of the population spectrum in the frequency domain is equivalent to the examination of the autocovariance function in the time domain analysis; however it provides an alternative way of inspecting the process. Because \\(f(\\omega)\\text{dω}\\) is interpreted as a contribution to the variance of components with frequencies in the range \\((\\omega,\\ \\omega + d\\omega)\\), a peak in the spectrum indicates an important contribution to the variance at frequencies near the value that corresponds to this peak.\nAs \\(e\\^{- i\\omega} = cos\\omega - isin\\omega\\), the spectrum can be also expressed as in equation [3].\n\\[\nf(\\omega) = \\frac{1}{\\pi}\\sum_{k = - \\infty}^{\\infty}{(cos\\omega k - isin\\omega k)\\gamma(k)}\n\\]\nSince \\(\\gamma(k) = \\gamma( - k)\\) (i.e. \\(\\gamma(k)\\) is an even function of \\(k\\)) and \\(\\sin{( - x)}\\  = \\operatorname{-sin}x\\), [3] can be presented as equation\n\\[\nf(\\omega) = \\frac{1}{\\pi}\\lbrack \\ \\gamma(0) + 2\\sum_{k = 1}^{\\infty}{\\ \\gamma(k)}cos\\text{ωk} \\rbrack\n\\],\nThis implies that if autocovariances are absolutely summable the population spectrum exists and is a continuous, real-valued function of \\(\\omega\\). Due to the properties of trigonometric functions \\((\\cos( - \\omega k) = \\cos(\\text{ωk})\\) and \\(\\cos(\\omega + 2\\pi j)k = cos(\\omega k))\\) the spectrum is a periodic, even function of \\(\\omega\\), symmetric around \\(\\omega = 0\\). Therefore, the analysis of the spectrum can be reduced to the interval \\(( - \\pi,\\pi)\\). The spectrum is non-negative for all \\(\\omega \\in ( - \\pi,\\pi)\\).\nThe shortest cycle that can be distinguished in a time series lasts two periods. The frequency which corresponds to this cycle is \\(\\omega = \\pi\\) and is called the Nyquist frequency. The frequency of the longest cycles that can be observed in the time series with \\(n\\) observations is \\(\\omega = \\frac{2\\pi}{n}\\) and is called the fundamental (Fourier) frequency.\nNote that if \\(x_{t}\\) is a white noise process with zero mean and variance \\(\\sigma^{2}\\), then for all \\(|k|> 0\\) \\(\\gamma(k)=0\\) and the spectrum of \\(x_{t}\\) is constant (\\(f(\\omega)= \\frac{\\sigma^{2}}{\\pi}\\)) since each frequency in the spectrum contributes equally to the variance of the process3.\nThe aim of spectral analysis is to determine how important cycles of different frequencies are in accounting for the behaviour of a time series4. Since spectral analysis can be used to detect the presence of periodic components, it is a natural diagnostic tool for detecting trading day effects as well as seasonal effects5. Among the tools used for spectral analysis are the autoregressive spectrum and the periodogram.\nThe explanations given in the subsections of this node derive mainly from DE ANTONIO, D., and PALATE, J. (2015) and BROCKWELL, P.J., and DAVIS, R.A. (2006).\ncomment1: end old intro: ok\n\n16.2.1 Theoretical spectral density of an ARIMA model"
  },
  {
    "objectID": "M-spectral-analysis.html#spectral-density-estimation",
    "href": "M-spectral-analysis.html#spectral-density-estimation",
    "title": "16  Spectral Analysis Principles and Tools",
    "section": "16.3 Spectral density estimation",
    "text": "16.3 Spectral density estimation\n\n16.3.1 Method 1: The periodogram\nFor any given frequency \\(\\omega\\) the sample periodogram is the sample analog of the sample spectrum. In general, the periodogram is used to identify the periodic components of unknown frequency in the time series. X-13ARIMA-SEATS and TRAMO-SEATS use this tool for detecting seasonality in raw time series and seasonally adjusted series. Apart from this it is applied for checking randomness of the residuals from the ARIMA model.\nTo define the periodogram, first consider the vector of complex numbers6:\n\\[\n\\mathbf{x} = \\begin{bmatrix}\n  x_{1} \\\\\n  x_{2} \\\\\n  . \\\\\n  . \\\\\n  . \\\\\n  x_{n} \\\\\n  \\end{bmatrix} \\in \\mathbb{C}^{n}\n\\]\nwhere \\(\\mathbb{C}^{n}\\) is the set of all column vectors with complex-valued components.\nThe Fourier frequencies associated with the sample size \\(n\\) are defined as a set of values \\(ω_{j} = \\frac{2\\pi j}{n}\\), \\(j = - \\lbrack \\frac{n-1}{2}\\rbrack,\\ldots,\\lbrack\\frac{n}{2}\\rbrack\\), \\(-\\pi< \\omega_{j} \\leq \\pi\\), \\(j\\in F_{n}\\), where \\({\\lbrack n\\rbrack}\\) denotes the largest integer less than or equal to \\(n\\). The Fourier frequencies, which are called harmonics, are given by integer multiples of the fundamental frequency \\(\\ \\frac{2\\pi}{n}\\).\nNow the \\(n\\) vectors \\(e_{j} = n^{- \\frac{1}{2}}(e^{-i\\omega_{j}},e^{-i{2\\omega}_{j}},\\ldots,e^{- inω_{j}})^{'}\\) can be defined. Vectors \\(e_{1},\\ldots, e_{n}\\) are orthonormal in the sense that:\n\\[\n{\\mathbf{e}_{j}^{*}\\mathbf{e}}_{k} = n^{- 1}\\sum_{r = 1}^{n}e^{ir(\\omega_{j} - \\omega_{k})} = { \\begin{matrix}\n  1,\\ if\\ j = k \\\\\n  0,\\ if\\ j \\neq k \\\\\n  \\end{matrix}}\n\\]\nwhere \\(\\mathbf{e}_{j}^{*}\\) denotes the row vector, which \\(k^{th}\\) component is the complex conjugate of the \\(k^{th}\\) component of \\(\\mathbf{e}_{j}\\).7 These vectors are a basis of \\(F_{n}\\), so that any \\(\\mathbf{x}\\in\\mathbb{C}^{n}\\) can be expressed as a sum of \\(n\\) components:\n\\[\n\\mathbf{x} = \\sum_{j = - \\lbrack\\frac{n - 1}{2}\\rbrack}^{\\lbrack\\frac{n}{2}\\rbrack}{a_{j}\\mathbf{e}_{j}}\n\\]\nwhere the coefficients \\(a_{j} = \\mathbf{e}_{j}^{*}\\mathbf{x}=n^{-\\frac{1}{2}}\\sum_{t = 1}^{n}x_{t}e^{-it\\omega_{j}}\\) are derived from [3] by multiplying the equation on the left by \\(\\mathbf{e}_{j}^{*}\\) and using [1].\nThe sequence of \\(\\{a_{j},j\\in F_{n}\\}\\) is referred as a discrete Fourier transform of \\(\\mathbf{x}\\mathbb{\\in C}^{n}\\) and the periodogram \\(I(\\omega_{j})\\) of \\(\\mathbf{x}\\) at Fourier frequency \\(\\omega_{j} = \\frac{2\\pi j}{n}\\) is defined as the square of the Fourier transform \\(\\{a_{j}\\}\\) of \\(\\mathbf{x}\\):\n\\[\n{I(\\omega_{j})\\mathbf{=}{| a_{j} |^{2}}_{\\ } = n^{- \\ 1}| \\sum_{t = 1}^{n}x_{t}e^{- it\\omega_{j}} |^{2}}_{\\mathbf{\\ }}\n\\]\nFrom [2] and [3] it can be shown that in fact the periodogram decomposes the total sum of squares \\(\\sum_{t = 1}^{n}| x_{t} |^{2}\\) into a sums of components associated with the Fourier frequencies \\[ω_{j}\\]:\n\\[\n  \\sum_{t=1}^{n}{|x_{t}|}^{2} = \\sum_{j = - \\lbrack\\frac{n - 1}{2}\\rbrack}^{\\lbrack\\frac{n}{2}\\rbrack}|a_{j}|^{2} = \\sum_{j = - \\lbrack\\frac{n - 1}{2}\\rbrack}^{\\lbrack\\frac{n}{2}\\rbrack}{I(\\omega_{j})}\n\\]\nIf \\(\\ \\mathbf{x\\  \\in}\\ {R}^{n}\\), \\(\\omega_{j}\\) and \\({-\\omega}_{j}\\) are both in \\(\\lbrack- \\pi, -\\pi \\rbrack\\) and \\(a_{j}\\) is presented in its polar form (i.e. \\(a_{j} = r_{j}\\exp( i\\theta_{j})\\)), where \\(r_{j}\\) is the modulus of \\(a_{j}\\), then [3] can be rewritten in the form:\n\\[\n\\mathbf{x} = a_{0}\\mathbf{e}_{0} + \\sum_{j = 1}^{\\lbrack\\frac{n - 1}{2}\\rbrack}{ {2^{1/2}r}_{j}{(\\mathbf{c}}_{j}\\cos\\theta_{j}{- \\mathbf{s}}_{j}\\sin\\theta_{j}) + a_{n/2}\\mathbf{e}_{n/2}}\n\\]\nThe orthonormal basis for \\({R}^{n}\\) is \\(\\{\\mathbf{e}_{0},\\mathbf{c}_{1},\\mathbf{s}_{1},\\ldots,\\mathbf{c}_{\\lbrack\\frac{n - 1}{2}\\rbrack},\\mathbf{s}_{\\lbrack\\frac{n - 1}{2}\\rbrack},\\mathbf{e}_{\\frac{n}{2}(excluded\\ if\\ n\\ is\\ odd)}\\}\\), where:\n\\(\\mathbf{e}_{0}\\) is a vector composed of n elements equal to \\(n^{- 1/2}\\), which implies that \\(\\mathbf{a}_{0}\\mathbf{e}_{0} = {(n^{-1}\\sum_{t = 1}^{n}x_{t},\\ldots,n^{- 1}\\sum_{t=1}^{n}x_{t})}^{'}\\);\n\\[\n\\mathbf{c}_{j}=(\\frac{n}{2})^{- 1/2}{(\\cos\\omega_{j},\\cos{2\\omega}_{j},\\ldots,\\cos{n\\omega_{j}})}^{'}, for 1 \\leq j \\leq \\lbrack \\frac{(n - 1)}{2}\\rbrack\n\\] ;\n\\[\n\\mathbf{s}_{j} = {(\\frac{n}{2})}^{-1/2}{(\\sin{\\omega_{j}},\\sin{2\\omega_{j}},\\ldots,\\sin{n\\omega_{j}})}^{'},\\ for\\ 1 \\leq j \\leq \\lbrack \\frac{(n - 1)}{2} \\rbrack\n\\];\n\\[\n\\mathbf{e}_{n/2} = {(- (n^{-\\frac{1}{2}}),n^{- \\frac{1}{2}},\\ldots,{-(n)}^{- \\frac{1}{2}}),n^{-\\frac{1}{2}})}^{'}\n\\].\nEquation [5] can be seen as an OLS regression of \\(x_{t}\\) on a constant and the trigonometric terms. As the vector of explanatory variables includes \\(n\\) elements, the number of explanatory variables in [5] is equal to the number of observations. HAMILTON, J.D. (1994) shows that the explanatory variables are linearly independent, which implies that an OLS regression yields a perfect fit (i.e. without an error term). The coefficients have the form of a simple OLS projection of the data on the orthonormal basis:\n\\[\n  {\\widehat{a}}_{0}=\\frac{1}{\\sqrt{n}}\\sum_{t=1}^{n}x_{t}\n\\] [7] \n\\[\n  {\\widehat{a}}_{n/2}=\\frac{1}{\\sqrt{n}}\\sum_{t=1}^{n}{(-1)}^{t}x_{t}(   \\text{only when n is even})\n\\] [8] \n\\[\n  {\\widehat{a}}_{0}=\\frac{1}{\\sqrt{n}}\\sum_{t=1}^{n}x_{t}\n\\] [9] \n\\[\n  {\\widehat{\\alpha}}_{j} = 2^{1/2}r_{j}\\cos{\\theta_{j}} = {(\\frac{n}{2})}^{- 1/2}\\sum_{t = 1}^{n}x_{t}\\cos{(t\\frac{2\\pi j}{n})}, j   = 1,\\ldots,\\lbrack\\frac{n - 1}{2}\\rbrack\n\\] [10] \n\\[\n  {\\widehat{\\beta}}_{j} = 2^{1/2}r_{j}\\sin{\\theta_{j}} = {(\\frac{n}{2})}^{-1/2}\\sum_{t = 1}^{n}x_{t}\\sin{(t\\frac{2\\pi j}{n})}, j = 1,\\ldots,\\lbrack\\frac{n - 1}{2}\\rbrack\n\\] [11] \nWith [5] the total sum of squares \\(\\sum_{t = 1}^{n}| x_{t} |^{2}\\) can be decomposed into \\(2 \\times \\lbrack\\frac{n - 1}{2}\\rbrack\\) components corresponding to \\(\\mathbf{c}_{j}\\) and \\(\\mathbf{s}_{j}\\), which are grouped to produce the “frequency \\(ω_{j}\\)” component for \\(1 \\geq j \\geq \\lbrack\\frac{n - 1}{2}\\rbrack\\). As it is shown in the table below, the value of the periodogram at the frequency \\(\\omega_{j}\\) is the contribution of the \\(j\\^{\\text{th}}\\) harmonic to the total sum of squares \\(\\sum_{t = 1}^{n}| x_{t} |^{2}\\).\nDecomposition of sum of squares into components corresponding to the harmonics\n{: .table .table-style} |Frequency |Degrees of freedom |Sum of squares decomposition| |———————————————– |———————— |————————————————————-| |\\(\\omega_{0}\\)(mean) |1 |\\({a_{0}^{2}}_{\\ }=n^{- 1}(\\sum_{t=1}^{n}x_{t})^{2} = I( 0)\\)| |\\(\\omega_{1}\\) |2 |\\({2r_{1}^{2}}_{\\ } = 2{\\|a_{1}\\|}^{2} = 2I(\\omega_{1})\\)| |\\(\\vdots\\) |\\(\\vdots\\) |\\(\\vdots\\)| |\\(\\omega_{k}\\) |2 |\\({2r_{k}^{2}}_{\\ } = 2{\\|a_{k}\\|}^{2} = 2I(\\omega_{k})\\)| |\\[\\vdots\\] |\\(\\vdots\\) |\\(\\vdots\\)| |\\(\\omega_{n/2} = \\pi\\) (excluded if \\(n\\) is odd) |1 |\\(a_{n/2}^{2} = I(\\pi)\\)| |Total |\\(\\mathbf{n}\\) |\\(\\sum_{\\mathbf{t = 1}}^{\\mathbf{n}}\\mathbf{x}_{\\mathbf{t}}^{\\mathbf{2}}\\)|\nSource: DE ANTONIO, D., and PALATE, J. (2015).\nObviously, if series were random then each component \\(I(\\omega\\_{j})\\) would have the same expectation. On the contrary, when the series contains a systematic sine component having a frequency \\(j\\) and amplitude \\(A\\) then the sum of squares \\(I(\\omega_{j})\\) increases with \\(A\\). In practice, it is unlikely that the frequency \\(j\\) of an unknown systematic sine component would exacly match any of the frequencies, for which peridogram have been calcuated. Therefore, the periodogram would show an increase in intensities in the immediate vicinity of \\(j\\).8\nNote that in JDemetra+ the periodogram object corresponds exactly to the contribution to the sum of squares of the standardised data, since the series are divided by their standard deviation for computational reasons.\nUsing the decomposition presented in table above the periodogram can be expressed as:\n\\[\nI(\\omega_{j})\\mathbf{=}\\begin{matrix}                                                                                r_{j}^{2} = \\frac{1}{2}{(\\alpha}_{j}^{2} + \\beta_{j}^{2}) = \\ {\\frac{1}{n}(\\sum_{t = 1}^{n}{x_{t}\\cos{( {t\\frac{2\\pi j}{n}}_{\\ })\\ }})}^{2} + \\frac{1}{n}(\\sum_{t = 1}^{n}{x_{t}\\sin( t\\frac{2\\pi j}{n})_{\\ }})^{2} \\\\\n\\end{matrix}\n\\] [12] \nwhere \\(j = 0,\\ldots,lbrack \\frac{n}{2} rbrack\\).\nSince \\(\\mathbf{x} - \\overline{\\mathbf{x}}\\) are generated by an orthonormal basis, and \\(\\overline{\\mathbf{x}}\\mathbf{=}a_{0}\\mathbf{e}_{0}\\) [5] can be rearranged to show that the sum of squares is equal to the sum of the squared coefficients:\n\\[\n\\mathbf{x} - a_{0}\\mathbf{e}_{0} =\\sum_{j=1}^{\\lbrack(n - 1)/2\\rbrack}(\\alpha_{j}\\mathbf{c}_{j}+\\beta_{j}\\mathbf{s}_{j}) + a_{n/2}\\mathbf{e}_{n/2}\n\\]. [13] \nThus the sample variance of \\[x_{t}\\] can be expressed as:\n\\[\nn^{- 1}\\sum_{t=1}^{n}{(x_{t}-\\overline{x})}^{2}=n^{-1}(\\sum_{k=1}^{\\lbrack(n - 1)/2\\rbrack}2{r_{j}}^{2}\n+{a_{n/2}}^{2})\n\\], [14] \nwhere \\(a_{n/2}^{2}\\) is excluded if \\(n\\) is odd.\nThe term \\(2{r_{j}}^{2}\\) in [14] is then the contribution of the \\(j^{\\text{th}}\\) harmonic to the variance and [14] shows then how the total variance is partitioned.\nThe periodogram ordinate \\(I(\\omega_{j})\\) and the autocovariance coefficient \\(\\gamma(k)\\) are both quadratic forms of \\(x_{t}\\). It can be shown that the periodogram and autocovarinace function are related and the periodogram can be written in terms of the sample autocovariance function for any non-zero Fourier frequency \\(ω_{j}\\) :9\n\\[\nI(\\omega_{j}) = \\sum_{| k | < n}^{\\ }{\\widehat{\\gamma}( k)}_{\\ }e^{- ik\\omega_{j}} = {\\widehat{\\gamma}( 0)}_{\\ } + 2\\sum_{k = 1}^{n - 1}{\\widehat{\\gamma}( k)\\cos{(k\\omega_{j})}}_{\\ }\n\\]\nand for the zero frequency \\(\\ I( 0) = n| \\overline{x} |^{2}\\).\nOnce comparing [15] with an expression for the spectral density of a stationary process:\n\\[\nf(\\omega_{\\ }) = \\frac{1}{2\\pi}\\sum_{k < - \\infty}^{\\infty}{\\gamma( k)}_{\\ }e^{- ik\\omega_{\\ }} = \\frac{1}{2\\pi}lbrack {\\gamma( 0)}_{\\ } + 2(\\sum_{k = 1}^{\\infty}{\\gamma( k)\\cos{(k\\omega_{\\ })}}) rbrack\n\\]\nIt can be noticed that the periodogram is a sample analog of the population spectrum. In fact, it can be shown that the periodogram is asymptotically unbiased but inconsistent estimator of the population spectrum \\(f(\\omega)\\).[^75] Therefore, the periodogram is a wildly fluctuating, with high variance, estimate of the spectrum. However, the consistent estimator can be achieved by applying the different linear smoothing filters to the periodogram, called lag-window estimators. The lag-window estimators implemented in JDemetra+ includes square, Welch, Tukey, Barlett, Hanning and Parzen. They are described in DE ANTONIO, D., and PALATE, J. (2015). Alternatively, the model-based consistent estimation procedure, resulting in autoregressive spectrum estimator, can be applied.\ncomment2: end part theory>spectral analysis>periodogram\n\n\n16.3.2 Method 2: Autoregressive spectrum estimation\nBROCKWELL, P.J., and DAVIS, R.A. (2006) point out that for any real-valued stationary process \\((x_{t})\\) with continuous spectral density \\(f(\\omega)\\) it is possible to find both \\(AR(p)\\) and \\(MA(q)\\) processes which spectral densities are arbitrarily close to \\(f(\\omega)\\). For this reason, in some sense, \\((x_{t})\\) can be approximated by either \\(AR(p)\\) or \\(MA(q)\\) process. This fact is a basis of one of the methods of achieving a consistent estimator of the spectrum, which is called an autoregressive spectrum estimation. It is based on the approximation of the stochastic process \\((x_{t})\\) by an autoregressive process of sufficiently high order \\(p\\):\n\\[\nx_{t} = \\mu + (\\phi_{1}B + \\ldots + \\phi_{p}B^{p})x_{t} + \\varepsilon_{t}\n\\]\nwhere \\(\\varepsilon_{t}\\) is a white-noise variable with mean zero and a constant variance.\nThe autoregressive spectrum estimator for the series \\(x_{t}\\) is defined as: 10\n\\[\n\\widehat{s}(\\omega) = 10\\operatorname{\\times}{\\log_{10}\\frac{\\sigma_{x}^{2}}{2\\pi{|1 - \\sum_{k = 1}^{p}{\\widehat{\\phi}}_{k}e^{- ik\\omega}|}^{2}}}\n\\]\nwhere:\n\\(\\omega\\)– frequency, \\(0 \\leq \\omega \\leq \\pi\\);\n\\(\\sigma_{x}^{2}\\) – the innovation variance of the sample residuals;\n\\({\\widehat{\\phi}}_{k}\\) – \\(\\text{AR}(k)\\) coefficient estimates of the linear regression of \\(x_{t} - \\overline{x}\\) on \\(x_{t - k} - \\overline{x}\\), \\(1 \\leq k \\leq p\\).\nThe autoregressive spectrum estimator is used in the visual spectral analysis tool for detecting significant peaks in the spectrum. The criterion of visual significance, implemented in JDemetra+, is based on the range \\({\\widehat{s}}^{\\max} - {\\widehat{s}}^{\\min}\\) of the \\(\\widehat{s}(\\omega)\\) values, where \\({\\widehat{s}}^{\\max} = \\max_{k}\\widehat{s}(\\omega_{k})\\); \\({\\widehat{s}}^{\\min} = \\min_{k}\\widehat{s}(\\omega_{k});\\) and \\(\\widehat{s}(\\omega\\_{k})\\) is \\(k^{\\text{th}}\\) value of autoregressive spectrum estimator.\nThe particular value is considered to be visually significant if, at a trading day or at a seasonal frequency \\(\\omega_{k}\\) (other than the seasonal frequency \\(\\omega_{60} = \\pi\\)), \\(\\widehat{s}(\\omega\\_{k})\\) is above the median of the plotted values of \\(\\widehat{s}(\\omega_{k})\\) and is larger than both neighbouring values \\(\\widehat{s}(\\omega_{k - 1})\\) and \\(\\widehat{s}(\\omega_{k + 1})\\) by at least \\(\\frac{6}{52}\\) times the range \\({\\widehat{s}}^{\\max} - {\\widehat{s}}^{\\min}\\).\nFollowing the suggestion of SOUKUP, R.J., and FINDLEY, D.F. (1999), JDemetra+ uses an autoregressive model spectral estimator of model order 30. This order yields high resolution of strong components, meaning peaks that are sharply defined in the plot of \\(\\widehat{s}(\\omega)\\) with 61 frequencies. The minimum number of observations needed to compute the spectrum is set to \\(n=80\\) for monthly data and to \\(n=60\\) for quarterly series while the maximum number of observations considered for the estimation is 121. Consequently, with these settings it is possible to identify up to 30 peaks in the plot of 61 frequencies. By choosing \\(\\omega_{k} = \\frac{\\text{πk}}{60}\\) for \\(k=0,1,...,60\\) the density estimates are calculated at exact seasonal frequencies (1, 2, 3, 4, 5 and 6 cycles per year).\nThe model order can also be selected based on the AIC criterion (in practice it is much lower than 30). A lower order produces the smoother spectrum, but the contrast between the spectral amplitudes at the trading day frequencies and neighbouring frequencies is weaker, and therefore not as suitable for automatic detection.\nSOUKUP, R.J., and FINDLEY, D.F. (1999) also explain that the periodogram can be used in the visual significance test as it has as good as those of the AR(30) spectrum abilities to detect trading day effect, but also has a greater false alarm rate11.\ncomment2: end part theory>spectral analysis>auto-regressive spectrum\n\n\n16.3.3 Method 3: Tukey spectrum"
  },
  {
    "objectID": "M-spectral-analysis.html#identification-of-spectral-peaks",
    "href": "M-spectral-analysis.html#identification-of-spectral-peaks",
    "title": "16  Spectral Analysis Principles and Tools",
    "section": "16.4 Identification of spectral peaks",
    "text": "16.4 Identification of spectral peaks\ncomment3: start part theory>spectral analysis>identification of spectral peaks\nIdentification of seasonal peaks in a Tukey periodogram and in an autoregressive spectrum\nIn order to decide whether a series has a seasonal component that is predictable (stable) enough, these tests use visual criteria and formal tests for the periodogram. The periodogram is calculated using complete years, so that the set of Fourier frequencies contains exactly all seasonal frequencies12.\nThe tests rely on two basic principles:\n\nThe peaks associated with seasonal frequencies should be larger than the median spectrum for all frequencies and;\nThe peaks should exceed the spectrum of the two adjacent values by more than a critical value.\n\n\nJDemetra+ performs this test on the original series. If these two requirements are met, the test results are displayed in green. The statistical significance of each of the seasonal peaks (i.e. frequencies \\(\\frac{\\pi}{6},\\ \\frac{\\pi}{3},\\ \\frac{\\pi}{2},\\ \\frac{2\\pi}{3}\\) and \\(\\frac{5\\pi}{6}\\) corresponding to 1, 2, 3, 4 and 5 cycles per year) is also displayed. The seasonal and trading days frequencies depends on the frequency of time series. They are shown in the table below. The symbol \\(d\\) denotes a default frequency and is described below the table.\n\nThe seasonal and trading day frequencies by time series frequency\n{: .table .table-style} |Number of months per full period | Seasonal frequency | Trading day frequency (radians)| |————————————–| ————————————————————————————-| ————————————| |12 | \\(\\frac{\\pi}{6},\\frac{\\pi}{3},\\ \\frac{\\pi}{2},\\frac{2\\pi}{3},\\ \\frac{5\\pi}{6},\\ \\pi\\) | \\(d\\), 2.714| |6 | \\(\\frac{\\pi}{3},\\frac{2\\pi}{3}\\), \\(\\pi\\) | \\(d\\) |4 | \\(\\frac{\\pi}{2}\\), \\(\\pi\\) | \\(d\\), 1.292, 1.850, 2.128| |3 | \\(\\pi\\) | \\(d\\)| |2 | \\(\\pi\\) | \\(d\\)|\nThe calendar (trading day or working day) effects, related to the variation in the number of different days of the week per period, can induce periodic patterns in the data that can be similar to those resulting from pure seasonal effects. From the theoretical point of view, trading day variability is mainly due to the fact that the average number of days in the months or quarters is not equal to a multiple of 7 (the average number of days of a month in the year of 365.25 days is equal to \\(\\frac{365.25}{12} = 30.4375 days\\)). This effect occurs \\(\\frac{365.25}{12} \\times \\frac{1}{7} = 4.3482 times per month\\): one time for each one of the four complete weeks of each month, and a residual of 0.3482 cycles per month, i.e. \\(0.3482 \\times 2\\pi = 2.1878 radians\\). This turns out to be a fundamental frequency for the effects associated with monthly data. In JDemetra+ the fundamental frequency corresponding to 0.3482 cycles per month is used in place of the closest frequency \\(\\frac{\\text{πk}}{60}\\). Thus, the quantity \\(\\frac{\\pi \\times 42}{60}\\) is replaced by \\(\\omega_{42} = 0.3482 \\times 2\\pi = 2.1878\\). The frequencies neighbouring \\(\\omega_{42}\\), i.e. \\(\\omega_{41}\\) and \\(\\omega_{43}\\) are set to, respectively, \\(2.1865 - \\frac{1}{60}\\) and \\(2.1865 + \\frac{1}{60}\\).\nThe default frequencies (\\(d\\)) for calendar effect are: 2.188 (monthly series) and 0.280 (quarterly series). They are computed as:\n\\[\n\\omega_{\\text{ce}} = \\frac{2\\pi}{7}\\left( n - 7 \\times \\left\\lbrack \\frac{n}{7} \\right\\rbrack \\right)\n\\], [1] \nwhere:\n\\(n = \\frac{365.25}{s}\\), \\(s = 4\\) for quarterly series and \\(s = 12\\) for monthly series.\nOther frequencies that correspond to trading day frequencies are: 2.714 (monthly series) and 1.292, 1.850, 2.128 (quarterly series).\nIn particular, the calendar frequency in monthly data (marked in red on the figure below) is very close to the seasonal frequency corresponding to 4 cycles per year \\(\\text{ω}_{40} = \\frac{2}{3}\\pi = 2.0944\\).\n\n\n\nText\n\n\nPeriodogram with seasonal (grey) and calendar (red) frequencies highlighted\nThis implies that it may be hard to disentangle both effects using the frequency domain techniques.\ncomment3: end part theory>spectral analysis>identification of spectral peaks\n\n16.4.0.1 in Tukey spectrum\ncomes from Identification of seasonal peaks in a Tukey spectrum\n\n\n16.4.0.2 Tukey Spectrum definition\nThe Tukey spectrum belongs to the class of lag-window estimators. A lag window estimator of the spectral density \\(f(\\omega)=\\frac{1}{2\\pi}\\sum_{k<-\\infty}^{\\infty}\\gamma(k)e^{i k \\omega}\\) is defined as follows:\n\\[\n\\hat{f}_{L}(\\omega)=\\frac{1}{2\\pi}\\sum_{\\left| h \\right| \\leq r } w(h/r)\\hat{\\gamma}(h)e^{i h \\omega}\n\\]\nwhere \\(\\hat{\\gamma}(.)\\) is the sample autocovariance function, \\(w(.)\\) is the lag window, and \\(r\\) is the truncation lag. \\(\\left| w(x)\\right|\\) is always less than or equal to one, \\(w(0)=1\\) and \\(w(x)=0\\) for \\(\\left| x \\right| > 1\\). The simple idea behind this formula is to down-weight the autocovariance function for high lags where \\(\\hat{\\gamma}(h)\\) is more unreliable. This estimator requires choosing \\(r\\) as a function of the sample size such that \\(r/n \\rightarrow 0\\) and \\(r\\rightarrow \\infty\\) when \\(n \\rightarrow \\infty\\) . These conditions guarantee that the estimator converges to the true density.\nJDemetra+ implements the so-called Blackman-Tukey (or Tukey-Hanning) estimator, which is given by \\(w(h/r)=0.5(1+cos(\\pi h/r))\\) if \\(\\left| h/r \\right| \\leq 1\\) and \\(0\\) otherwise.\nThe choice of large truncation lags \\(r\\) decreases the bias, of course, but it also increases the variance of the spectral estimate and decreases the bandwidth.\nJDemetra+ allows the user to modify all the parameters of this estimator, including the window function.\n\n\n16.4.0.3 Graphical Test\nThe current JDemetra+ implementation of the seasonality test is based on a \\(F(d_{1},d_{2})\\) approximation that has been originally proposed by Maravall (2012) for TRAMO-SEATS. This test is has been designed for a Blackman-Tukey window based on a particular choices of the truncation lag \\(r\\) and sample size. Following this approach, we determine visually significant peaks for a frequency \\(\\omega_{j}\\) when\n\\[\n\\frac{2 f_{x}(\\omega_{j})}{\\left[ f_{x}(\\omega_{j+1})+ f_{x}(\\omega_{j-1}) \\right]} \\ge CV(\\omega_{j})\n\\]\nwhere \\(CV(\\omega_{j})\\) is the critical value of a \\(F(d_{1},d_{2})\\) distribution, where the degrees of freedom are determined using simulations. For \\(\\omega_{j}= \\pi\\), we have a significant peak when \\(\\frac{f_{x}(\\omega_{[n/2]})}{\\left[ f_{x}(\\omega_{[(n-1)/2]})\\right]} \\ge CV(\\omega_{j})\\)\nTwo significant levels for this test are considered: \\(\\alpha=0.05\\) (code “t”) and \\(\\alpha=0.01\\) (code “T”).\nAs opposed to the AR spectrum, which is computed on the basis of the last \\(120\\) data points, we will use here all available observations. Those critical values have been calculated given the recommended truncation lag \\(r=79\\) for a sample size within the interval \\(\\in [80,119]\\) and \\(r=112\\) for \\(n \\in [120,300]\\) . The \\(F\\) approximation is less accurate for sample sizes larger than \\(300\\). For quarterly data, \\(r=44\\), but there are no recommendations regarding the required sample size.\n\n\n16.4.0.4 Use\nThe test can be applied directly to any series by selecting the option Statistical Methods >> Seasonal Adjustment >> Tools >> Seasonality Tests. This is an example of how results are displayed for the case of a monthly series:\n\n\n\ntktest\n\n\nJDemetra+ considers critical values for \\(\\alpha=1\\%\\) (code “T”) and \\(\\alpha=5\\%\\) (code “t”) at each one of the seasonal frequencies represented in the table below, e.g. frequencies \\(\\frac{\\pi}{6}, \\frac{\\pi}{3}, \\frac{\\pi}{2}, \\frac{2\\pi}{3}\\text{ and } \\frac{5\\pi}{6}\\) corresponding to 1, 2, 3, 4, 5 and 6 cycles per year in this example, since we are dealing with monthly data. The codes “a” and “A” correpond to the so-called AR spectrum, so ignore them for the moment.\nThe seasonal and trading day frequencies by time series frequency\n\n\n\n\n\n\n\n\nNumber of months per full period\nSeasonal frequency\nTrading day frequency (radians)\n\n\n\n\n12\n\\(\\frac{\\pi}{6},\\frac{\\pi}{3},\\ \\frac{\\pi}{2},\\frac{2\\pi}{3},\\ \\frac{5\\pi}{6},\\ \\pi\\)\n\\(d\\), 2.714\n\n\n6\n\\(\\frac{\\pi}{3},\\frac{2\\pi}{3}\\), \\(\\pi\\)\n\\(d\\)\n\n\n4\n\\(\\frac{\\pi}{2}\\), \\(\\pi\\)\n\\(d\\), 1.292, 1.850, 2.128\n\n\n3\n\\(\\pi\\)\n\\(d\\)\n\n\n2\n\\(\\pi\\)\n\\(d\\)\n\n\n\nCurrently, only seasonal frequencies are tested, but the program allows you to manually plot the Tukey spectrum and focus your attention on both seasonal and trading day frequencies.\n\n\n16.4.0.5 References\n\nTukey, J. (1949). The sampling theory of power spectrum estimates., Proceedings Symposium on Applications of Autocorrelation Analysis to Physical Problems, NAVEXOS-P-735, Office of Naval Research, Washington, 47-69\n\n\n\n16.4.0.6 in AR Spectrum definition\ncomes from: “Identification of seasonal peaks in autoregressive spectrum”\nThe estimator of the spectral density at frequency \\(\\lambda \\in [0,\\pi]\\) will be given by the assumption that the series will follow an AR(p) process with large \\(p\\). The spectral density of such model, with an innovation variance \\(var(x_{t})=\\sigma^2_x\\), is expressed as follows:\n\\[\n10\\times log_{10} f_x(\\lambda)=10\\times log_{10} \\frac{\\sigma^2_x}{2\\pi \\left|\\phi(e^{i\\lambda}) \\right|^2 }=10\\times log_{10} \\frac{\\sigma^2_x}{2\\pi \\left|1-\\sum_{k=1}^{p}\\phi_k e^{i k \\lambda}) \\right|^2 }\n\\]\nwhere \\(\\phi_k\\) denotes the AR(k) coefficient, and \\(e^{-ik\\lambda}=cos⁡(-ik\\lambda)+i sin⁡(-ik\\lambda)\\).\nSoukup and Findely (1999) suggest the use of p=30, which in practice much larger than the order that would result from the AIC criterion. The minimum number of observations needed to compute the spectrum is set to n=80 for monthly data (or n=60) for quarterly series. In turn, the maximum number of observations considered for the estimation is n=121. This choice offers enough resolution, being able to identify a maximum of 30 peaks in a plot of 61 frequencies: by choosing \\(\\lambda_j=\\pi j/60\\),for \\(j=0,1,…,60\\), we are able to calculate our density estimates at exact seasonal frequencies (1, 2, 3, 4, 5 and 6 cycles per year). Note that \\(x\\) cycles per year can be converted into cycles per month by simply dividing by twelve, \\(x/12\\), and to radians by applying the transformation \\(2\\pi(x/12)\\).\nThe traditional trading day frequency corresponding to 0.348 cycles per month is used in place of the closest frequency \\(\\pi j/60\\). Thus, we replace \\(\\pi 42/60\\) by \\(\\lambda_{42}=0.348\\times 2 \\pi = 2.1865\\). The frequencies neighbouring \\(\\lambda_{42}\\) are set to \\(\\lambda_{41}= 2.1865-1/60\\) and \\(\\lambda_{43}= 2.1865+1/60\\). The periodogram below illustrates the proximity of this trading day frequency \\(\\lambda_{42}\\) (red shade) and the frequency corresponding to 4 cycles per year \\(\\lambda_{40}=2.0944\\). This proximity is precisely what poses the identification problems: the AR spectrum boils down to a smoothed version of the periodogram and the contribution of the of the trading day frequency may be obscured by the leakage resulting from the potential seasonal peak at \\(\\lambda_{40}\\), and vice-versa.\n\n\n\nText\n\n\nPeriodogram with seasonal (grey) and calendar (red) frequencies highlighted\nJDemetra+ allows the user to modify the number of lags of this estimator and to change the number of observations used to determine the AR parameters. These two options can improve the resolution of this estimator.\n\n\n16.4.0.7 Graphical Test\nThe statistical significance of the peaks associated to a given frequency can be informally tested using a visual criterion, which has proved to perform well in simulation experiments. Visually significant peaks for a frequency \\(\\lambda_{j}\\) satisfy both conditions:\n\n\\(\\frac{f_{x}(\\lambda_{j})- \\max \\left\\{f_{x}(\\lambda_{j+1}),f_{x}(\\lambda_{j-1}) \\right\\}}{\\left[ \\max_{k}f_{x}(\\lambda_{k})-\\min_{i}f_{x}(\\lambda_{i}) \\right]}\\ge CV(\\lambda_{j})\\), where \\(CV(\\lambda_{j})\\) can be set equal to \\(6/52\\) for all \\(j\\)\n\\(f_{x}(\\lambda_{j})> median_{j} \\left\\{ f_{x}(\\lambda_{j}) \\right\\}\\), which guarantees \\(f_{x}(\\lambda_{j})\\) it is not a local peak.\n\nThe first condition implies that if we divide the range \\(\\max_{k}f_{x}(\\lambda_{k})-\\min_{i}f_{x}(\\lambda_{i})\\) in 52 parts (traditionally represented by stars) the height of each pick should be at least 6 stars.\n\n\n16.4.0.8 Use\nThe test can be applied directly to any series by selecting the option Statistical Methods >> Seasonal Adjustment >> Tools >> Seasonality Tests. This is an example of how results are displayed for the case of a monthly series:\n\n\n\nartest\n\n\nJDemetra+ considers critical values for \\(\\alpha=1\\%\\) (code “A”) and \\(\\alpha=5\\%\\) (code “a”) at each one of the seasonal frequencies represented in the table below, e.g. frequencies \\(\\frac{\\pi}{6}, \\frac{\\pi}{3}, \\frac{\\pi}{2}, \\frac{2\\pi}{3}\\text{ and } \\frac{5\\pi}{6}\\) corresponding to 1, 2, 3, 4, 5 and 6 cycles per year in this example, since we are dealing with monthly data. The codes “t” and “T” correpond to the so-called Tukey spectrum, so ignore them for the moment.\nThe seasonal and trading day frequencies by time series frequency\n{: .table .table-style} |Number of months per full period | Seasonal frequency | Trading day frequency (radians)| |————————————–| ————————————————————————————-| ————————————| |12 | \\(\\frac{\\pi}{6},\\frac{\\pi}{3},\\ \\frac{\\pi}{2},\\frac{2\\pi}{3},\\ \\frac{5\\pi}{6},\\ \\pi\\) | \\(d\\), 2.714| |6 | \\(\\frac{\\pi}{3},\\frac{2\\pi}{3}\\), \\(\\pi\\) | \\(d\\) |4 | \\(\\frac{\\pi}{2}\\), \\(\\pi\\) | \\(d\\), 1.292, 1.850, 2.128| |3 | \\(\\pi\\) | \\(d\\)| |2 | \\(\\pi\\) | \\(d\\)|\nCurrently, only seasonal frequencies are tested, but the program allows you to manually plot the AR spectrum and focus your attention on both seasonal and trading day frequencies. Agustin Maravall has conducted a simulation experiment to calculate \\(CV(\\lambda_{42})\\) (trading day frequency) and proposes to set for all \\(j\\) equal to the critical value associated to the trading frequency, but this is currently not part of the current automatic testing procedure of JDemetra+.\n\n\n16.4.0.9 References\n\nSoukup, R.J., and D.F. Findley (1999) On the Spectrum Diagnosis used by X12-ARIMA to Indicate the Presence of Trading Day Effects After Modeling or Adjustment. In Proceedengs of the American Statistical Association. Business and Economic Statistics Section, 144-149, Alexandria, VA.\n\n\n\n16.4.0.10 in a Periodogram\ncomes from: Identification of seasonal peaks in periodogram\nThe periodogram \\(I(\\omega_j)\\) of \\(\\mathbf{X} \\in \\mathbb{C}^n\\) is defined as the squared of the Fourier transform\n\\[\nI(\\omega_{j})=a_{j}^{2}=n^{-1}\\left| \\sum_{t=1}^{n}\\mathbf{X_t} e^{-it\\omega_j} \\right|^{2},\n\\]\nwhere the Fourier frequencies \\(\\omega_{j}\\) are given by multiples of the fundamental frequency \\(\\frac{2\\pi}{n}\\):\n\\[\n\\omega_{j}= \\frac{2\\pi j}{n}, -\\pi < \\omega_{j} \\leq \\pi\n\\]\nAn orthonormal basis in \\(\\mathbb{R}^n\\):\n\\[\n\\left\\{ e_0, ~~~~~~c_1, s_1, ~~~~~\\ldots~~~~~\\ , ~~~~c_{[(n-1)/2]}, s_{[(n-1)/2]}~~~~,~~~~~~ e_{n/2}  \\right\\},\n\\] where \\(e_{n/2}\\) is excluded if \\(n\\) is odd,\ncan be used to project the data and obtain the spectral decomposition\nThus, the periodogram is given by the projection coefficients and represents the contribution of the jth harmonic to the total sum of squares, as illustrated by Brockwell and Davis (1991):\n\n\n\nSource\nDegrees of freedom\n\n\n\n\nFrequency \\(\\omega_{0}\\)\n1\n\n\nFrequency \\(\\omega_{1}\\)\n2\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\nFrequency \\(\\omega_{k}\\)\n2\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\nFrequency \\(\\omega_{n/2}=\\pi\\)\n1\n\n\n(excluded if \\(n\\) is odd)\n\n\n\n\\(=========\\)\n\\(======\\)\n\n\nTotal\nn\n\n\n\n\\[\n~~~~\n\\]\nIn JDemetra+, the periodogram of \\(\\mathbf{X} \\in \\mathbb{R}^n\\) is computed for the standardized time series.\n\n\n16.4.0.11 Defining a F-test\nBrockwell and Davis (1991, section 10.2) exploit the fact that the periodogram can be expressed as the projection on the orthonormal basis defined above to derive a test. Thus, under the null hypothesis:\n\n\\(2I(\\omega_{k})= \\| P_{\\bar{sp}_{\\left\\{ c_{k},s_{k} \\right\\}}} \\mathbf{X} \\|^{2} \\sim \\sigma^{2} \\chi^{2}(2)\\), for Fourier frequencies \\(0 < \\omega_{k}=2\\pi k/n < \\pi\\)\n\\(I(\\pi)= \\| P_{\\bar{sp}_{\\left\\{ e_{n/2} \\right\\}}} \\mathbf{X} \\|^{2} \\sim \\sigma^{2} \\chi^{2}(1)\\), for \\(\\pi\\)\n\nBecause \\(I(\\omega_{k})\\) is independent from the projection error sum of squares, we can define our F-test statistic as follows:\n\n\\(\\frac{ 2I(\\omega_{k})}{\\|\\mathbf{X}-P_{\\bar{sp}_{\\left\\{ e_0,c_{k},s_{k} \\right\\}}} \\mathbf{X}\\|^2} \\frac{n-3}{2} \\sim F(2,n-3)\\), for Fourier frequencies \\(0 < \\omega_{k}=2\\pi k/n < \\pi\\)\n\\(\\frac{ I(\\pi)}{\\|\\mathbf{X}-P_{\\bar{sp}_{\\left\\{ e_0,e_{n/2} \\right\\}}} \\mathbf{X}\\|^2} \\frac{n-2}{1} \\sim F(1,n-2)\\), for \\(\\pi\\)\n\nwhere - \\(\\|\\mathbf{X}-P_{\\bar{sp}_{\\left\\{ e_0,c_{k},s_{k} \\right\\}}} \\mathbf{X}\\|^2 = \\sum_{i=1}^{n}\\mathbf{X^2_i}-I(0)-2I(\\omega_{k}) \\sim \\sigma^{2} \\chi^{2}(n-3)\\) for Fourier frequencies \\(0 < \\omega_{k}=2\\pi k/n < \\pi\\) - \\(\\|\\mathbf{X}-P_{\\bar{sp}_{\\left\\{ e_0,e_{n/2} \\right\\}}} \\mathbf{X}\\|^2 = \\sum_{i=1}^{n}\\mathbf{X^2_i}-I(0)-I(\\pi) \\sim \\sigma^{2} \\chi^{2}(n-2)\\) for \\(\\pi\\)\nThus, we reject the null if our F-test statistic computed at a given seasonal frequency (different from \\(\\pi\\)) is larger than \\(F_{1-α}(2,n-3)\\). If we consider \\(\\pi\\), our test statistic follows a \\(F_{1-α}(1,n-2)\\) distribution.\n\n\n16.4.0.12 Seasonality test\nThe implementation of JDemetra+ considers simultaneously the whole set of seasonal frequencies (1, 2, 3, 4, 5 and 6 cycles per year). Thus, the resulting test-statistic is:\n\\[\n\\frac{ 2I(\\pi/6)+ 2I(\\pi/3)+ 2I(2\\pi/3)+ 2I(5\\pi/6)+ \\delta I(\\pi)}{\\left\\|\\mathbf{X}-P_{\\bar{sp}_{\\left\\{ e_0,c_{1},s_{1},c_{2},s_{2},c_{3},s_{3},c_{4},s_{4},c_{5},s_{5}, \\delta e_{n/2} \\right\\}}} \\mathbf{X} \\right\\|^2} \\frac{n-12}{11} \\sim F(11-\\delta,n-12+\\delta)\n\\] where \\(\\delta=1\\) if \\(n\\) is even and 0 otherwise.\nIn small samples, the test performs better when the periodogram is evaluated as the exact seasonal frequencies. JDemetra+ modifies the sample size to ensure the seasonal frequencies belong to the set of Fourier frequencies. This strategy provides a very simple and effective way to eliminate the leakage problem.\nExample of how results are displayed:\n\n\n\nperiodtest\n\n\n\n\n16.4.0.13 References\nBrockwell, P.J., and R.A. Davis (1991). Times Series: Theory and Methods. Springer Series in Statistics."
  },
  {
    "objectID": "M-spectral-analysis.html#spectral-graphs",
    "href": "M-spectral-analysis.html#spectral-graphs",
    "title": "16  Spectral Analysis Principles and Tools",
    "section": "16.5 Spectral graphs",
    "text": "16.5 Spectral graphs\nprobably moove this part to GUI (Tools), just leave a link\ncomment3: start part case studies > spectral graphs\nThis scenario is designed for advanced users interested in an in-depth analysis of time series in the frequency domain using three spectral graphs. Those graphs can also be used as a complementary analysis for a better understanding of the results obtained with some of the tests described above.\nEconomic time series are usually presented in a time domain (X-axis). However, for analytical purposes it is convenient to convert the series to a frequency domain due to the fact that any stationary time series can be expressed as a combination of cosine (or sine) functions. These functions are characterized with different periods (amount of time to complete a full cycle) and amplitudes (maximum/minimum value during the cycle).\nThe tool used for the analysis of a time series in a frequency domain is called a spectrum. The peaks in the spectrum indicate the presence of cyclical movements with periodicity between two months and one year. A seasonal series should have peaks at the seasonal frequencies. Calendar adjusted data are not expected to have peak at with a calendar frequency.\nThe periodicity of the phenomenon at frequency f is \\(\\frac{2\\pi}{f}\\). It means that for a monthly time series the seasonal frequencies \\(\\frac{\\pi}{6}, \\frac{\\pi}{3}, \\frac{\\pi}{2}, \\frac{2\\pi}{3}, \\frac{5\\pi}{6}\\) and \\(\\pi\\) correspond to 1, 2, 3, 4, 5 and 6 cycles per year. For example, the frequency \\(\\frac{\\pi}{3}\\) corresponds to a periodicity of 6 months (2 cycles per year are completed). For the quarterly series there are two seasonal frequencies: \\(\\frac{\\pi}{2}\\) (one cycle per year) and \\(\\pi\\) (two cycles per year). A peak at the zero frequency always corresponds to the trend component of the series. Seasonal frequencies are marked as grey vertical lines, while violet vertical lines represent the trading-days frequencies. The trading day frequency is 0.348 and derives from the fact that a daily component which repeats every seven days goes through 4.348 cycles in a month of average length 30.4375 days. It is therefore seen to advance 0.348 cycles per month when the data are obtained at twelve equally spaced times in 365.25 days (the average length of a year).\nThe interpretation of the spectral graph is rather straightforward. When the values of a spectral graph for low frequencies (i.e. one year and more) are large in relation to its other values it means that the long-term movements dominate in the series. When the values of a spectral graph for high frequencies (i.e. below one year) are large in relation to its other values it means that the series are rather trendless and contains a lot of noise. When the values of a spectral graph are distributed randomly around a constant without any visible peaks, then it is highly probable that the series is a random process. The presence of seasonality in a time series is manifested in a spectral graph by the peaks on the seasonal frequencies.\nSpectral graphs in GUI\n\n\n\nText\n\n\nAuto-regressive spectrum’s properties\n\nThe spectral graphs are available from: Tools → Spectral analysis.\n\n\n\nText\n\n\nTools for spectral analysis\nWhen the first option is chosen JDemetra+ displays an empty Auto-regressive spectrum window. To start an analysis drag a single time series from the Providers window and drop it into the Drop data here area.\n\n\n\nText\n\n\nLaunching an auto-regressive spectrum\nAn auto-regressive spectrum graph available in JDemetra+ is based on the relevant tool from the X-13ARIMA-SEATS program. It shows the spectral density (spectrum) function, which reformulates the content of the stationary time series’ autocovariances in terms of amplitudes at frequencies of half a cycle per month or less. The number of observations, data transformations and other options such as the specification of the frequency grid and the order of the autoregressive polynomial (30 by default) can be specified by opening the Window → Properties from the main menu.\nThe Auto-regressive - Properties window contains the following options:\n\nLog - a log transformation of a time series;\nDifferencing - transforms a data by calculating a regular (order 1,2..) or seasonal (order 4, 12, depending on the time series frequency) differences;\nDifferencing lag - the number of lags that the program will use to take differences. For example, if Differencing lag = 3 then the differencing filter does not apply to the first lag (default) but to the third lag.\nLast years - a number of years at the end of the time series taken to produce autoregresive spectrum. By default, it is 0, which means that the whole time series is considered.\nAuto-regressive polynomial order - the number of lags in the AR model that is used to estimate the spectral density. By default, the order of the autoregressive polynomial is set to 30 lags.\nResolution - the value 1 plots the spectral density estimate for the frequencies \\(\\omega_{j} = \\frac{2\\pi j}{n}\\), where \\(n \\in ( - \\pi;\\pi)\\) is the size of the sample used to estimate the AR model. Increasing this value, which is set to 5 by default, will increase the precision of this grid.\n\nThe seasonality test described above uses an empirical criterion to check whether the series has a seasonal component that is predictable (stable) enough that it can be estimated with reasonable success. The peak in the auto-regressive spectrum has to be greater than the median of the 61 spectrum ordinates and has to exceed the two adjacent spectral values by more than a critical value. When such a case is detected, the test results are displayed in green.\n\n\n\nText\n\n\nAn example of an-auto-regressive spectrum\nThe second spectral graph is a periodogram. To perform the analysis of a single time series using this tool, choose Tools →Spectral analysis → Periodogram and drag and drop a series from the Providers window to the empty Periodogram window.\n\n\n\nText\n\n\nLaunching a periodogram\nThe sample size and data transformations can be specified by opening the Window → Properties, in the main menu. The Periodogram - Properties window contains the following options:\n\nLog - a log transformation of a time series;\nDifferencing - transforms the data by calculating regular (order 1,2..) or seasonal (order 4, 12, depending on the time series frequency) differences;\nDifferencing lag - the number of lags that you will use to take differences. For example, if Differencing lag = 3 then the differencing filter does not apply to the first lag (default) but to the third lag.\nLast years - the number of years at the end of the time series taken to produce periodogram. By default it is 0, which means that the whole time series is considered.\n\n\n\n\nText\n\n\nPeriodogram’s properties\nThe periodogram was one of the earliest tools used for the analysis of time series in the frequency domain. It enables the user to identify the dominant periods (or frequencies) of a time series. In general, the periodogram is a wildly fluctuating estimate of the spectrum with a high variance and is less stable than an auto-regressive spectrum.\n\n\n\nText\n\n\nAn example of a periodogram\nThe third spectral graph is the Tukey spectrum. To perform the analysis of time series using this tool, choose Tools → Spectral analysis → Tukey spectrum and drag and drop a single series from the Providers window to the empty Periodogram window.\n\n\n\nText\n\n\nLaunching a Tukey spectrum\nThe Tukey spectrum estimates the spectral density by smoothing the periodogram.\n\n\n\nText\n\n\nAn example of a Tukey spectrum\nThe options for the Tuckey window can be specified by opening the Window → Properties from the main menu. The Periodogram - Properties window contains the following options:\n\nLog - a log transformation of a time series.\nDifferencing - transforms the data by calculating regular (order 1, 2..) or seasonal (order 4, 12, depending on the time series frequency) differences.\nDifferencing lag - the number of lags that you will use to take differences. For example, if Differencing lag = 3 then the differencing filter does not apply to the first lag (default) but to the third lag.\nTaper part – parameter larger than 0 and smaller or equal to one that shapes the curvature of the smoothing function that is applied to the auto-covariance function.\nWindow length – the size of the window that is used to smooth the auto-covariance function. A value of zero includes the whole series.\nWindow type – it refers to the weighting scheme that it is used to smooth the auto-covariance function. The available windows types (Square, Welch, Tukey, Barlett, Hamming, Parzen) are suitable to estimate the spectral density.\n\n\n\n\nText\n\n\nTukey spectrum’s properties\n\ncomment3: end part case studies > spectral graphs"
  },
  {
    "objectID": "M-reg-arima-modelling.html",
    "href": "M-reg-arima-modelling.html",
    "title": "17  Reg-Arima models",
    "section": "",
    "text": "lot of information might be recycled from the old online documentation cf file 18-Meth-Reg-Arima-Modelling.Rmd where info from the old pages is gathered formulas and tables compatibility with quarto have to be checked before pasting in the book\nIn the chapter on SA, in the pre-adjustment section, we tackle: purpose, principles and results of reg-arima models (tramo or reg-arima)\nObjectives of the chapter: * all the technical non SA specific details * differences (none left ?) between Tramo and Reg-Arima (X13)"
  },
  {
    "objectID": "M-reg-arima-modelling.html#regarima-model",
    "href": "M-reg-arima-modelling.html#regarima-model",
    "title": "17  Reg-Arima models",
    "section": "17.2 RegARIMA model",
    "text": "17.2 RegARIMA model\nThe primary aim of seasonal adjustment is to remove the unobservable seasonal component from the observed series. The decomposition routines implemented in the seasonal adjustment methods make specific assumptions concerning the input series. One of the crucial assumptions is that the input series is stochastic, i.e. it is clean of deterministic effects. Another important limitation derives from the symmetric linear filter used in TRAMO-SEATS and X-13ARIMA-SEATS. A symmetric linear filter cannot be applied to the first and last observations with the same set of weights as for the central observations[^1]. Therefore, for the most recent observations these filters provide estimates that are subject to revisions.\nTo overcome these constrains both seasonal adjustment methods discussed here include a modelling step that aims to analyse the time series development and provide a better input for decomposition purposes. The tool that is frequently used for this purpose is the ARIMA model, as discussed by BOX, G.E.P., and JENKINS, G.M. (1970). However, time series are often affected by the outliers, other deterministic effects and missing observations. The presence of these effects is not in line with the ARIMA model assumptions. The presence of outliers and other deterministic effects impede the identification of an optimal ARIMA model due to the important bias in the estimation of parameters of sample autocorrelation functions (both global and partial)[^3]. Therefore, the original series need to be corrected for any deterministic effects and missing observations. This process is called linearisation and results in the stochastic series that can be modelled by ARIMA.\nFor this purpose both TRAMO and RegARIMA use regression models with ARIMA errors. With these models TRAMO and RegARIMA also produce forecasts."
  },
  {
    "objectID": "M-moving-average-decomposition.html",
    "href": "M-moving-average-decomposition.html",
    "title": "18  Moving average based decomposition",
    "section": "",
    "text": "goal of the chapter : details on X-11 which won’t be in the SA chapter"
  },
  {
    "objectID": "M-moving-average-decomposition.html#x-11-moving-average-based-decomposition",
    "href": "M-moving-average-decomposition.html#x-11-moving-average-based-decomposition",
    "title": "18  Moving average based decomposition",
    "section": "18.2 X-11 moving average based decomposition",
    "text": "18.2 X-11 moving average based decomposition\nA complete documentation of the X-11 method is available in LADIRAY, D., and QUENNEVILLE, B. (2001). The X-11 program is the result of a long tradition of non-parametric smoothing based on moving averages, which are weighted averages of a moving span of a time series (see hereafter). Moving averages have two important drawbacks:\n\nThey are not resistant and might be deeply impacted by outliers;\nThe smoothing of the ends of the series cannot be done except with asymmetric moving averages which introduce phase-shifts and delays in the detection of turning points.\n\nThese drawbacks adversely affect the X-11 output and stimulate the development of this method. To overcome these flaws first the series are modelled with a RegARIMA model that calculates forecasts and estimates the regression effects. Therefore, the seasonal adjustment process is divided into two parts.\n\nIn a first step, the RegARIMA model is used to clean the series from non-linearities, mainly outliers and calendar effects. A global ARIMA model is adjusted to the series in order to compute the forecasts.\nIn a second step, an enhanced version of the X-11 algorithm is used to compute the trend, the seasonal component and the irregular component.\n\n\n\n\nText\n\n\nThe flow diagram for seasonal adjustment with X-13ARIMA-SEATS using the X-11 algorithm.\n\n18.2.0.1 Moving averages\nThe moving average of coefficient \\(\\theta_{i}\\) is defined as:\n\\[\nM\\left( X_{t} \\right) = \\sum_{k = - p}^{+ f}\\theta_{k}X_{t + k}\n\\]\n\\[\n1\n\\]\nThe value at time \\(t\\) of the series is therefore replaced by a weighted average of \\(p\\) “past” values of the series, the current value, and \\(f\\) “future” values of the series. The quantity \\(p + f + 1\\) is called the moving average order. When \\(p\\) is equal to \\(f\\), that is, when the number of points in the past is the same as the number of points in the future, the moving average is said to be centred. If, in addition, \\(\\theta_{- k} = \\theta_{k}\\) for any \\(k\\), the moving average \\(M\\) is said to be symmetric. One of the simplest moving averages is the symmetric moving average of order \\(P = 2p + 1\\) where all the weights are equal to \\(\\frac{1}{P}\\).\nThis moving average formula works well for all time series observations, except for the first \\(p\\) values and last \\(f\\) values. Generally, with a moving average of order \\(p + f + 1\\) calculated for instant \\(t\\) nwith points \\(p\\) in the past and points \\(f\\) in the future, it will be impossible to smooth out the first \\(p\\) values and the last \\(f\\) values of the series because of lack of input to the moving average formula.\nIn the X-11 method, symmetric moving averages play an important role as they do not introduce any phase-shift in the smoothed series. But, to avoid losing information at the series ends, they are either supplemented by ad hoc asymmetric moving averages or applied on the series extended by forecasts.\nFor the estimation of the seasonal component, X-13ARIMA-SEATS uses \\(P \\times Q\\) composite moving averages, obtained by composing a simple moving average of order \\(P\\), which coefficients are all equal to \\(\\frac{1}{P}\\), and a simple moving average of order \\(Q\\), which coefficients are all equal to \\(\\frac{1}{Q}\\).\nThe composite moving averages are widely used by the X-11 method. For an initial estimation of trend X-11 method uses a \\(2 \\times 4\\) moving average in case of a quarterly time series while for a monthly time series a \\(2 \\times 12\\) moving average is applied. The \\(2 \\times 4\\) moving average is an average of order 5 with coefficients \\(\\frac{1}{8}\\left\\{1, 2, 2, 2, 1\\right\\}\\). It eliminates frequency \\(\\frac{\\pi}{2}\\) corresponding to period 4 and therefore it is suitable for seasonal adjustment of the quarterly series with a constant seasonality. The \\(2 \\times 12\\) moving average, with coefficients \\(\\frac{1}{24}\\left\\{1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1\\right\\}\\) that retains linear trends, eliminates order-\\(12\\) constant seasonality and minimises the variance of the irregular component. The \\(2 \\times 4\\) and \\(2 \\times 12\\) moving averages are also used in the X-11 method to normalise the seasonal factors. The composite moving averages are also used to extract the seasonal component. These, which are used in the purely automatic run of the X-11 method (without any intervention from the user) are \\(3 \\times 3\\), \\(3 \\times 5\\) and \\(3 \\times 9\\).\nIn the estimation of the trend also Henderson moving averages are used. These filters have been chosen for their smoothing properties. The coefficients of a Henderson moving average of order \\(2p + 1\\) may be calculated using the formula:\n\\[\n\\theta_{i} = \\frac{315\\left\\lbrack \\left( n - 1 \\right)^{2} - i^{2} \\right\\rbrack\\left\\lbrack n^{2} - i^{2} \\right\\rbrack\\left\\lbrack \\left( n + 1 \\right)^{2} - i^{2} \\right\\rbrack\\left\\lbrack {3n}^{2} - 16 - 11i^{2} \\right\\rbrack}{8n\\left( n^{2} - 1 \\right)\\left( {4n}^{2} - 1 \\right)\\left( {4n}^{2} - 9 \\right)\\left( 4n^{2} - 25 \\right)}\n\\],\n\\[\n2\n\\]\nwhere: \\(n = p + 2\\)\\(n = p + 2\\).\n\n\n18.2.0.2 The basic algorithm of the X-11 method\nThe X-11 method is based on an iterative principle of estimation of the different components using appropriate moving averages at each step of the algorithm. The successive results are saved in tables. The list of the X-11 tables displayed in JDemetra+ is included at the end of this section.\nThe basic algorithm of the X-11 method will be presented for a monthly time series \\(X_{t}\\) that is assumed to be decomposable into trend, seasonality and irregular component according to an additive model \\(X_{t} = TC_{t} + S_{t} + I_{t}\\).\nA simple seasonal adjustment algorithm can be thought of in eight steps. The steps presented below are designed for the monthly time series. In the algorithm that is run for the quarterly time series the \\(2 \\times 4\\) moving average instead of the \\(2 \\times 12\\) moving average is used.\nStep 1: Estimation of Trend by \\(\\mathbf{2 \\times 12}\\) moving average:\n\\[\nTC_{t}^{(1)} = M_{2 \\times 12}(X_{t})\n\\]\n\\[\n3\n\\]\nStep 2: Estimation of the Seasonal-Irregular component:\n\\[\n\\left( S_{t} + I_{t} \\right)^{(1)} = X_{t} - \\text{TC}_{t}^{(1)}\n\\]\n\\[\n4\n\\]\nStep 3: Estimation of the Seasonal component by \\[\n\\mathbf{3 \\times 3}\n\\] moving average over each month:\n\\[\nS_{t}^{(1)} - M_{3 \\times 3}\\left\\lbrack \\left( S_{t} + I_{t} \\right)^{(1)} \\right\\rbrack\n\\]\n\\[\n5\n\\]\nThe moving average used here is a \\(3 \\times 3\\) moving average over \\(5\\) terms, with coefficients \\(\\frac{1}{9} \\left\\{1, 2, 3, 2, 1 \\right\\}\\). The seasonal component is then centred using a \\(2 \\times 12\\) moving average.\n\\[\n\\widetilde{S}_{t}^{(1)} = S_{t}^{(1)} - M_{2 \\times 12}\\left( S_{t}^{(1)} \\right)\n\\]\n\\[\n6\n\\]\nStep 4: Estimation of the seasonally adjusted series:\n\\[\nSA_{t}^{\\left( 1 \\right)} = \\left( \\text{TC}_{t} + I_{t} \\right)^{(1)} = X_{t} - {\\widetilde{S}}_{t}^{(1)}\n\\]\n\\[\n7\n\\]\nThis first estimation of the seasonally adjusted series must, by construction, contain less seasonality. The X-11 method again executes the algorithm presented above, changing the moving averages to take this property into account.\nStep 5: Estimation of Trend by 13-term Henderson moving average:\n\\[\nTC_{t}^{(2)} = H_{13}\\left( \\text{SA}_{t}^{\\left( 1 \\right)} \\right)\n\\]\n\\[\n8\n\\]\nHenderson moving averages, while they do not have special properties in terms of eliminating seasonality (limited or none at this stage), have a very good smoothing power and retain a local polynomial trend of degree \\(2\\) and preserve a local polynomial trend of degree \\(3\\).\nStep 6: Estimation of the Seasonal-Irregular component:\n\\[\n\\left( S_{t} + I_{t} \\right)^{(2)} = X_{t} - \\text{TC}_{t}^{(2)}\n\\]\n\\[\n9\n\\]\nStep 7: Estimation of the Seasonal component by \\(\\mathbf{3 \\times 5}\\) moving average over each month:\n\\[\nS_{t}^{(2)} - M_{3 \\times 3}\\left\\lbrack \\left( S_{t} + I_{t} \\right)^{(2)} \\right\\rbrack\n\\]\n\\[\n10\n\\]\nThe moving average used here is a \\(3 \\times 5\\) moving average over \\(7\\) terms, of coefficients \\[\n\\frac{1}{15} \\left\\{ 1,\\ 2,\\ 3,\\ 3,\\ 3,\\ 2,\\ 1 \\right\\}\n\\] and retains linear trends. The coefficients are then normalised such that their sum over the whole \\(12\\)-month period is approximately cancelled out:\n\\[\n{ \\widetilde{S}}_{t}^{(2)} = S_{t}^{(2)} - M_{2 \\times 12}\\left( S_{t}^{(2)} \\right)\n\\]\n\\[\n11\n\\]\nStep 8: Estimation of the seasonally adjusted series:\n\\[\nSA_{t}^{\\left( 2 \\right)} = \\left(TC_{t} + I_{t} \\right)^{(2)} = X_{t} - {\\widetilde{S}}_{t}^{(2)}\n\\]\n\\[\n12\n\\]\nThe whole difficulty lies, then, in the choice of the moving averages used for the estimation of the trend in steps \\(1\\) and \\(5\\) on the one hand, and for the estimation of the seasonal component in steps \\(3\\) and \\(5\\). The course of the algorithm in the form that is implemented in JDemetra+ is presented in the figure below. The adjustment for trading day effects, which is present in the original X-11 program, is omitted here, as since calendar correction is performed by the RegARIMA model, JDemetra+ does not perform further adjustment for these effects in the decomposition step.\nA workflow diagram for the X-11 algorithm based upon training material from the Deutsche Bundesbank\n\n18.2.0.2.1 The iterative principle of X-11\nTo evaluate the different components of a series, while taking into account the possible presence of extreme observations, X-11 will proceed iteratively: estimation of components, search for disruptive effects in the irregular component, estimation of components over a corrected series, search for disruptive effects in the irregular component, and so on.\nThe Census X-11 program presents four processing stages (A, B, C, and D), plus 3 stages, E, F, and G, that propose statistics and charts and are not part of the decomposition per se. In stages B, C and D the basic algorithm is used as is indicated in the figure below.\nA workflow diagram for the X-11 algorithm implemented in JDemetra+. Source: Based upon training material from the Deutsche Bundesbank\n\nPart A: Pre-adjustments\n\nThis part, which is not obligatory, corresponds in X-13ARIMA-SEATS to the first cleaning of the series done using the RegARIMA facilities: detection and estimation of outliers and calendar effects (trading day and Easter), forecasts and backcasts[^61] of the series. Based on these results, the program calculates prior adjustment factors that are applied to the raw series. The series thus corrected, Table B1 of the printouts, then proceeds to part B.\n\nPart B: First automatic correction of the series\n\nThis stage consists of a first estimation and down-weighting of the extreme observations and, if requested, a first estimation of the calendar effects. This stage is performed by applying the basic algorithm detailed earlier. These operations lead to Table B20, adjustment values for extreme observations, used to correct the unadjusted series and result in the series from Table C1.\n\nPart C: Second automatic correction of the series\n\nApplying the basic algorithm once again, this part leads to a more precise estimation of replacement values of the extreme observations (Table C20). The series, finally “cleaned up”, is shown in Table D1 of the printouts.\n\nPart D: Seasonal adjustment\n\nThis part, at which our basic algorithm is applied for the last time, is that of the seasonal adjustment, as it leads to final estimates:\n\nof the seasonal component (Table D10);\nof the seasonally adjusted series (Table D11);\nof the trend component (Table D12);\nof the irregular component (Table D13).\n\n\n\nPart E: Components modified for large extreme values\n\nParts E includes:\n\nComponents modified for large extreme values;\nComparison the annual totals of the raw time series and seasonally adjusted time series;\nChanges in the final seasonally adjusted series;\nChanges in the final trend;\nRobust estimation of the final seasonally adjusted series.\n\nThe results from part E are used in part F to calculate the quality measures.\n\nPart F: Seasonal adjustment quality measures\n\nPart F contains statistics for judging the quality of the seasonal adjustment. JDemetra+ presents selected output for part F, i.e.:\n\nM and Q statistics;\nTables.\n\n\n\nPart G: Graphics\n\nPart G presents spectra estimated for:\n\nRaw time series adjusted a priori (Table B1);\nSeasonally adjusted time series modified for large extreme values (Table E2);\nFinal irregular component adjusted for large extreme values (Table E3).\n\nOriginally, graphics were displayed in character mode. In JDemetra+, these graphics are replaced favourably by the usual graphics software.\nThe Henderson moving average and the trend estimation\nIn iteration B (Table B7), iteration C (Table C7) and iteration D (Table D7 and Table D12) the trend component is extracted from an estimate of the seasonally adjusted series using Henderson moving averages. The length of the Henderson filter is chosen automatically by X-13ARIMA-SEATS in a two-step procedure.\nIt is possible to specify the length of the Henderson moving average to be used. X-13ARIMA-SEATS provides an automatic choice between a 9-term, a 13-term or a 23-term moving average. The automatic choice of the order of the moving average is based on the value of an indicator called \\(\\frac{\\overline{I}}{\\overline{C}}\\) ratio which compares the magnitude of period-on-period movements in the irregular component with those in the trend. The larger the ratio, the higher the order of the moving average selected. Moreover, X-13ARIMA-SEATS allows the user to choose manually any odd‑numbered Henderson moving average. The procedure used in each part is very similar; the only differences are the number of options available and the treatment of the observations in the both ends of the series. The procedure below is applied for a monthly time series.\nIn order to calculate \\(\\frac{\\overline{I}}{\\overline{C}}\\) ratio a first decomposition of the SA series (seasonally adjusted) is computed using a 13-term Henderson moving average.\nFor both the trend (\\(C\\)) and irregular (\\(I\\)) components, the average of the absolute values for monthly growth rates (multiplicative model) or for monthly growth (additive model) are computed. They are denoted as \\(\\overline{C}\\) and \\(\\overline{I}\\), receptively, where \\(\\overline{C} = \\frac{1}{n - 1}\\sum_{t = 2}^{n}\\left| C_{t} - C_{t - 1} \\right|\\) and \\(\\overline{I} = \\frac{1}{n - 1}\\sum_{t = 2}^{n}\\left| I_{t} - I_{t - 1} \\right|\\).\nThen the value of \\(\\frac{\\overline{I}}{\\overline{C}}\\) ratio is checked and in iteration B:\n\nIf the ratio is smaller than 1, a 9-term Henderson moving average is selected;\nOtherwise, a 13-term Henderson moving average is selected.\n\nThen the trend is computed by applying the selected Henderson filter to the seasonally adjusted series from Table B6. The observations at the beginning and at the end of the time series that cannot be computed by means of symmetric Henderson filters are estimated by ad hoc asymmetric moving averages.\nIn iterations C and D:\n\nIf the ratio is smaller than 1, a 9-term Henderson moving average is selected;\nIf the ratio is greater than 3.5, a 23-term Henderson moving average is selected.\nOtherwise, a 13-term Henderson moving average is selected.\n\nThe trend is computed by applying selected Henderson filter to the seasonally adjusted series from Table C6, Table D7 or Table D12, accordingly. At the both ends of the series, where a central Henderson filter cannot be applied, the asymmetric ends weights for the 7 term Henderson filter are used.\n\n\n18.2.0.2.2 Choosing the composite moving averages when estimating the seasonal component\nIn iteration D, Table D10 shows an estimate of the seasonal factors implemented on the basis of the modified SI (Seasonal – Irregular) factors estimated in Tables D4 and D9bis. This component will have to be smoothed to estimate the seasonal component; depending on the importance of the irregular in the SI component, we will have to use moving averages of varying length as in the estimate of the Trend/Cycle where the \\(\\frac{\\overline{I}}{\\overline{C}}\\) ratio was used to select the length of the Henderson moving average. The estimation includes several steps.\nStep 1: Estimating the irregular and seasonal components\nAn estimate of the seasonal component is obtained by smoothing, month by month and therefore column by column, Table D9bis using a simple 7-term moving average, i.e. of coefficients \\(\\frac{1}{7} \\left\\{1,\\ 1,\\ 1,\\ 1,\\ 1,\\ 1,\\ 1\\right\\}\\). In order not to lose three points at the beginning and end of each column, all columns are completed as follows. Let us assume that the column that corresponds to the month is composed of \\(N\\) values \\(\\left\\{ x_{1},\\ x_{2},\\ x_{3},\\ \\ldots x_{N - 1},\\ x_{N} \\right\\}\\). It will be transformed into a series \\(\\left\\{ {x_{- 2},x_{- 1}{,x}_{0},x}_{1},\\ x_{2},\\ x_{3},\\ \\ldots x_{N - 1},\\ x_{N},x_{N + 1},\\ x_{N + 1},\\ x_{N + 2},\\ x_{N + 3} \\right\\}\\) with \\(x_{- 2} = x_{- 1} = x_{0} = \\frac{x_{1} + x_{2} + x_{3}}{3}\\) and \\(x_{N + 1} = x_{N + 2} = x_{N + 3} = \\frac{x_{N} + x_{N - 1} + x_{N - 2}}{3}\\). We then have the required estimates: \\(S = M_{7}(D9bis)\\) and \\(I = D9bis - S\\).\nStep 2: Calculating the Moving Seasonality Ratios\nFor each \\(i^{\\text{th}}\\) month the mean annual changes for each component is obtained by calculating \\[\n{\\overline{S}}_{i} = \\frac{1}{N_{i} - 1}\\sum_{t = 2}^{N_{i}}\\left| S_{i,t} - S_{i,t - 1} \\right|\n\\] and \\[\n{\\overline{I}}_{i} = \\frac{1}{N_{i} - 1}\\sum_{t = 2}^{N_{i}}\\left| I_{i,t} - I_{i,t - 1} \\right|\n\\], where \\(N_{i}\\) refers to the number of months \\(\\text{i}\\)in the data, and the moving seasonality ratio of month \\(i\\): \\[\nMSR_{i} = \\frac{\\ {\\overline{I}}_{i}}{ {\\overline{S}}_{i}}\n\\]. These ratios are presented in Details of the Quality Measures node under the Decomposition (X11) section. These ratios are used to compare the year-on-year changes in the irregular component with those in the seasonal component. The idea is to obtain, for each month, an indicator capable of selecting the appropriate moving average for the removal of any noise and providing a good estimate of the seasonal factor. The higher the ratio, the more erratic the series, and the greater the order of the moving average should be used. As for the rest, by default the program selects the same moving average for each month, but the user can select different moving averages for each month.\nStep 3: Calculating the overall Moving Seasonality Ratio\nThe overall Moving Seasonality Ratio is calculated as follows:\n\\[\n\\text{MSR}_{i} = \\frac{\\sum_{i}^{}{N_{i}\\ }\\ {\\overline{I}}_{i}}{\\sum_{i}^{}N_{i}{\\overline{S}}_{i}}\n\\]\n\\[\n13\n\\]\nStep 4: Selecting a moving average and estimating the seasonal component\nDepending on the value of the ratio, the program automatically selects a moving average that is applied, column by column (i.e. month by month) to the Seasonal/Irregular component in Table D8 modified, for extreme values, using values in Table D9.\nThe default selection procedure of a moving average is based on the Moving Seasonality Ratio in the following way:\n\nIf this ratio occurs within zone A (MSR < 2.5), a \\(3 \\times 3\\) moving average is used; if it occurs within zone C (3.5 < MSR < 5.5), a \\(3 \\times 5\\) moving average is selected; if it occurs within zone E (MSR  6.5), a \\(3 \\times 9\\) moving average is used;\nIf the MSR occurs within zone B or D, one year of observations is removed from the end of the series, and the MSR is recalculated. If the ratio again occurs within zones B or D, we start over again, removing a maximum of five years of observations. If this does not work, i.e. if we are again within zones B or D, a \\(3 \\times 5\\) moving average is selected.\n\nThe chosen symmetric moving average corresponds, as the case may be 5 (\\(3 \\times 3\\)), 7 (\\(3 \\times 5\\)) or 11 (\\(3 \\times 9\\) \\(3 \\times 9\\)) terms, and therefore does not provide an estimate for the values of seasonal factors in the first 2 (or 3 or 5) and the last 2 (or 3 or 5) years. These are then calculated using associated asymmetric moving averages.\nMoving average selection procedure, source: DAGUM, E. B.(1999)\n\n\n18.2.0.2.3 Identification and replacement of extreme values\nX-13ARIMA-SEATS detects and removes outliers in the RegARIMA part. However, if there is a seasonal heteroscedasticity in a time series i.e. the variance of the irregular component is different in different calendar months. Examples for this effect could be the weather and snow-dependent output of the construction sector in Germany during winter, or changes in Christmas allowances in Germany and resulting from this a transformation in retail trade turnover before Christmas. The ARIMA model is not on its own able to cope with this characteristic. The practical consequence is given by the detection of additional extreme values by X-11. This may not be appropriate if the seasonal heteroscedasticity is produced by political interventions or other influences. The ARIMA models assume a constant variance and are therefore not by themselves able to cope with this problem. Choosing longer (in the case of diverging weather conditions in the winter time for the construction sector) or shorter filters (in the case of a changing pattern of retail trade turnover in the Christmas time) may be reasonable in such cases. It may even be sensible to take into account the possibility of period-specific (e.g. month-specific) standard deviations, which can be done by changing the default settings of the calendarsigma parameter (see Specifications-X13 section). The value of the calendarsigma parameter will have an impact on the method of calculation of the moving standard deviation in the procedure for extreme values detection presented below.\nStep 1: Estimating the seasonal component\nThe seasonal component is estimated by smoothing the SI component separately for each period using a \\(3 \\times 3\\) moving average, i.e.:\n\\[\n  \\frac{1}{9} \\times \\begin{Bmatrix}\n  1,0,0,0,0,0,0,0,0,0,0,0, \\\\\n  2,0,0,0,0,0,0,0,0,0,0,0, \\\\\n  3,0,0,0,0,0,0,0,0,0,0,0, \\\\\n  2,0,0,0,0,0,0,0,0,0,0,0, \\\\\n  1,0,0,0,0,0,0,0,0,0,0,0, \\\\\n  \\end{Bmatrix}\n\\]\n\\[\n14\n\\]\nStep 2: Normalizing the seasonal factors\nThe preliminary seasonal factors are normalized in such a way that for one year their average is equal to zero (additive model) or to unity (multiplicative model).\nStep 3: Estimating the irregular component\nThe initial normalized seasonal factors are removed from the Seasonal-Irregular component to provide an estimate of the irregular component.\nStep 4: Calculating a moving standard deviation\nBy default, a moving standard deviation of the irregular component is calculated at five-year intervals. Each standard deviation is associated with the central year used to calculate it. The values in the central year, which in the absolute terms deviate from average by more than the Usigma parameter are marked as extreme values and assigned a zero weight. After excluding the extreme values the moving standard deviation is calculated once again.\nStep 5: Detecting extreme values and weighting the irregular\nThe default settings for assigning a weight to each value of irregular component are:\n\nValues which are more than Usigma (2.5, by default) standard deviations away (in the absolute terms) from the 0 (additive) or 1 (multiplicative) are assigned a zero weight;\nValues which are less than 1.5 standard deviations away (in the absolute terms) from the 0 (additive) or 1 (multiplicative) are assigned a full weight (equal to one);\nValues which lie between 1.5 and 2.5 standard deviations away (in the absolute terms) from the 0 (additive) or 1 (multiplicative) are assigned a weight that varies linearly between 0 and 1 depending on their position.\n\nThe default boundaries for the detection of the extreme values can be changed with LSigma and USigma parameters\nStep 6: Adjusting extreme values of the seasonal-irregular component\nValues of the SI component are considered extreme when a weight less than 1 is assigned to their irregular. Those values are replaced by a weighted average of five values:\n\nThe value itself with its weight;\nThe two preceding values, for the same period, having a full weight(if available);\nThe next two values, for the same period, having full a weight (if available).\n\nWhen the four full-weight values are not available, then a simple average of all the values available for the given period is taken.\nThis general algorithm is used with some modification in parts B and C for detection and replacement of extreme values.\n\n\n18.2.0.2.4 X-11 tables\nThe list of tables produced by JDemetra+ is presented below. It is not identical to the output produced by the original X-11 program.\nPart A – Preliminary Estimation of Outliers and Calendar Effects.\nThis part includes prior modifications to the original data made in the RegARIMA part:\n\nTable A1 – Original series;\nTable A1a – Forecast of Original Series;\nTable A2 – Leap year effect;\nTable A6 – Trading Day effect (1 or 6 variables);\nTable A7 – The Easter effect;\nTable A8 – Total Outlier Effect;\nTable A8i – Additive outlier effect;\nTable A8t – Level shift effect;\nTable A8s – Transitory effect;\nTable A9 – Effect of user-defined regression variables assigned to the seasonally adjusted series or for which the component has not been defined;\nTable 9sa – Effect of user-defined regression variables assigned to the seasonally adjusted series;\nTable9u – Effect of user-defined regression variables for which the component has not been defined.\n\nPart B – Preliminary Estimation of the Time Series Components:\n\nTable B1 – Original series after adjustment by the RegARIMA model;\nTable B2 – Unmodified Trend (preliminary estimation using composite moving average);\nTable B3 – Unmodified Seasonal – Irregular Component (preliminary estimation);\nTable B4 – Replacement Values for Extreme SI Values;\nTable B5 – Seasonal Component;\nTable B6 – Seasonally Adjusted Series;\nTable B7 – Trend (estimation using Henderson moving average);\nTable B8 – Unmodified Seasonal – Irregular Component;\nTable B9 – Replacement Values for Extreme SI Values;\nTable B10 – Seasonal Component;\nTable B11 – Seasonally Adjusted Series;\nTable B13 – Irregular Component;\nTable B17 – Preliminary Weights for the Irregular;\nTable B20 – Adjustment Values for Extreme Irregulars.\n\nPart C – Final Estimation of Extreme Values and Calendar Effects:\n\nTable C1 – Modified Raw Series;\nTable C2 – Trend (preliminary estimation using composite moving average);\nTable C4 – Modified Seasonal – Irregular Component;\nTable C5 – Seasonal Component;\nTable C6 – Seasonally Adjusted Series;\nTable C7 – Trend (estimation using Henderson moving average);\nTable C9 – Seasonal – Irregular Component;\nTable C10 – Seasonal Component;\nTable C11 – Seasonally Adjusted Series;\nTable C13 – Irregular Component;\nTable C20 – Adjustment Values for Extreme Irregulars.\n\nPart D – Final Estimation of the Different Components:\n\nTable D1 – Modified Raw Series;\nTable D2 – Trend (preliminary estimation using composite moving average);\nTable D4 – Modified Seasonal – Irregular Component;\nTable D5 – Seasonal Component;\nTable D6 – Seasonally Adjusted Series;\nTable D7 – Trend (estimation using Henderson moving average);\nTable D8 – Unmodified Seasonal – Irregular Component;\nTable D9 – Replacement Values for Extreme SI Values;\nTable D10 – Final Seasonal Factors;\nTable D10A – Forecast of Final Seasonal Factors;\nTable D11 – Final Seasonally Adjusted Series;\nTable D11A – Forecast of Final Seasonally Adjusted Series;\nTable D12 – Final Trend (estimation using Henderson moving average);\nTable D12A – Forecast of Final Trend Component;\n\n\n\nTable D13 – Final Irregular Component;\nTable D16 – Seasonal and Calendar Effects;\nTable D16A – Forecast of Seasonal and Calendar Component;\nTable D18 – Combined Calendar Effects Factors.\n\nPart E – Components Modified for Large Extreme Values:\n\nTable E1 – Raw Series Modified for Large Extreme Values;\nTable E2 – SA Series Modified for Large Extreme Values;\nTable E3 – Final Irregular Component Adjusted for Large Extreme Values;\nTable E11 – Robust Estimation of the Final SA Series.\n\nPart F – Quality indicators:\n\nTable F2A – Changes, in the absolute values, of the principal components;\nTable F2B – Relative contribution of components to changes in the raw series;\nTable F2C – Averages and standard deviations of changes as a function of the time lag;\nTable F2D – Average duration of run;\nTable F2E – I/C ratio for periods span;\nTable F2F – Relative contribution of components to the variance of the stationary part of the original series;\nTable F2G – Autocorrelogram of the irregular component.\n\n\n\n\n18.2.0.3 Filter length choice\nA seasonal filter is a weighted average of a moving span of fixed length within a time series that can be used to remove a fixed seasonal pattern. X-13ARIMA-SEATS uses several of these filters, according to the needs of the different stages of the program. As only X-13ARIMA-SEATS allows the user to manually select seasonal filters, this case study can be applied only to the X-13ARIMA-SEATS specifications.\nThe automatic seasonal adjustment procedure uses the default options to select the most appropriate moving average. However there are occasions when the user will need to specify a different seasonal moving\naverage to that identified by the program. For example, if the SI values do not closely follow the seasonal component, it may be appropriate to use a shorter moving average. Also the presence of sudden breaks in the seasonal pattern – e.g. due to changes in the methodology – can negatively impact on the automatic selection of the most appropriate seasonal filter. In such cases the usage of short seasonal filters in the selected months or quarters can be considered. Usually, a shorter seasonal filter \\((3 \\times 1)\\) allows seasonality to change very rapidly over time. However, a very short seasonal filter should not normally be used, as it might often lead to large revisions as new data becomes available. If a short filter is to be used it will usually be limited to one month/quarter with a known reason for wanting to capture a rapidly changing seasonality.\nIn the standard situation one seasonal filter is applied to all individual months/quarters. The estimation of seasonal movements is therefore based on the sample windows of equal lengths for each individual month/quarter (i.e. for each month/quarter the seasonal filter length or the number of years representing the major part of the seasonal filter weights is identical). This approach relies on the assumption that the number of past periods in which the conditions causing seasonal behaviour are sufficiently homogenous is the same in all months/quarters. However, this assumption does not always hold. Seasonal causes may change in one month, while staying the same in others1. For instance, seasonal heteroskedasticity might require different filter lengths in different months or quarters.\nAnother interesting example is industrial production in Germany. It can be influenced by school holidays, since many employees have school-age children, which interrupt their working pattern during these school holidays. Consequently, businesses may temporarily suspend or lower production during these periods. Since school holidays do not occur at the same time throughout Germany and their timing varies from year to year in the individual federal states, the effect is not completely captured by seasonal adjustment. And since school holidays are treated as usual working days, these effects are not captured by calendar adjustment either. The majority of school holidays in Germany can take place either in July or in August. This yields higher variances in the irregular component for these months compared to the rest of the year. Therefore, in this case a longer seasonal filter is used for these months to account for this.\nAnother example might be given by German retail trade. Due to changes in the consumers’ behaviour around Christmas – possibly more gifts of money – the seasonal peak in December has become steadily less pronounced. To account for this moving seasonality, shorter seasonal filters in December than during the rest of the year need to be applied.\nJDemetra+ offers the options to assign a different seasonal filter length to each period (month or quarter). The program offers these options in the single spec mode as well as in the multispec mode, albeit they are available only in the Specifications window, after a document is created.\n\n\n18.2.1 M-stats\nThe details about the measures are given below.\n\n\\(M1\\) measures the contribution of the irregular component to the total variance. When it is above 1 some changes in outlier correction should be considered.\n\\(M2\\), which is a very similar to \\(M1\\), is calculated on the basis of the contribution of the irregular component to the stationary portion of the variance. When it is above 1, some changes in an outlier correction should be considered.\n\\(M3\\) compares the irregular to the trend taken from a preliminary estimate of the seasonally adjusted series. If this ratio is too large, it is difficult to separate the two components from each other. When it is above 1 some changes in outlier correction should be considered.\n\\(M4\\) tests the randomness of the irregular component. A value above 1 denotes a correlation in the irregular component. In such case a shorter seasonal moving average filter should be considered.\n\\(M5\\) is used to compare the significance of changes in the trend with that in the irregular. When it is above 1 some changes in outlier correction should be considered.\n\\(M6\\) checks the \\(\\text{SI}\\) (seasonal – irregular components ratio). If annual changes in the irregular component are too small in relation to the annual changes in the seasonal component, the \\(3 \\times 5\\) seasonal filter used for the estimation of the seasonal component is not flexible enough to follow the seasonal movement. In such case a longer seasonal moving average filter should be considered. It should be stressed that \\(M6\\) is calculated only if the \\(3 \\times 5\\) filter has been applied in the model.\n\\(M7\\) is the combined test for the presence of an identifiable seasonality. The test compares the relative contribution of stable and moving seasonality2.\n\\(M8\\) to \\(M11\\) measure if the movements due to the short-term quasi-random variations and movements due to the long-term changes are not changing too much over the years. If the changes are too strong then the seasonal factors could be erroneous. In such case a correction for a seasonal break or the change of the seasonal filter should be considered.\n\nThe \\(Q\\) statistic is a composite indicator calculated from the \\(M\\) statistics.\nEdit : problem with tables display\n\\[Q = \\frac{10M1 + 11M2 + 10M3 + 8M4 + 11M5 + 10M6 + 18M7 + 7M8 + 7M9 + 4M10 + 4M11}{100}\\] [1]\n\\(Q = Q - M2\\) (also called \\(Q2\\)) is the \\(Q\\) statistic for which the \\(M2\\) statistics was excluded from the formula, i.e.:\n\\[Q - M2 = \\frac{10M1 + 10M3 + 8M4 + 11M5 + 10M6 + 18M7 + 7M8 + 7M9 + 4M10 + 4M11}{89}\\] [2]\nIf a time series does not cover at least 6 years, the \\(M8\\), \\(M9\\), \\(M10\\) and \\(M11\\) statistics cannot be calculated. In this case the \\(Q\\) statistic is computed as:\n\\[Q = \\frac{14M1 + 15M2 + 10M3 + 8M4 + 11M5 + 10M6 + 32M7}{100}\\]\nThe model has a satisfactory quality if the \\(Q\\) statistic is lower than 1.\nThe tables displayed in the Quality measures → Details node correspond to the F-set of tables produced by the original X-11 algorithm. To facilitate the analysis of the results, the numbers and the names of the tables are given under each table following the convention used in LADIRAY, D., and QUENNEVILLE, B. (1999).\n\n\n18.2.2 Detailed tables\nThe first table presents the average percent change without regard to sign of the percent changes (multiplicative model) or average differences (additive model) over several periods (from 1 to 12 for a monthly series, from 1 to 4 for a quarterly series) for the following series:\n\n\\(O\\) – Original series (Table A1);\n\\(\\text{CI}\\) – Final seasonally adjusted series (Table D11);\n\\(I\\) – Final irregular component (Table D13);\n\\(C\\) – Final trend (Table D12);\n\\(S\\) – Final seasonal factors (Table D10);\n\\(P\\) – Preliminary adjustment coefficients, i.e. regressors estimated by the RegARIMA model (Table A2);\n\\(TD\\& H\\) – Final calendar component (Tables A6 and A7);\n\\(\\text{Mod.O}\\) – Original series adjusted for extreme values (Table E1);\n\\(\\text{Mod.CI}\\) – Final seasonally adjusted series corrected for extreme values (Table E2);\n\\(\\text{Mod.I}\\) – Final irregular component adjusted for extreme values (Table E3).\n\nIn the case of an additive decomposition, for each component the average absolute changes over several periods are calculated as3:\n\\[\n\\text{Component}_{d} = \\frac{1}{n - d}\\sum_{t = d + 1}^{n}|Table_{t} - Table_{t - d}|\n\\] [4]\nwhere:\n\\(d\\) – time lag in periods (from a monthly time series \\(d\\) varies from to 4 or from 1 to 12);\n\\(n\\) – total number of observations per period;\n\\(\\text{Component}\\) – the name of the component;\n\\(\\text{Table}\\) – the name of the table that corresponds to the component.\n\n\n\nText\n\n\nTable F2A – changes, in the absolute values, of the principal components\nNext, Table F2B of relative contributions of the different components to the differences (additive model) or percent changes (multiplicative model) in the original series is displayed. They express the relative importance of the changes in each component. Assuming that the components are independent, the following relation is valid:\n\\[O_{d}^{2} \\approx C_{d}^{2} + S_{d}^{2} + I_{d}^{2} + P_{d}^{2} + {TD\\& H}_{d}^{2}\\]. [5]\nIn order to simplify the analysis, the approximation can be replaced by the following equation:\n\\[O_{d}^{*2} = C_{d}^{2} + S_{d}^{2} + I_{d}^{2} + P_{d}^{2} + {TD\\& H}_{d}^{2}\\]. [6]\nThe notation is the same as for Table F2A. The column \\(\\text{Total}\\) denotes total changes in the raw time series.\nData presented in Table F2B indicate the relative contribution of each component to the percent changes (differences) in the original series over each span, and are calculated as:\n\\(\\frac{I_{d}^{2}}{O_{d}^{*2}}\\),\n\\(\\frac{C_{d}^{2}}{O_{d}^{*2}}\\),\n\\(\\frac{S_{d}^{2}}{O_{d}^{*2}}\\),\n\\(\\frac{P_{d}^{2}}{O_{d}^{*2}}\\)\nand \\(\\frac{TD\\& H_{d}^{2}}{O_{d}^{*2}}\\)\nwhere: \\(O_{d}^{*2} = I_{d}^{2} + C_{d}^{2} + S_{d}^{2} + P_{d}^{2}{+ TD\\& H}_{d}^{2}\\).\nThe last column presents the Ratio calculated as:\n\\(100 \\times \\frac{O_{d}^{*2}}{O_{d}^{2}}\\),\nwhich is an indicator of how well the approximation\n\\({(O_{d}^{*})}^{2} \\approx O_{d}^{2}\\)\nholds.\n\n\n\nText\n\n\nTable F2B – relative contribution of components to changes in the raw series\nWhen an additive decomposition is used, Table F2C presents the average and standard deviation of changes calculated for each time lag \\(d\\), taking into consideration the sign of the changes of the raw series and its components. In case of a multiplicative decomposition the respective table shows the average percent differences and related standard deviations.\n\n\n\nText\n\n\nTable F2C – Averages and standard deviations of changes as a function of the time lag\nAverage duration of run is an average number of consecutive monthly (or quarterly) changes in the same direction (no change is counted as a change in the same direction as the preceding change). JDemetra+ displays this indicator for the seasonally adjusted series, for the trend and for the irregular component.\n\n\n\nText\n\n\nTable F2D – Average duration of run\nThe \\(\\frac{I}{C}\\) ratios for each value of time lag \\(d\\), presented in Table F2E, are computed on a basis of the data in Table F2A.\n\n\n\nText\n\n\nTable F2E – \\(\\frac{\\mathbf{I}}{\\mathbf{C}}\\mathbf{\\ }\\)ratio for periods span\nThe relative contribution of components to the variance of the stationary part of the original series is calculated for the irregular component (\\(I\\)), trend made stationary4 (\\(C\\)), seasonal component (\\(S\\)) and calendar effects (TD&H). The short description of the calculation method is given in LADIRAY, D., and QUENNEVILLE, B. (1999).\n\n\n\nText\n\n\nTable F2F – Relative contribution of components to the variance of the stationary part of the original series\nThe last table shows the autocorrelogram of the irregular component from Table D13. In the case of multiplicative decomposition it is calculated for time lags between 1 and the number of periods per year +2 using the formula5:\n\\[\n\\text{Corr}_{k}I = \\frac{\\sum_{t = k + 1}^{N}{(I_{t} - 1)(I_{t - k} - 1)}}{\\sum_{t = 1}^{N}{(I_{t} - 1)}^{2}}]\n\\] [7]\nwhere \\(N\\) is number of observations in the time series and \\(\\text{k}\\) the lag.\n\n\n\nText\n\n\nTable F2G – Autocorrelation of the irregular component\nThe Cochran test is design to identify the heterogeneity of a series of variances. X-13-ARIMA-SEATS uses this test in the extreme value detection procedure to check if the irregular component is heteroskedastic. In this procedure the standard errors of the irregular component are used for an identification of extreme values. If the null hypothesis that for all the periods (months, quarters) the variances of the irregular component are identical is rejected, the standard errors will be computed separately for each period (in case the option Calendarsigma=signif has been selected).\n\n\n\nText\n\n\nCochran test\nFor each \\(i^{\\text{th}}\\) month we will be looking at the mean annual changes for each component by calculating:\n\\[\n{\\overline{S}}_{i} = \\frac{1}{N_{i} - 1}\\sum_{t = 2}^{N_{i}}|S_{i,t} - S_{i,t - 1}|\n\\]\nand\n\\[\n{\\overline{I}}_{i} = \\frac{1}{N_{i} - 1}\\sum_{t = 2}^{N_{i}}| I_{i,t} - I_{i,t - 1}|\n\\],\nwhere \\(N_{i}\\) refers to the number of months \\(\\text{i}\\) in the data, and the moving seasonality ratio of month \\(i\\):\n\\[\nMSR_{i} = \\frac{\\overline{I}_{i}}{\\overline{S}_{i}}\n\\]\nThese ratios are published in Table D9A in X13ARIMA-SEATS software. In JDemetra+ they are presented in the details of the quality measures.\nThe Moving Seasonality Ratio (MSR) is used to measure the amount of noise in the Seasonal-Irregular component. By studying these values, the user can select for each period the seasonal filter that is the most suitable given the noisiness of the series.\n\n\n\nText\n\n\nTable D9a – Moving seasonality ratios"
  },
  {
    "objectID": "M-local-regression-decomposition.html",
    "href": "M-local-regression-decomposition.html",
    "title": "19  Local regression decomposition",
    "section": "",
    "text": "goal of the chapter : details on STL which are not in the SA chapter"
  },
  {
    "objectID": "M-SEATS-decomposition.html",
    "href": "M-SEATS-decomposition.html",
    "title": "20  SEATS decomposition",
    "section": "",
    "text": "In the original software SEATS can be used either with TRAMO, operating on the input received from the latter, or alone, fitting an ARIMA model to the series.↩︎\nGÓMEZ, V., and MARAVALL, A. (1998).↩︎\nGÓMEZ, V., and MARAVALL, A. (1997).↩︎\nGÓMEZ, V., and MARAVALL, A. (2001a).↩︎\nFor description of the spectrum see section Spectral Analysis.↩︎\nMARAVALL, A. (1995).↩︎\nDescription based on KAISER, R., and MARAVALL, A. (2000) and MARAVALL, A. (2008c).↩︎\nFor details see MARAVALL, A., CAPORELLO, G., PÉREZ, D., and LÓPEZ, R. (2014).↩︎\nIn JDemetra+ this argument is called Trend boundary.↩︎\nThe AR roots close to or at the trading day frequency generates a stochastic trading day component. A stochastic trading day component is always modelled as a stationary ARMA(2,2), where the AR part contains the roots close to the TD frequency, and the MA(2) is obtained from the model decomposition (MARAVALL, A., and PÉREZ, D. (2011)). This component, estimated by SEATS, is not implemented by the current version of JDemetra+.↩︎\nThe term pseudo-spectrum is used for a non-stationary time series, while the term spectrum is used for a stationary time series.↩︎\nIf the ARIMA model estimated in TRAMO does not accept an admissible decomposition, SEATS replaces it with a decomposable approximation. The modified model is therefore used to decompose the series. There are also other rare situations when the ARIMA model chosen by TRAMO is changed by SEATS. It happens when, for example, the ARIMA models generate unstable seasonality or produce a senseless decomposition. Such examples are discussed by MARAVALL, A. (2009).↩︎\nHILLMER, S.C., and TIAO, G.C. (1982).↩︎\nGÓMEZ, V., and MARAVALL, A. (2001a).↩︎\nHILLMER, S.C., and TIAO, G.C. (1982).↩︎\nMARAVALL, A. (1986).↩︎\nIbid.↩︎\nKAISER, R., and MARAVALL, A. (2000).↩︎\nThe choice of the estimation method is controlled by the Method parameter, explained in the SEATS specification section.↩︎\nMARAVALL, A. (2008c).↩︎\nMARAVALL, A. (1995).↩︎\nMARAVALL, A., and PLANAS, C. (1999).↩︎\nMARAVALL, A. (1998).↩︎\nGÓMEZ, V., and MARAVALL, A. (2001a).↩︎\nIbid.↩︎\nKAISER, R., and MARAVALL, A. (2000).↩︎\nMARAVALL, A. (1995).↩︎\nMARAVALL, A. (2009).↩︎\nThe section is based on KAISER, R., and MARAVALL, A. (2000).↩︎\nSee section PsiE-weights. For further details see MARAVALL, A. (2008).↩︎"
  },
  {
    "objectID": "M-tests.html#overview",
    "href": "M-tests.html#overview",
    "title": "21  Tests",
    "section": "21.2 Overview",
    "text": "21.2 Overview\nin this chapter description of tests available vias JD+ algorithms (and some indications on what is also available in R)\ntests are mentioned when relevant in the chapters dedicated to algorithms description, but comprehensive explanations are here"
  },
  {
    "objectID": "M-tests.html#tests-on-residuals",
    "href": "M-tests.html#tests-on-residuals",
    "title": "21  Tests",
    "section": "21.3 Tests on residuals",
    "text": "21.3 Tests on residuals\ntable with all tests by purpose and accessibility\n\nTests on Residuals\n\n\nTest\nPurpose\nGUI\nR package\n\n\n\n\nLjung-Box\nautocorrelation\n\n\n\n\nBox-Pierce\nautocorrelation\n\n\n\n\nDoornik-Hansen\nnormality\n\n\n\n\n\n\n\n\n\n\n\n\n21.3.1 Ljung-Box\nThe Ljung-Box Q-statistics are given by:\n\\[\n  \\text{LB}\\left( k \\right) = n \\times (n + 2) \\times \\sum_{k = 1}^{K}\\frac{\\rho_{a,k}^{2}}{n - k}\n  \\], [1]\nwhere \\(\\rho_{a,k}^{2}\\) is the autocorrelation coefficient at lag \\(k\\) of the residuals \\({\\widehat{a}}_{t}\\), \\(n\\) is the number of terms in differenced (? differenciated ?) series, \\(K\\) is the maximum lag being considered, set in JDemetra+ to \\(24\\) (monthly series) or \\(8\\) (quarterly series).\nIf the residuals are random (which is the case for residuals from a well specified model), they will be distributed as \\(\\chi_{(K - m)}^{2}\\), where \\(m\\) is the number of parameters in the model which has been fitted to the data. (edit: not the residuals, but \\(\\widehat{\\rho}\\) )\nThe Ljung-Box and Box-Pierce tests sometimes fail to reject a poorly fitting model. Therefore, care should be taken not to accept a model on a basis of their results. For the description of autocorrelation concept see section Autocorrelation function and partial autocorrelation function.\n\n\n21.3.2 Box-Pierce\nThe Box-Pierce Q-statistics are given by:\n\\[\\text{BP}\\left( k \\right) = n\\sum_{k = 1}^{K}\\rho_{a,k}^{2}\\]\nwhere:\n\\(\\rho_{a,k}^{2}\\) is the autocorrelation coefficient at lag \\(k\\) of the residuals \\({\\widehat{a}}_{t}\\).\n\\(n\\) is the number of terms in differenced (differenciated?) series;\n\\(K\\) is the maximum lag being considered, set in JDemetra+ to \\(24\\) (monthly series) or \\(8\\) (quarterly series).\nIf the residuals are random (which is the case for residuals from a well specified model), they will be distributed as \\(\\chi_{(K - m)}^{2}\\) degrees of freedom, where \\(m\\) is the number of parameters in the model which has been fitted to the data.(edit: same as above)\n\n\n21.3.3 Dornik-Hansen\nThe Doornik-Hansen test for multivariate normality (DOORNIK, J.A., and HANSEN, H. (2008)) is based on the skewness and kurtosis of multivariate data that is transformed to ensure independence. It is more powerful than the Shapiro-Wilk test for most tested multivariate distributions1.\nThe skewness and kurtosis are defined, respectively, as: \\[s = \\frac{m_{3}}{\\sqrt{m_{2}}^{3}}\\] and $k = \\frac{m_{4}}{m_{2}^{2}},\\ $where: \\(m_{i} = \\frac{1}{n}\\sum_{i = 1}^{n}{(x_{i}}{- \\overline{x})}^{i}\\) \\(\\overline{x} = \\frac{1}{n}\\sum_{i = 1}^{n}x_{i}\\) and \\(n\\) is a number of (non-missing) residuals.\nThe Doornik-Hansen test statistic derives from SHENTON, L.R., and BOWMAN, K.O. (1977) and uses transformed versions of skewness and kurtosis.\nThe transformation for the skewness \\(s\\) into \\(\\text{z}_{1}\\) is as in D’AGOSTINO, R.B. (1970):\n\\[\n  \\beta = \\frac{3(n^{2} + 27n - 70)(n + 1)(n + 3)}{(n - 2)(n + 5)(n + 7)(n + 9)}\n  \\]\n\\[\n  \\omega^{2} = - 1 + \\sqrt{2(\\beta - 1)}\n  \\]\n\\[\n  \\delta = \\frac{1}{\\sqrt{\\log{(\\omega}^{2})}}\n  \\]\n\\[\n  y = s\\sqrt{\\frac{(\\omega^{2} - 1)(n + 1)(n + 3)}{12(n - 2)}}\n  \\]\n\\[\n  z_{1} = \\delta log(y + \\sqrt{y^{2} - 1})\n  \\]\nThe kurtosis \\(k\\) is transformed from a gamma distribution to \\(\\chi^{2}\\), which is then transformed into standard normal \\(z_{2}\\) using the Wilson-Hilferty cubed root transformation:\n\\[\n  \\delta = (n - 3)(n + 1)(n^{2} + 15n - 4)\n  \\]\n\\[\n  a = \\frac{(n - 2)(n + 5)(n + 7)(n^{2} + 27n - 70)}{6\\delta}\n  \\]\n\\[\n  c = \\frac{(n - 7)(n + 5)(n + 7)(n^{2} + 2n - 5)}{6\\delta}\n  \\]\n\\[\n  l= \\frac{(n + 5)(n + 7)({n^{3} + 37n}^{2} + 11n - 313)}{12\\delta}\n  \\]\n\\[\n  \\alpha = a + c \\times s^{2}\n  \\]\n\\[\n  \\chi = 2l(k - 1 - s^{2})\n  \\]\n\\[\n  z_{2} = \\sqrt{9\\alpha}\\left( \\frac{1}{9\\alpha} - 1 + \\sqrt[3]{\\frac{\\chi}{2\\alpha}} \\right)\n  \\]\nFinally, the Doornik-Hansen test statistic is defined as the sum of squared transformations of the skewness and kurtosis. Approximately, the test statistic follows a \\(\\chi^{2}\\)distribution, i.e.:\n\\[DH = z_{1}^{2} + z_{2}^{2}\\sim\\chi^{2}(2)\\]"
  },
  {
    "objectID": "M-tests.html#seasonality-tests",
    "href": "M-tests.html#seasonality-tests",
    "title": "21  Tests",
    "section": "21.4 Seasonality tests",
    "text": "21.4 Seasonality tests\ntable with all tests by purpose and accessibility\n\nSeasonality tests\n\n\n\n\n\n\n\n\nTest\nPurpose\nGUI\nR package\n\n\n\n\nQS test\nAutocorrelation at seasonal lags\n\n\n\n\nF-test with seasonal dummies\nStable seasonality\n\n\n\n\nIdentification of spectral peaks\nSeasonal frequencies\n\n\n\n\nFriedman test\nStable seasonality\n\n\n\n\nTwo-way variance analysis\nMoving seasonality\n\n\n\n\n\n\n21.4.0.1 QS Test on autocorrelation at seasonal lags\nThe QS test is a variant of the Ljung-Box test computed on seasonal lags, where we only consider positive auto-correlations\nMore exactly,\n\\[ QS=n \\left(n+2\\right)\\sum_{i=1}^k\\frac{\\left[ \\max  \\left(0, \\hat\\gamma_{i \\cdot l}\\right)\\right]^2}{n-i \\cdot l}\\]\nwhere \\[k=2\\], so only the first and second seasonal lags are considered. Thus, the test would checks the correlation between the actual observation and the observations lagged by one and two years. Note that \\[l=12\\] when dealing with monthly observations, so we consider the autocovariances \\[\\hat\\gamma_{12}\\] and \\[\\hat\\gamma_{24}\\] alone. In turn, \\[k=4\\] in the case of quarterly data.\nUnder H0, which states that the data are independently distributed, the statistics follows a \\[\\chi \\left(k\\right)\\] distribution. However, the elimination of negative correlations makes it a bad approximation. The p-values would be given by \\(P(\\chi^{2}\\left( k \\right) > Q)\\) for \\(k = 2\\). As \\({P(\\chi}^{2}(2)) > 0.05 = 5.99146\\) and \\({P(\\chi}^{2}(2)) > 0.01 = 9.21034\\), \\(QS > 5.99146\\) and \\(QS > 9.21034\\) would suggest rejecting the null hypothesis at \\(95\\%\\) and \\(99\\%\\) significance levels, respectively.\n\n\n21.4.0.2 Modification\nMaravall (2012) proposes approximate the correct distribution (p-values) of the QS statistic using simulation techniques. Using 1000K replications of sample size 240, the correct critical values would be 3.83 and 7.09 with confidence levels of \\(95\\%\\) and \\(99\\%\\), respectively (lower than the 5.99146 and 9.21034 shown above). For each of the simulated series, he obtains the distribution by assuming \\(QS=0\\) when \\[\\hat\\gamma_{12}\\], so in practice this test will detect seasonality only when any of these conditions hold: - Statistically significant positive autocorrelation at lag 12 - Nonnegative sample autocorrelation at lag 12 and statistically significant positive autocorrelation at lag 24\n\n\n21.4.0.3 Use\nThe test can be applied directly to any series by selecting the option Statistical Methods >> Seasonal Adjustment >> Tools >> Seasonality Tests. This is an example of how results are displayed for the case of a monthly series:\n\n\n\nqs\n\n\nThe test can be applied to the input series before any seasonal adjustment method has been applied. It can also be applied to the seasonally adjusted series or to the irregular component.\n\n\n21.4.0.4 References\n\nLJUNG G. M. and G. E. P. BOX (1978). “On a Measure of a Lack of Fit in Time Series Models”. Biometrika 65 (2): 297–303. doi:10.1093/biomet/65.2.297\nMARAVALL, A. (2011). “Seasonality Tests and Automatic Model Identification in Tramo-Seats”. Manuscript\nMARAVALL, A. (2012). “Update of Seasonality Tests and Automatic Model Identification in TRAMO-SEATS”. Bank of Spain (November 2012)\n\n\n\n21.4.1 F-test on seasonal dummies\nThe F-test on seasonal dummies checks for the presence of deterministic seasonality. The model used here uses seasonal dummies (mean effect and 11 seasonal dummies for monthly data, mean effect and 3 for quarterly data) to describe the (possibly transformed) time series behaviour. The test statistic checks if the seasonal dummies are jointly statistically not significant. When this hypothesis is rejected, it is assumed that the deterministic seasonality is present and the test results are displayed in green.\nThis test refers to Model-Based $\\chi^{2}\\ $and F-tests for Fixed Seasonal Effects proposed by LYTRAS, D.P., FELDPAUSCH, R.M., and BELL, W.R. (2007) that is based on the estimates of the regression dummy variables and the corresponding t-statistics of the RegARIMA model, in which the ARIMA part of the model has a form (0,1,1)(0,0,0). The consequences of a misspecification of a model are discussed in LYTRAS, D.P., FELDPAUSCH, R.M., and BELL, W.R. (2007).\nFor a monthly time series the RegARIMA model structure is as follows:\n\\[\\left( 1 - B \\right)\\left( y_{t} - \\beta_{1}M_{1,t} - \\ldots - \\beta_{11}M_{11,t} - \\gamma X_{t} \\right) = \\mu + (1 - B)a_{t}\n\\], [1] \nwhere:\n\\[\nM_{j,t} =\n\\begin{cases}\n1 & \\text{ in month } j = 1, \\ldots, 11 \\\\\n- 1 & \\text{ in December}\\\\\n0 & \\text{ otherwise}\n\\end{cases} \\text{ - dummy variables;}\n\\]\n\\(y_{t}\\) – the original time series;\n\\(B\\) – a backshift operator;\n\\(X_{t}\\) – other regression variables used in the model (e.g. outliers, calendar effects, user-defined regression variables, intervention variables);\n\\(\\mu\\) – a mean effect;\n\\(a_{t}\\) – a white-noise variable with mean zero and a constant variance.\nIn the case of a quarterly series the estimated model has a form:\n\\[\\left( 1 - B \\right)\\left( y_{t} - \\beta_{1}M_{1,t} - \\ldots - \\beta_{3}M_{3,t} - \\gamma X_{t} \\right) = \\mu + (1 - B)a_{t}\\], [2] \nwhere:\n\\[\nM_{j,t} =\n\\begin{cases}\n1 & \\text{ in quarter} j = 1, \\ldots, 3 \\\\\n- 1 & \\text{ in the fourth quarter}\\\\\n0 & \\text{ otherwise}\n\\end{cases} \\text{ - dummy variables;}\n\\]\nOne can use the individual t-statistics to assess whether seasonality for a given month is significant, or a chi-squared test statistic if the null hypothesis is that the parameters are collectively all zero. The chi-squared test statistic is \\({\\widehat{\\chi}}^{2} = {\\widehat{\\beta}}^{'}{\\lbrack Var(\\widehat{\\beta})}^{\\ })^{- 1}\\rbrack{\\widehat{\\beta}}^{\\ }\\) in this case compared to critical values from a \\(\\chi^{2}\\left( \\text{df} \\right)\\)-distribution, with degrees of freedom $df = 11\\ $(monthly series) or \\(df = 3\\) (quarterly series). Since the \\({Var(\\widehat{\\beta})}^{\\ }\\) computed using the estimated variance of \\(\\alpha_{t}\\) may be very different from the actual variance in small samples, this test is corrected using the proposed \\(\\text{F}\\) statistic:\n\\[\n  F = \\frac{ {\\widehat{\\chi}}^{2}}{s - 1} \\times \\frac{n - d - k}{n - d}\n  \\]\nwhere \\(n\\) is the sample size, \\(d\\) is the degree of differencing, s is time series frequency (12 for a monthly series, 4 for a quarterly series) and \\(k\\) is the total number of regressors in the RegARIMA model (including the seasonal dummies \\(\\text{M}_{j,t}\\) and the intercept).\nThis statistic follows a \\[F_{s - 1,n - d - k}\\] distribution under the null hypothesis.\n\n\n21.4.2 Identification of spectral peaks\nlink to relevant part in spectral analysis chapter ?\n\n\n21.4.3 Friedman test for stable seasonality test\nThe Friedman test is a non-parametric method for testing that samples are drawn from the same population or from populations with equal medians. The significance of the month (or quarter) effect is tested. The Friedman test requires no distributional assumptions. It uses the rankings of the observations. If the null hypothesis of no stable seasonality is rejected at the 0.10% significance level then the series is considered to be seasonal and the test’s outcome is displayed in green.\nThe test statistic is constructed as follows. Consider first the matrix of data \\[ \\left\\{x_{ij}\\right\\}_{n \\times k} \\] with \\[ n \\] rows (the blocks, i.e. number of years in the sample), \\[ k \\] columns (the treatments, i.e. either 12 months or 4 quarters, depending on the frequency of the data).\nThe data matrix needs to be replaced by a new matrix \\[ \\left\\{r_{ij}\\right\\}_{n \\times k} \\], where the entry \\[ r_{ij} \\] is the rank of \\[ x_{ij} \\] within block \\[ i \\] .\nThe test statistic is given by\n\\[\nQ=\\frac{SS_t}{SS_e}\n\\]\nwhere \\[ SS_t=n \\sum_{j=1}^{k}(\\bar{r}_{.j}-\\bar{r})^2 \\] and \\[ SS_e=\\frac{1}{n(k-1)} \\sum_{i=1}^{n}\\sum_{j=1}^{k}(r_{ij}-\\bar{r})^2 \\] It represents the variance of the average ranking across treatments j relative to the total.\nUnder the hypothesis of no seasonality, all months can be equally treated. For the sake of completeness: - \\[ \\bar{r}_{.j} \\] is the average ranks of each treatment (month) j within each block (year) - The average rank is given by \\[ \\bar{r}= \\frac{1}{nk}\\sum_{i=1}^{n}\\sum_{j=1}^{k}(r_{ij})\\]\nFor large \\[ n \\] or \\[ k \\] , i.e. n > 15 or k > 4, the probability distribution of \\[ Q \\] can be approximated by that of a chi-squared distribution. Thus, the p-value is given by \\[ P( \\chi^2_{k-1}>Q) \\] .\n\n21.4.3.1 Use\nThe test can be applied directly to any series by selecting the option Statistical Methods >> Seasonal Adjustment >> Tools >> Seasonality Tests. This is an example of how results are displayed for the case of a monthly series:\n\n\n\nfriedman\n\n\nIf the null hypothesis of no stable seasonality is rejected at the 1% significance level, then the series is considered to be seasonal and the outcome of the test is displayed in green.\nThe test can be applied to the input series before any seasonal adjustment method has been applied. It can also be applied to the seasonally adjusted series or to the irreguar component. In the case of X-13ARIMA-SEATS, the test is applied to the preliminary estimate of the unmodified Seasonal-Irregular component2 (time series shown in Table B3). In this estimate, the number of observations is lower than in the final estimate of the unmodified Seasonal-Irregular component. Thus, the number of degrees of freedom in the stable seasonality test is lower than the number of degrees of freedom in the test for the presence of seasonality assuming stability. For example, X-13ARIMA-SEATS uses a centred moving average of order 12 to calculate the preliminary estimation of trend. Consequently, the first six and last six points in the series are not computed at this stage of calculation. The preliminary estimation of the trend is then used for the calculation of the preliminary estimation of the unmodified Seasonal-Irregular.\n\n\n21.4.3.2 Related tests\n\nWhen using this kind of design for a binary response, one instead uses the Cochran’s Q test.\nKendall’s W is a normalization of the Friedman statistic between 0 and 1.\nThe Wilcoxon signed-rank test is a nonparametric test of non-independent data from only two groups.\n\n\n\n21.4.3.3 References\n\nFriedman, Milton (December 1937). “The use of ranks to avoid the assumption of normality implicit in the analysis of variance”. Journal of the American Statistical Association (American Statistical Association) 32 (200): 675–701. doi:10.2307/2279372. JSTOR 2279372.\nFriedman, Milton (March 1939). “A correction: The use of ranks to avoid the assumption of normality implicit in the analysis of variance”. Journal of the American Statistical Association (American Statistical Association) 34 (205): 109. doi:10.2307/2279169. JSTOR 2279169.\nFriedman, Milton (March 1940). “A comparison of alternative tests of significance for the problem of m rankings”. The Annals of Mathematical Statistics 11 (1): 86–92. doi:10.1214/aoms/1177731944. JSTOR 2235971.\n\n\n\n\n21.4.4 Moving seasonality test\nThe evolutive seasonality test is based on a two-way analysis of variance model. The model uses the values from complete years only. Depending on the decomposition type for the Seasonal – Irregular component it uses [1] (in the case of a multiplicative model) or [2] (in the case of an additive model):\n\\[\n  \\left|\\text{SI}_{\\text{ij}} - 1 \\right| = X_{\\text{ij}} = b_{i} + m_{j} + e_{\\text{ij}}\n  \\], [1] \n\\[\n  \\left| \\text{SI}_{\\text{ij}} \\right| = X_{\\text{ij}} = b_{i} + m_{j} + e_{\\text{ij}}\n  \\], [2] \nwhere:\n\\(m_{j}\\) – the monthly or quarterly effect for \\(j\\)-th period, \\(j = (1,\\ldots,k)\\), where \\(k = 12\\) for a monthly series and \\(k = 4\\) for a quarterly series;\n\\(b_{j}\\) – the annual effect \\(i\\), \\((i = 1,\\ldots,N)\\) where \\(N\\) is the number of complete years;\n\\(e_{\\text{ij}}\\) – the residual effect.\nThe test is based on the following decomposition:\n\\[S^{2} = S_{A}^{2} + S_{B}^{2} + S_{R}^{2},\\] [3] \nwhere:\n\\[\nS^{2} = \\sum_{j = 1}^{k}{\\sum_{i = 1}^{N}\\left( {\\overline{X}}_{\\text{ij}} - {\\overline{X}}_{\\bullet \\bullet} \\right)^{2}}\\\n\\] –the total sum of squares;\n\\[\nS_{A}^{2} = N\\sum_{j = 1}^{k}\\left( {\\overline{X}}_{\\bullet j} - {\\overline{X}}_{\\bullet \\bullet} \\right)^{2}\n\\] – the inter-month (inter-quarter, respectively) sum of squares, which mainly measures the magnitude of the seasonality;\n\\[\nS_{B}^{2} = k\\sum_{i = 1}^{N}\\left( {\\overline{X}}_{i \\bullet} - {\\overline{X}}_{\\bullet \\bullet} \\right)^{2}\n\\] – the inter-year sum of squares, which mainly measures the year-to-year movement of seasonality;\n\\[\nS_{R}^{2} = \\sum_{i = 1}^{N}{\\sum_{j = 1}^{k}\\left( {\\overline{X}}_{\\text{ij}} - {\\overline{X}}_{i \\bullet} - {\\overline{X}}_{\\bullet j} - {\\overline{X}}_{\\bullet \\bullet} \\right)^{2}}\n\\] – the residual sum of squares.\nThe null hypothesis $H_{0}\\ $is that \\(b_{1} = b_{2} = ... = b_{N}\\) which means that there is no change in seasonality over the years. This hypothesis is verified by the following test statistic:\n\\[\n   F_{M} = \\frac{\\frac{S_{B}^{2}}{(n - 1)}}{\\frac{S_{R}^{2}}{(n - 1)(k - 1)}}\n   \\]\nwhich follows an \\(F\\)-distribution with \\(k - 1\\) and \\(n - k\\) degrees of freedom.\n\n\n21.4.5 Combined seasonality test\nThis test combines the Kruskal-Wallis test along with test for the presence of seasonality assuming stability (\\(F_{S}\\)), and evaluative seasonality test for detecting the presence of identifiable seasonality (\\(F_{M}\\)). Those three tests are calculated using the final unmodified SI component. The main purpose of the combined seasonality test is to check whether the seasonality of the series is identifiable. For example, the identification of the seasonal pattern is problematic if the process is dominated by highly moving seasonality3. The testing procedure is shown in the figure below.\n\n\n\nText\n\n\nCombined seasonality test, source: LADIRAY, D., QUENNEVILLE, B. (2001)"
  },
  {
    "objectID": "Buffer.html",
    "href": "Buffer.html",
    "title": "23  Theoretical spectral density of an ARIMA model",
    "section": "",
    "text": "In the bottom part the panel the ARIMA model used by TRAMO is presented using symbolic notation \\((P,D,Q)(PB,DB,QB)\\). Estimated parameters’ coefficients (regular and seasonal AR and MA) are shown in closed form (i.e. using the backshift operator1 \\(B\\)). For each regular AR root (i.e. the solution of the characteristic equation) the argument and modulus are given.\n\n\n\nText\n\n\nThe details of ARIMA model used for modelling\nFor each regular AR root the argument and modulus are also reported (if present, i.e. if \\(\\mathbf{P > 0}\\)) to inform to which time series component the regular roots would be assigned.\n\n\n\n\n\nA backshift operator \\(B\\) is defined as: (\\(B^{k}x_{t} = x_{t - k})\\). It is used to denote lagged series.↩︎"
  }
]