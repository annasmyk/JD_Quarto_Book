# Reg-Arima models
 

## Overview

 lot of information might be recycled from the old online documentation
cf file 
18-Meth-Reg-Arima-Modelling.Rmd
where info from the old pages is gathered 
formulas and tables compatibility with quarto have to be checked before pasting in the book

## RegARIMA model

* in champ 4 on SA, inte pre-adj section: priciples, purpose, results
* here all the technical non algo specific details
* differences (none left ?) between Ts and ReGarima 

The primary aim of seasonal adjustment is to remove the unobservable
seasonal component from the observed series. The decomposition routines
implemented in the seasonal adjustment methods make specific assumptions
concerning the input series. One of the crucial assumptions is that the
input series is stochastic, i.e. it is clean of deterministic effects.
Another important limitation derives from the symmetric linear filter
used in TRAMO-SEATS and X-13ARIMA-SEATS. A symmetric linear filter
cannot be applied to the first and last observations with the same set
of weights as for the central observations[^1]. Therefore, for the most
recent observations these filters provide estimates that are subject to
revisions.

To overcome these constrains both seasonal adjustment methods discussed
here include a modelling step that aims to analyse the time series
development and provide a better input for decomposition purposes. The
tool that is frequently used for this purpose is the ARIMA model, as
discussed by BOX, G.E.P., and JENKINS, G.M. (1970). However, time series
are often affected by the outliers, other deterministic effects and
missing observations. The presence of these effects is not in line with
the ARIMA model assumptions. The presence of outliers and other
deterministic effects impede the identification of an optimal ARIMA
model due to the important bias in the estimation of parameters of
[sample autocorrelation functions](../theory/ACF_and_PACF.html) (both global and partial)[^3].
Therefore, the original series need to be corrected for any
deterministic effects and missing observations. This process is called
linearisation and results in the stochastic series that can be modelled
by ARIMA.

For this purpose both TRAMO and RegARIMA use regression models with
ARIMA errors. With these models TRAMO and RegARIMA also produce
forecasts.


  $z_{t} = y_{t}\mathbf{\beta} + x_{t}$,   \[1\] <!--- \[7.5\] -->
  

where:

$z_{t}$ is the original series;

$\mathbf{\beta} = (\beta_{1},\ldots,\beta_{n})$ -- a vector of
regression coefficients;

$y_{t} = (y_{1t},\ldots,y_{\text{nt}})$ -- $\text{n}$ regression
variables (the trading day variables, the leap year effect, outliers,
the Easter effect, ramps, intervention variables, user-defined
variables);

$x_{t}\ $-- a disturbance that follows the general ARIMA process:
$\phi\left( B \right)\delta\left( B \right)x_{t} = \theta(B)a_{t}$;
$\phi\left( B \right)$,$\ \theta(B)$ and $\delta\left( B \right)$ are
the finite polynomials in $B$; $a_{t}$ is a white-noise variable with
zero mean and a constant variance.

The polynomial $\phi\left( B \right)$ is a stationary autoregressive
(AR) polynomial in $B$, which is a product of the stationary regular AR
polynomial in $B$ and the stationary seasonal polynomial
in $\text{B}^{s}$:[^4]

  
  $$\phi\left( B \right) = \phi_{p}\left( B \right)\Phi_{p_{s}}\left( B^{s} \right) = (1 + \phi_{1}B + \ldots + \phi_{p}B^{p})(1 + \Phi_{1}B^{s} + \ldots + \Phi_{p_{s}}B^{p_{s}s})$$  \[2\] <!--- \[7.6\] -->
  

where:

$\text{p}$ -- the number of regular AR terms, (in
JDemetra+$\ \ p \leq 3)$;

$p_{s}$ -- the number of seasonal AR terms, (in
JDemetra+ $p_{s} \leq 1)$;

$\text{s}$ -- the number of observations per year (frequency of the
time series).

The polynomial $\theta(B)$ is an invertible moving average (MA)
polynomial in $B$, which is a product of the invertible regular MA
polynomial in $B$ and the invertible seasonal MA polynomial
in $\text{B}$:

  
  $\theta\left( B \right) = \theta_{q}\left( B \right)\Theta_{q_{s}}\left( B^{s} \right) = (1 + \theta_{1}B + \ldots + \theta_{q}B^{q})(1 + \Theta_{1}B^{s} + \ldots + \Theta_{q_{s}}B^{q_{s}s})$,  \[3\]<!--- \[7.7\] -->


where:

$q$ -- the number of regular MA terms, ($q \leq 3)$;

$q_{s}$ -- the number of seasonal MA terms, ($q_{s} \leq 1)$.

An MA polynomial (1+$\theta_{1}B + \ldots + \theta_{q}B^{q})\ $is
invertible if all its roots lie outside the unit circle, i.e. their
modulus is greater than 1. When invertible an MA process,
$x_{t} = \theta\left( B \right)a_{t},$ can be expressed as

An AR process of infinite order, $\pi\left( B \right)x_{t} = a_{t}$,
where , $\pi\left( B \right) = \pi_{0} + \pi_{1}B + \ldots$ [^5]

The polynomial $\delta\left( B \right)$ is the non-stationary AR
polynomial in $B$ (unit roots):

  
  $\delta\left( B \right) = {(1 - B)}^{d}{(1 - B^{s})}^{d_{s}}$,   \[4\]<!---\[7.8\]-->
  
where:

$d$ -- regular differencing order, ($d \leq 1)$;

$d_{s}\ $-- seasonal differencing order, ($d_{s} \leq 1)$.

JDemetra+ uses notation: $P,\ D,\ Q,BP,\ BD,\ BQ$ instead of,
respectively:$\ p,\ d,q,p_{s},\ d_{s},\ q_{s}$. Therefore, the structure
of the ARIMA$\ (p,\ d,\ q)(P,\ D,\ Q)$ model is denoted in JDemetra+ as
ARIMA $(P,\ D,\ Q)(BP,\ BD,\ BQ)$.

Both TRAMO and X-12-ARIMA allows for an automatic identification of an
ARIMA model extended for the regression variables. The procedure
includes a test for logarithmic transformation (so-called the test for
the log-level specification), selection of the ARIMA model structure and
regressors. The estimated deterministic effects are removed from the
time series to improve the estimation of the time series components.
Forecasts produced by the ARIMA model provide an input for the linear
filters used in the decomposition step. In summary, the application of
an ARIMA model to the original series vastly improves the quality and
stability of the estimated components. The details of the estimation
procedure of the ARIMA model with regression variables are presented in
the later in this section.

Among the deterministic effects one can distinguish between the calendar
effects and outliers. The calendar effects are discussed in the [Calendar effects](../theory/Calendars.html) section. The
impact of different types of outliers on time series is illustrated with
several examples by FRANSES, P.H. (1998) For example, it was explained
that additive outliers, which are described later in this section, yield
large values of skewness and kurtosis, and hence failure of a normality
test. They also increase the standard error of the estimation
parameters. This effect is especially prominent when the size of the
outlier is considerable.

KAISER, R., and MARAVALL, A. (1999) express the impact of the outliers
on the observed series as:[^6]

  
  $y_{t}^{*} = \sum_{j = 1}^{k}\xi_{j}\left( B \right)\omega_{j}I_{t}^{\left( \tau_{j} \right)} + y_{t}$,   \[5\] <!---\[7.9\]-->
 

where:

$y_{t}^{*}$ -- an observed time series;

$y_{t}$ -- a series that follows the ARIMA model;

$\omega_{j}$ -- an initial impact of the outlier at time
${t = \tau}_{j}$;

$I_{t}^{\left( \tau_{j} \right)}$ -- an indicator variable such that is
1 for ${t = \tau}_{j}$ and 0 otherwise;

$\xi_{j}\left( B \right)$ -- an expression that determines the dynamics
of the outlier occurring at time ${t = \tau}_{j}$;

$B$ -- a backshift operator (i.e. $B^{k}X_{t} = X_{t - k}$).

The optimal choice of regression variables (and/or intervention
variables) requires knowledge from the user about the time series being
modelled[^7]. On the contrary, outliers, frequently used in modelling
seasonal economic time series, can be automatically detected by
JDemetra+ (see [Specifiations - TRAMO](../reference-manual/modelling-spec-tramo.html) section for TRAMO and [Specifiations - ARIMA](../reference-manual/modelling-spec-arima.html) section for RegARIMA). The
procedure described in GÓMEZ, V., and MARAVALL, A. (2001a) identifies
the ARIMA model structure in the presence of deterministic effects.
Therefore, the number of identified outliers may depend on the ARIMA
model estimated on the series.

The types of outliers that can be automatically identified and estimated
by JDemetra+ without any user intervention are:

-   Additive Outlier (AO) -- a point outlier which occurs at a given
     time $t_0$. For the additive outlier
     $\xi_{j}( B) = 1$, which results in the regression
     variable:

  
  $$
  AO_{t}^{t_{0}} = 
  \begin{cases}   
  1  \text{ for }  t = t_{0} \\\\                              
  0  \text{ for }  t \neq t_{0}                            
  \end{cases}
  $$;              \[6\] <!---\[7.10\]-->           
  

-   Level shift (LS) -- a variable for a constant level shift beginning
     at the given time $t_{0}$. For the level shift
     $\xi_{j}\left( B \right) = \frac{1}{1 - B}$, which results in the
     regression variable:

  $$
  LS_{t}^{t_{0}} = 
  \begin{cases}   
  -1  \text{ for }  t < t_{0} \\\\                              
  0  \text{ for }  t \geq t_{0}                            
  \end{cases}
  $$;              \[7\] <!---\[7.11\]-->
                          
  

-   Temporary change[^8] (TC) -- a variable for an effect on the given
     time $t_{0}$ that decays exponentially over the following
     periods. For the temporary change
     $\xi_{j}\left( B \right) = \frac{1}{1 - \delta B}$, which results
     in the regression variable:

  $$
  TC_{t}^{t_{0}} = 
  \begin{cases}   
  0  \text{ for }  t < t_{0} \\\\                              
  \alpha^{t - t_{0}}  \text{ for }  t \geq t_{0}                            
  \end{cases}
  $$;               \[8\]<!---\[7.12\]-->
                         
  

> where $\alpha$ is a rate of decay back to the previous level
> $(0 < \alpha < 1)$.

-   Seasonal outliers (SO) -- a variable that captures an abrupt change
     in the seasonal pattern on the given date $t_{0}\ $and maintains
     the level of the series with a contrasting change spread over the
     remaining periods. It is modelled by the regression variable:

  
  $$
  SO_{t}^{t_{0}} = 
  \begin{cases}   
  0  \text{ for }  t < t_{0} \\\\                              
  1  \text{ for }  t \geq t_{0}, \text{$t$ same month/quarter as $t_{0}$}   \\\\                    \frac{- 1}{(s - 1)} \text{ otherwise}       
  \end{cases}
  $$;               \[9\] <!---\[7.13\]-->
                             
  

where $s$ is a frequency of the time series ($s = 12\ $for a monthly
time series, $s = 4\ $for a quarterly time series).

The shapes generated by the formulas given above are presented in the figure below.



![Text](All_images/UG_A_image1.png)

**Pre-defined outliers built in to JDemetra+**

Within the RegARIMA model it can be also tested if a series of level
shifts cancels out to form a temporary level change effect, which is a
permanent level shift spanned between two given dates. It is modelled by
the regression variable:

  $$
  TLS_{t}^{ {(t}_{0}{,\ t}_{1})} =
  \begin{cases}   
  0  \text{ for }  t < t_{0} \\\\                              
  1  \text{ for }  t_{1} \geq t \geq t_{0}   \\\\                    
  0 \text{ for } t > t_{1}       
  \end{cases}
  $$;               \[10\]<!---\[7.14\]-->
                                           
  

JDemetra+ also identifies other pre-defined regression variables, for
which a necessary input, such as location and process that generates the
variable, is provided by the user. This group includes:

-   Ramp -- a variable for a linear increase or decrease in the level of
    the series over a specified time interval $t_{0}$ to$\ t_{1}$. It is
    modelled by a regression variable:

$$
  RP_{t}^{ {(t}_{0}{,\ t}_{1})} =
  \begin{cases}   
 -1 \text{ for } t \leq t_{0} \\\\                              
 -\frac{t - t_{0}}{t_{1} - t_{0}} - 1 \text{ for } t_{0} < t < t_{1}   \\\\    0 \text{ for } t \geq t_{1}       
  \end{cases}
  $$;               \[11\] <!---\[7.15\]-->
                                             


   Intervention variables which are combinations of five basic structures[^9]:

<!-- -->

-   dummy variables[^10];

-   any possible sequence of ones and zeros;

-   $\frac{1}{(1 - \delta B)}$, $(0 < \delta \leq 1)$;

-   $\frac{1}{(1 - \delta_{s}B^{s})}$, $(0 < \delta_{s} \leq 1)$;

-   $\frac{1}{(1 - B)(1 - B^{s})}$;

where $s$ is frequency of the time series ($s = 12\ $for a monthly time
series, $s = 4\ $for a quarterly time series).

The structures considered by intervention variables allow for generation
of all pre-defined outliers described by \[6\] -- \[11\], as well
as some more sophisticated effects. An example can be a level shift
effect reached after a sequence of damped overshootings and
undershootings, presented in the figure below and denoted there as IV. Another
example of an outlier that can be created with intervention variables is
a pure seasonal outlier ($PSO)$, which, in contrast to the seasonal
outlier described above, does not affect the trend. The set of pure
seasonal outliers is used to model the seasonal change of regime
($\text{SCR}$) effect, which describes a sudden and sustained change in
the seasonal pattern affecting from $t_{0}$ (possibly) all seasons of
the series. It is defined as:

  $$
  SCR_{i,j} =
  \begin{cases}   
 1 \text{ for } t \epsilon j \text{ and } t < t_{0} \\\\                    
 0 \text{ for } t \notin j \text{ or } t \geq t_{0} \\\\    
 -1 \text{ for } t \epsilon s \text{ and } t < t_{0}       
  \end{cases}
  $$;               \[12\]<!---\[7.16\]-->
                     

where $j = 1,\ldots s - 1$.



![Text](All_images/UG_A_image2.png)

**Examples of intervention variables**

#### Automatic model identification procedure in TRAMO

An algorithm for Automatic Model Identification in the Presence of
Outliers (AMI) implemented in TRAMO is based on TSAY, C. (1986) and
CHEN, B.-C., and LIU, L.-M. (1993) with some modifications (see GÓMEZ,
V., and MARAVALL, A. (2001)). It iterates between two stages: the first
is automatic outlier detection and correction and the second automatic
model identification. The parameters of the AMI procedure are described
in the [Specifiations - ARIMA](../reference-manual/modelling-spec-arima.html) section. Unless the parameters are set by the user the program runs
with the default values.

The algorithm starts with the identification of the default model and
pre-testing procedure, where on the first step a test for a log-level
specification is performed. It is based on the maximum likelihood
estimation of the parameter $\lambda$ in the Box-Cox transformation,
which is a power transformation such that the transformed values of the
time series $y\ $are a monotonic function of the observations, i.e.
$$
  y^{\alpha} =
  \begin{cases}   
 \frac{\left( y_{i}^{\alpha} - 1 \right)}{\lambda}, \lambda \neq 0 \\\\       \log{y_{i}^{\alpha}, \lambda = 0}       
  \end{cases}
  $$; 

First, two Airline models (i.e. ARIMA(0,1,1)(0,1,1)) with a mean) are fitted to the time series: one in logs
($\lambda = 0$), other without logs ($\lambda = 1$). The test compares
the sum of squares of the model without logs with the sum of squares
multiplied by the square of the geometric mean of the (regularly and
seasonally) differenced series in the case of the model in logs. Logs
are taken in the case this last function is the minimum[^11]. By
default, both TRAMO and X-12-ARIMA have a slight bias towards the log
transformation.

Next, the test for calendar effects is performed with regressions using
the default model for the noise and, if the model is subsequently
changed, the test is redone. For seasonal series the default model is
the Airline model (ARIMA (0,1,1)(0,1,1)) while for the non-seasonal
series ARIMA (0,1,1)(0,0,0) with a constant term is used. The default
model, which is used as a benchmark model at some next steps of AMI, is
determined by the result of the pre-test for possible presence of
seasonality.

Once these pre-tests have been completed, the original series is
corrected for all pre-specified outliers and regression effects provided
by the user, if any. Next, the order of the differencing
polynomial $\text{δ}\left( B \right)$­ that contains the unit roots is
identified and it is decided whether to specify a mean for the series or
not. The identification of the ARMA model, i.e. the order of
$\phi\left( B \right)\ $and $\text{θ}\left( B \right)\ $ ­is performed
using the Hannan and Rissanen method by the means of minimising the
Bayesian information criterion with some constraints aimed at increasing
the parsimony and favouring balanced models[^12]. This procedure
produces the initial values of the ARIMA model parameters. The search is
done sequentially: for the fixed regular polynomials, the seasonal ones
are obtained, and vice versa. When the estimated roots of the AR and MA
processes are close to each other, [the order of the ARIMA model can be
reduced](../theory/SA_lin.html#cancellation-of-ar-and-ma-factors). 
The identification and estimation of the model is carried
out using Exact Maximum Likelihood (EML) or the Unconditional Least
Squares method.

If the calendar effects were identified in the default model, they are
included in a new ARIMA model provided that these effects are
significant for this model. The estimated residuals from the modified
ARIMA model with fixed parameters and the median absolute deviation of
the standard deviation of these residuals are used in the outlier
detection procedure. For each observation, $t$-tests are computed for
all types of outlier considered in the automatic procedure (AO, TC, LS,
SO), following the procedure proposed in CHEN, B.-C., and LIU, L.-M.
(1993). The program compares$\text{t}$-statistics to a critical value
determined by the series length. If there are outliers, for which the
absolute $t$-values are greater than a critical value, the one with the
greatest absolute$\text{t}$-value is selected. After correcting for
the identified outlier, the process is started again to test if there is
another outlier. The procedure is repeated until for none of the
potential outliers, the $t$-statistic exceeds the critical value. If
outliers are detected, then a multiple regression is performed using the
Kalman filter and the [QR algorithm](../theory/SA_lin.html#least-squares-estimation-by-means-of-the-qr-decomposition)
 to avoid (as much as possible)
masking effects (i.e. detecting spurious outliers) and to correct for
the bias produced in the estimators sequentially obtained[^3]. If there
are outliers for which the absolute $t$-values are greater than the
critical value, the one with the greatest absolute $t$-value is selected
and the algorithm continues to the estimation of the ARMA model
parameters. Otherwise, the algorithm stops[^16]. The estimated residuals
from the final ARIMA model are checked for adequacy against the
estimated residuals produced by the balanced model. The final model
identified by the AMI procedure must show some improvement over the
default model in these residual diagnostics; otherwise, the program will
accept the default model[^7].

#### Automatic model identification procedure in RegARIMA

The original RegARIMA algorithm developed by the U.S. Census Bureau
includes two automatic model selection procedures: *automdl* that is
based on TRAMO and *pickmdl* that originates from X-11-ARIMA[^18]. The
algorithm implementation in JDemetra+ for RegARIMA follows the TRAMO
logic. It is very similar to the TRAMO procedure presented in the
previous section, but contains modifications to make use of the
X-13ARIMA-SEATS estimation procedure, which is different from the one
that TRAMO uses[^7]. The examples of extensions that are specific to
RegARIMA only are: special treatment of the leap year effect in the
multiplicative model, automatic detection of the length of the Easter
effect[^20], option to reduce a series of level shifts to the temporary
level shift. In comparison with TRAMO, there are also differences in the
values of the default parameters. Besides, by default, RegARIMA does not
favour balanced models. The log/level test in RegARIMA is based on the
[Corrected Akaike Information Criterion (AICC)](../theory/SA_lin.html#model-selection-criteria). Similarly, the
estimation of calendar effects is based on AICC. As a result, the model
selected by RegARIMA can differ from the model that TRAMO would select,
especially in the case of series contaminated with some deterministic
effects and/or those, which are modelled with the mixed ARIMA models.

#### Model selection criteria

Model selection criteria are statistical tools for selecting the optimal
order of the ARIMA model. The basic idea behind all these criteria is to
obtain as much explanatory power (measured by the value of the
likelihood function) with only a few parameters. The model selection
criteria essentially choose the model with the best fit, as measured by
the likelihood function, and it is a subject to a penalty term, to
prevent over-fitting that increases with the number of parameters in the
model.[^22] Some of the most well-known information criteria are: Akaike
Information Criterion (AIC)[^23], Corrected Akaike Information Criterion
(AICC)[^24], HQ Information Criterion (HannanQuinn)[^25] and
Schwarz-Bayes Information Criterion (BIC)[^26].

The formulae for the model selection criteria used by JDemetra+ are:

  $$
  AIC_{N} = -2L_{N} + {2n}_{p}
  $$                                          \[13\]<!---\[7.17\]-->
  

$$
AICC_{N} = - 2L_{N} + 2n_{p}\left( 1 - \frac{n_{p} + 1}{N} \right)^{- 1}
$$   \[14\]<!---\[7.18\]-->

$$
HQ_{N} = - 2L_{N} + 2n_{p}\text{ln(ln(N))}
$$                         \[15\]<!---\[7.19\]-->

$$
BIC_{N} = - 2L_{N} + n_{p}\text{logN}
$$                           \[16\]<!---\[7.20\]-->

where:

$\text{N}$-- a number of observations in time series;

$n_{p}$ -- a number of estimate parameters;

$L_{N}$ -- a loglikelihood function.

For each model selection criteria the model with the smaller value is
preferred. As it can be shown that AIC is biased for small samples, it
is often replaced by AICC. To choose the ARIMA model parameters the
original RegARIMA uses AICC while TRAMO uses BIC with some constraints
aimed at increasing the parsimony and favouring balanced models. The
automatic model identification methods implemented in JDemetra+ mostly
use BIC, however other criteria are used as well. It should be noted
that the BIC criterion imposes a greater penalty term than AIC and HQ,
so that BIC tends to select simpler models than those chosen by AIC. The
difference between both criteria can be huge if $N$ is large[^27].

#### Hannan-Rissanen algorithm

The Hannan-Rissanen algorithm[^28] is a penalty function method based on
the BIC criterion, where the estimates of the ARMA model parameters are
computed by means of a linear regression. The Hannan-Rissanen procedure
operates on a stationary transformation of the original series,
therefore only AR and MA parameters are being identified.

In the first step, a high-order$\ AR(m)$, where $m > max(p,q)$, model is
fitted to the time series $X_{t}$. The maximum order of $p$ and $q$ is
given a-priori. In JDemetra+ it is equal to 3 for both $p$ and
$q$. The residuals $$\widehat{a}_{k}$$ from this model are used
to provide estimates of the innovations in the ARMA model $a_{t}$:

  
  $$a_{t} = X_{t} - \sum_{k = 1}^{m}{\widehat{a}_{k}X}_{t - k}$$   \[17\]<!---\[7.21\]-->
  

In the second step the parameters $p$ and $q$ of the ARMA model are
estimated using a least squares linear regression of $X_{t}$ onto
$X_{t - 1},\ldots X_{t - p},a_{t - 1},\ldots a_{t - q}$ for a
combination of values $p$ and $q$. The first step of the procedure is
skipped if the ARMA model fitted in the second step includes only an
autoregressive part.

Finally, the Hannan-Rissanen algorithm selects a pair of $p$ and $q$
values for which $BIC_{p,q}$ is the smallest. $BIC_{p,q}$
is defined as:

 
  $$\text{BIC}_{p,q} = \log\left( \sigma_{p,q}^{2} \right) + \frac{\left( p + q \right)\log\left( n - d \right)}{n - d}$$   \[18\]<!---\[7.22\]-->


where:

$\sigma_{p,q}^{2}$ is the maximum likelihood estimator of $\sigma^{2}$;

$n - d\ $is the number of series in the (regularly and/or) seasonally
differenced series.

The advantage of the Hannan-Rissanen algorithm is the speed of
computation in comparison with an exact likelihood estimation.

#### Initial values for ARIMA model estimation

By default, the initial parameter value in X-13ARIMA-SEATS is 0.1 for
all AR and MA parameters. For the majority of time series this default
value seems to be appropriate. Introducing better initial values (as
might be obtained, e.g., by first fitting the model using conditional
likelihood) could slightly speed up a convergence. Users are allowed to
introduce manually initial values for AR and MA parameters that are then
used to start the iterative likelihood maximization. This is rarely
necessary, and, in general, not recommended. A possible exception to
this occurs if the initial estimates that are likely to be extremely
accurate are already available, such as when one is re-estimating a
model with a small amount of new data added to a time series. However,
the main reason for specifying initial parameter values is to deal with
convergence problems that may arise in difficult estimation
situations[^7].

#### Cancellation of AR and MA factors

A cancellation issue consists in cancelling some factors on both sides
of the ARIMA model. This problem concerns the mixed ARIMA
$(p,d,q)(P,D,Q)$ models (i.e. $p > 0\ $and$\ q > 0$, or $P > 0$ and
$Q > 0$). For example, a cancellation problem occurs with ARMA (1,1)
model, $\left( 1 - \phi B \right)z_{t} = (1 - \theta B)a_{t}$ when
$\phi = \theta\ $as then model is simply of the form: $z_{t} = a_{t}$.
Such model causes problems with convergence of the nonlinear estimation.
For this reason the X-13ARIMA-SEATS and TRAMO-SEATS programs deal with a
cancellation problem by computing zeros of the AR and MA polynomials. As
the cancellation does not need to be exact, the cancellation limit can
be provided by the user[^7].

#### Least squares estimation by means of the QR decomposition

We consider the regression model:


  $y = X\beta + \varepsilon$   \[19\]<!---\[7.23\]-->
  

The least squares problem consists in minimizing the quantity
$\left\| X\beta - y \right\|^{2}$.

Provided that the regression variables are independent, it is possible
to find an orthogonal matrix $Q$, so that $Q \bullet X = \left( \frac{R}{0} \right)$ where $R$ is upper
triangular. 

We have now to minimize:

  $$\left\| QX\beta - Qy \right\|_{2}^{2} = \left\|                                                  
  \left( \frac{R}{0} \right)                                                                                       
  \ \beta - Qy \right\|_{2}^{2} = \left\| R\beta - a \right\|_{2}^{2} + \left\| b \right\|_{2}^{2}$$   \[20\]<!---\[7.24\]-->
  

where $Qy_{0\ldots x - 1} = a$ and
$Qy_{x\ldots n - 1} = b$.

The minimum of the previous norm is obtained by setting
$\beta = R^{- 1}a$. In that case
$\left\| R\ \beta - a \right\|_{2}^{2} = 0$. The residuals obtained by
this procedure are then $b$, as defined above.

It should be noted that the $\text{QR}$ factorization is not unique, and
that the final residuals also depend on the order of the regression
variables (the columns of $X$).



[^1]: DAGUM, E.B. (1980).
[^3]: GÓMEZ, V., and MARAVALL, A. (2001b).
[^4]: The notation used by TRAMO for the polynomials is different from the one commonly used in the literature, for example in HAMILTON, J.D. (1994) the AR polynomial is denoted as .
[^5]: BOX G.E.P., JENKINS, G.M., and REINSEL, G.C. (2007).
[^6]: KAISER, R., and MARAVALL, A. (1999).
[^7]: ‘X-13ARIMA-SEATS Reference Manual’ (2015).
[^8]: In the TRAMO-SEATS method this type of outlier is called a transitory change.
[^9]: GÓMEZ, V., and MARAVALL, A. (1997).
[^10]: Dummy variable is the variable that takes the values 0 or 1 to indicate the absence or presence of some effect.
[^11]: GÓMEZ, V., and MARAVALL, A. (2010).
[^12]: Parsimonious models are those which have a great deal of explanatory power using a relatively small number of parameters. Balanced models are models for which the order of the combined AR and differencing operators is equal to the order of the combined MA operator (see GÓMEZ, V., and MARAVALL, A. (1997)). A model is said to be more balanced than a competing model if the absolute difference between the total orders of the AR plus differencing and MA operators is smaller for one model than another. For description of the Hannan-Rissanen algorithm see he respective section above, a well as HANNAN, E.J., and RISSANEN, J. (1982), GÓMEZ, V., and MARAVALL, A. (2001b).
[^15]: GÓMEZ, V., and MARAVALL, A. (2001b).
[^16]: MARAVALL, A. (2000).
[^17]: ‘X-13ARIMA-SEATS Reference Manual’ (2015).
[^18]: DAGUM, E.B. (1988).
[^19]: ‘X-13ARIMA-SEATS Reference Manual’ (2015).
[^20]: The pre-tested options are: one, eight, and fifteen days before Easter.
[^22]: CHATFIELD, C. (2004).
[^23]: AKAIKE, H. (1973).
[^24]: HURVICH, C.M., and TSAI, C. (1989).
[^25]: HANNAN, E.J., and QUINN, B.G. (1979).
[^26]: SCHWARZ, G. (1978).
[^27]: PEÑA, D. (2001).
[^28]: HANNAN, E.J., and RISSANEN, J. (1982), NEWBOLD, D., and BOS, T. (1982).
[^29]: ‘X-13ARIMA-SEATS Reference Manual’ (2015).
[^30]: ‘X-13ARIMA-SEATS Reference Manual’ (2015).

## Autocorrelation function

The correlation is a measure of the strength and the direction of a
linear relationship between two variables. For time series the
correlation can refer to the relation between its observations, e.g.
between the current observation and the observation lagged by a given
number of units. In this case all observations come from one variable,
so similarity between a given time series and a $k$-lagged version of
itself over successive time intervals is called an autocorrelation.

The autocorrelation coefficient at lag $k$ is defined as:

$$\rho\left( k \right) = \frac{\sum_{t = k + 1}^{n}\left( x_{t} - \overline{x} \right)}{\sum_{t = 1}^{n}\left( x_{t} - \overline{x} \right)^{2}}
$$,   \[1\] <!--- \[7.159\]--> 

where:

$x_{t}$ -- time series;

$n$ -- total number of observations;

$\overline{x}$ -- mean of the time series.

The set of autocorrelation coefficients $(k)$ arranged as a function of
$k$ is the autocorrelation function (ACF). The graphical or numerical
representation of the ACF is called an autocorrelogram.

![Text](All_images/UG_A_image20.png)


**Autocorrelation function**

The autocorrelation function is a valuable tool for investigating
properties of an empirical time series.[^91] The assessment of the order
of an AR process simply from the sample ACF is not straightforward.
While for a first-order process the theoretical ACF decreases
exponentially and the sample function is expected to have the similar
shape, for the higher-order processes the ACF maybe a mixture of damper
exponential or sinusoidal functions, which makes the order of the AR
process difficult to identify.[^92] JDemetra+ displays the values of
autocorrelation function for the residuals from the ARIMA model (see
section [Residuals](..\reference-manual/residuals.html)). The ACF graph (figure above), presents autocorrelation
coefficients and the confidence intervals. If the autocorrelation
coefficient is in the confidence interval, it is regarded as not
statistically significant. Therefore, the user should focus on the
values where the value of the ACF is outside the confidence interval. In 
JDemetra+ the confidence interval is indicated by two grey, horizontal,
dotted lines.


**Partial autocorrelation function**

The partial autocorrelation is a tool for the identification and
estimation of the ARIMA model. It is defined as the amount of
correlation between two variables that is not explained by their mutual
correlations with a given set of other variables.

Partial autocorrelation at lag $k$ is defined as the autocorrelation
between $x_{t}$ and $x_{t - k}$ that is not accounted for by lags 1
through to $k$-1, which means that correlations with all the elements up
to lag $k$ are removed. Following this definition, a partial
autocorrelation for lag 1 is equivalent to an autocorrelation.

The partial autocorrelation function (PACF) is the set of partial
autocorrelation coefficients $(k)$ arranged as a function
of $k$. This function can be used to detect the presence of an
autoregressive process in time series and identify the order of this
process. Theoretically, the number of significant lags determines the 
order of the autoregressive process.


![Text](All_images/UG_A_image21.png)


**Partial autocorrelation function**


The PACF graph above, which
is available from the *Tools*$\  \rightarrow \ $*Differencing* menu
presents partial autocorrelation coefficients and the confidence
intervals (two grey, horizontal, dotted lines). If the partial autocorrelation coefficient is in the
confidence interval, it is regarded as statistically insignificant.
Therefore, the user should focus on the values, for which the absolute value
of the PACF is outside the confidence interval. 

[^91]: MAKRIDAKIS, S., WHEELWRIGHT, S.C., and HYNDMAN, R.J. (1998).

[^92]: CHATFIELD, C. (2004). 

## Estimation Of Arma models

The computation of [exact likelihood](../../stats/likelihood/ll.md) requires the evaluation of two main quantities: the determinant of the covariance matrix
 and the sum of the squared residuals.
The different algorithms for the computation of the likelihood of ARMA models provides efficient solutions for those two problems.
The quantity
$$ y' \Omega^-1 y $$ 
is computed by defining a linear transformation of the observations such that
$$ y' \Omega^-1 y = \left(y' T' \right) \left(T y \right) $$ 
An obvious solution will be the use of the inverse of the Cholesky factor of the covariance matrix. However, any transformation
$$ T\sim m \times n $$
such that
$$ \Omega^-1 =  T' T $$
might be considered. Note that m can be larger than n (which means that the transformed observations will not be independent).



JD+ provides several routines for estimating the exact likelihood of ARMA models

| Algorithm | Use |
| --------- | --- | 
| Kalman filter  | Default |
| [Ansley](./ansley.md)  | Large regression models |
| [X12](./x12.md)  | Legacy |
| Ljung-Box  | Deprecated |

## Maximum likelihood estimation
### Likelihood of a multivariate normal distribution 


The pdf of a multivariate normal distribution is:

$$p\left( y \right) = \left( 2 \pi \right)^{-\frac{n}{2}} \vert \Sigma \vert ^{-\frac{1}{2}}e^{ {-\frac{1}{2}y' \Sigma ^{-1} y} } $$

If we set

$$ y' \Sigma ^{-1} y=u'u \:\: or \:\:  L^{-1}y = u $$ 

the log-likelihood is:

$$ l \left( \theta | y \right) =- \frac{1}{2} \left(n \log{2 \pi}+ \log{|\Sigma |} +u'u\right) $$

In most cases, we will use a covariance matrix with a (unknown) scaling factor:

$$ \Sigma = \sigma^2 \Omega $$

If we set

$$ L^{-1}y = e , \quad LL' = \Omega$$ 

the log-likelihood can then be written:

$$ l \left(\theta, \sigma | y \right ) = - \frac{1}{2} \left(n \log{2 \pi}+ n \log {\sigma^2} + \log{|\Omega |} + \frac{1}{\sigma ^2} e'e \right) $$

The scaling factor can be concentrated out of the likelihood. Its estimator is

$$ \hat{\sigma} ^2 = \frac{e' e}{n} $$

so that :

$$ l_c \left( \theta | y \right ) = - \frac{1}{2} \left(n \log{2 \pi} + n\log{\frac{e'e}{n}} + \log{|\Omega |} + n \right) $$

or

$$ l_c \left( \theta | y \right ) = - \frac{n}{2} \left(\log{2 \pi}+ 1 - \log {n} + \log{e'e} + \log{|\Omega |^\frac{1}{n}}\right) $$

Maximizing $ l_c $ is equivalent to minimizing the deviance 

$$ d \left( y | \theta\right ) = e'e |\Omega |^\frac{1}{n} = v'v, \quad where \quad v = \sqrt{ |\Omega |^\frac{1}{n} }\: e$$ 

This last formulation will be used in optimization procedures based on sums of squares (Levenberg-Marquardt and similar algorithms).

### Linear model with gaussian noises

The likelihood is often computed on a linear model
$$ y=X\beta + \mu \quad \mu \sim N\left(0, \sigma^2\Omega\right) $$

The log-likelihood is then
$$ l \left(\theta,\beta , \sigma | y \right ) = - \frac{1}{2} \left(n \log{2 \pi}+ n \log {\sigma^2} + \log{|\Omega |} + \frac{1}{\sigma ^2} \left(y-X\beta \right)'\Omega^{-1}\left(y-X\beta \right) \right) $$

The maximum likelihood estimator of $\beta$ is

$$ \hat{\beta} = \left( X'\Omega^{-1}X\right)^{-1}X'\Omega^{-1}y $$

which is normally distributed with variance
$$ \sigma^2 \left( X'\Omega^{-1}X\right)^{-1} $$

The formulae of the likelihood are still valid, using
$$ e=L^{-1} \left(y-X\hat\beta \right) $$

### Implementation

Those representations of the concentrated likelihood are defined in the interfaces ___demetra.likelihood.ILikelihood___ and ___demetra.likelihood.IConcentratedLikelihood___

#### Correspondance between the elements of the likelihood (see formulae) and the methods of the classes

* $n$ : dim()
* $e'e$ : ssq()
* $e$ : e()
* $\log \|\Omega\|$ : logDeterminant()
* $v$ : v()
* $\|\Omega\|^{\frac{1}{n}}$ : factor() 

### Missing values 

Missing values are not taken into account in the likelihood. More especially, when they are estimated by means of additive outliers, all the different elements of the likelihood (dimension,  determinantal term, coefficients…) should be adjusted to remove their effect.

### Perfect collinearity in X

In the case of perfect collinearity in the linear model, the dimensions of the coefficients and of the related matrices are not modified. However, information related to the redundant variables is set to 0.

### References

_Gomez V. and Maravall A._ (1994): "Estimation, Prediction, and Interpolation for Nonstationary Series With the Kalman Filter", Journal of the American Statistical Association, vol. 89, n° 426, 611-624.


### X-13 implementation

Estimation of the exact likelihood of an ARMA model

We suppose that 
$$ y_t, \quad 1 \le t \le n $$
follows an ARMA model.

The X12 implementation computes the exact likelihood in two main steps

#### Overview

We consider the transformation

$$ z_t = \begin{pmatrix} z_{1t} \\ z_{2t} \end{pmatrix} = \begin{cases} y_t, & 1 \le t \le p \\ \Phi\left(B\right) y_t, & p \lt t \le n\end{cases}$$

It is obvious that 
$$ p\left(y_t\right) = p\left(z_t\right) $$

We will estimate
$$ p\left(z_t\right) = p\left(z_{2t}\right)  p\left(z_{1t}\right | z_{2t} )$$

#### Step 1: likelihood of a pure moving average process

$$z_{2t}$$
is a pure moving average process. Its exact likelihood is estimated as follows (see [1] for details).
We list below the different steps of the algorithm.

* Compute the conditional least squares residuals by the recursion:
$$ a_{0t} = \begin{cases} 0,& -q \lt t \leq 0 \\ z_t-\theta_1 a_{0t-1}- \cdots --\theta_q a_{0t-q},&  0 \lt t \leq n  \end{cases} $$

* Compute the Pi-Weights of the model. They defines the (n+q x q) matrix

$$ G = \begin{pmatrix} 1 & 0 & \cdots & 0 \\ \pi_1 & 1 & \cdots & 0 \\ \pi_2 & \pi_1 & \cdots & \vdots\\ \vdots & \vdots & \vdots & \vdots \\ \pi_{n+q-1} & \pi_{n+q-2} & \cdots & \pi_{n} \end{pmatrix} $$

* Compute by recursion
$$ G'a \quad and \quad G'G $$ 

* Compute by Cholesky decomposition
$$ G'G \hat z_* = G'a \quad and \quad |G'G| $$ 

* Obtain by recursion the exact likelihood residuals
$$ \Theta\left(B\right)\begin{pmatrix}\hat a_* \\ \hat a_t \end{pmatrix}  = \begin{pmatrix}z_* \\ z_t \end{pmatrix} $$

The processing defines the linear transformation 
$$ T z_t = \begin{pmatrix}\hat a_* \\ \hat a_t \end{pmatrix} $$ and the searched determinant.

#### Step 2: conditional distribution of the initial observations

$$ p\left(z_{1t}\right | z_{2t} ) $$
is easily obtained by considering the join distribution of 
$$ \left( z_{1t}, z_{2t} \right) \sim N \left( 0, \begin{pmatrix} \Sigma_{11} && \Sigma{12} \\ \Sigma_{21} && \Sigma{22} \end{pmatrix}\right) \ $$

It is distributed as 
$$ N\left( \Sigma_{12} \Sigma_{22}^{-1}v, \Sigma_{11}-\Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21} \right)$$

$$= N\left( \Sigma_{12} T'Tv, \Sigma_{11}-\Sigma_{12} T'T\Sigma_{21} \right)$$

$$  = N\left( U'Tv, \Sigma_{11}-U'U \right)$$

where
* v is obtained by applying the auto-regressive polynomial on the observations
* T is the linear transformation defined in step 1
* 
$$ U = T \Sigma_{21} $$ 

##### References 

[1] ___Otto M. C., Bell W.R., Burman J.P.___ (1987), "An Iterative GLS Approach to Maximum Likelihood Estimation of Regression Models with Arima Errors", Bureau of The Census, SRD Research Report CENSUS/SRD/RR_87/34.

## Handling of missing observations in (Reg)ARIMA models

### Skipping approach

### Additive outlier approach

### References

__GOMEZ V. , MARAVALL A.  AND  PEÑA D.__ (1999): "_Missing observations in ARIMA models: Skipping approach versus additive outlier approach_", Journal of econometrics 88, 341-363.


## Tests on residuals 

### Autocorrelation 

#### The Durbin-Watson statistic is defined by[^81]:

 $$
 d = \frac{\sum_{t = 2}^{N}\left( {\widehat{a}}_{t} - {\widehat{a}}_{t - 1} \right)^{2}}{\sum_{t = 1}^{N}{\widehat{a}}_{t}^{2}}
 $$ 

where:

${\widehat{a}}_{t}$ : residual from the model.

Since
$$\sum_{t = 2}^{N}\left( {\widehat{a}}_{t} - {\widehat{a}}_{t - 1} \right)^{2} \cong \ $$2$$\sum_{t = 1}^{N}{\widehat{a}}_{t}^{2} - 2\sum_{t = 2}^{N}{ {\widehat{a}}_{t}{\widehat{a}}_{t - 1}}$$,
then the approximation $d \cong 2(1 - r_{z,1})$, where
$$r_{z,1} = \frac{\sum_{t = 1}^{N}{ {\widehat{a}}_{t}{\widehat{a}}_{t - 1}}}{\sum_{t = 1}^{N}{\widehat{a}}_{t}^{2}}$$
is the autocorrelation coefficient of the residuals at lag 1, is true.

The Durbin-Watson statistics is between 0 and 4. When the model provides
an adequate description of the data, then $r_{z,1}$ should be close to 0
and therefore the Durbin-Watson statistics is close to 2. When the
Durbin--Watson statistic is substantially less than 2, there is evidence
of positive serial correlation, while when   it is substantially greater
than 2 it indicates that the successive error terms are, on average,
much different in value from one another, i.e., negatively correlated.

More formally, to test for   a positive autocorrelation   at
significance   level $\alpha$, the Durbin-Watson statistics is compared to
the lower ($d_{L,\alpha}\ )\ $and upper ($d_{U,\alpha})$ critical
values:

-   If   $d < d_{L,\alpha}$ there is statistical evidence that the error terms are positively autocorrelated.

-   If   $d > d_{U,\alpha}$ there is   no   statistical evidence that the error terms are positively autocorrelated.

-   If      $d_{L,\alpha}$   $< d < d_{U,\alpha}$ the test is inconclusive.

Positive serial correlation is serial correlation in which a positive
error for one observation increases the chances of a positive error for
another observation.

To test for   negative autocorrelation   at significance$\ \alpha$, the test
statistic $(4 - d)$   is compared to the lower ($d_{L,\alpha}\ )\ $and
upper ($d_{U,\alpha})$ critical values:

-   If $\left( 4 - d \right) < d_{L,\alpha}$ there is statistical evidence that the error terms are negatively autocorrelated.

-   If $\left( 4 - d \right) > d_{U,\alpha}$ there is   no   statistical evidence that the error terms are negatively autocorrelated.

-   If $d_{U,\alpha} < \left( 4 - d \right) < d_{U,\alpha}$ the test is inconclusive.
    
    
[^81]: CHATFIELD, C. (2004).

#### Ljung-Box test

(described twice: merge)

The Ljung-Box Q-statistics are given by:

  $$
  \text{LB}\left( k \right) = n \times (n + 2) \times \sum_{k = 1}^{K}\frac{\rho_{a,k}^{2}}{n - k}
  $$,   \[1\] <!---\[7.144\]      --> 

where:

$$\rho_{a,k}^{2}$$ is the autocorrelation coefficient at lag $k$ of the
residuals $${\widehat{a}}_{t}$$.

$n$ is the number of terms in differenced series;

$$K$$ is the maximum lag being considered, set in JDemetra+ to $24$
(monthly series) or $8$ (quarterly series).

If the residuals are random (which is the case for residuals from a well
specified model), they will be distributed as $\chi_{(K - m)}^{2}$,
where $m$ is the number of parameters in the model which has been fitted
to the data.

The Ljung-Box and Box-Pierce tests sometimes fail to reject a poorly
fitting model. Therefore, care should be taken not to accept a model on
a basis of their results. For the description of autocorrelation concept
see section [Autocorrelation function and partial autocorrelation function](../theory/ACF_and_PACF.html).




The Ljung-Box test checks the "overall" randomnes of a time series using a given number of [autocorrelations](../../descriptive.md).   
It tests wether any of a group of autocorrelations of a time series are significantly different from 0.

##### Algorithm

We consider the autocorrelations $\hat\gamma_l, \cdots, \hat\gamma_{l\cdot k}$. Typically, $l=1$ when testing the independence of the series or $l=freq$ when testing seasonality.

The value of the test is defined by

$$ lb=n \left(n+2\right)\sum_{i=1}^k\frac{\hat\gamma_{i \cdot l}^2}{n-i \cdot l}$$

It is asymptotically distributed as a $\chi \left(k\right)$


##### Impementation in GUI 

##### Impementation in R

##### Java Library


This test is implemented in the class `demetra.stats.tests.LjungBoxTest`

```java
    int N=100;
    DataBlock sample=DataBlock.make(N);
    Random rnd=new Random();
    LjungBoxTest lb=new LjungBoxTest(sample);
    StatisticalTest test = lb
             .lag(3)
             .autoCorrelationsCount(10)
             .build();
```
#### Box-Pierce Test

(described twice: merge)

The Box-Pierce Q-statistics are given by:

  $$\text{BP}\left( k \right) = n\sum_{k = 1}^{K}\rho_{a,k}^{2}
  $$,   \[1\] <!---\[7.145\]      -->
 

where:

$\rho_{a,k}^{2}$ is the autocorrelation coefficient at lag $k$ of the
residuals ${\widehat{a}}_{t}$.

$n$ is the number of terms in differenced series;

$K$ is the maximum lag being considered, set in JDemetra+ to $24$
(monthly series) or $8$ (quarterly series).

If the residuals are random (which is the case for residuals from a well
specified model), they will be distributed as $\chi_{(K - m)}^{2}$
degrees of freedom, where $m$ is the number of parameters in the model
which has been fitted to the data.

The Ljung-Box and Box-Pierce tests sometimes fail to reject a poorly
fitting model. Therefore, care should be taken not to accept a model on
a basis of their results. For the description of autocorrelation concept
see section [Autocorrelation function and partial autocorrelation function](../theory/ACF_and_PACF.html).

Explain difference with Ljung-Box test

The Box-Pierce test checks the "overall" randomnes of a time series using a given number of [autocorrelations](../../descriptive.md).   
It tests wether any of a group of autocorrelations of a time series are significantly different from 0.

##### Statistic

We consider the autocorrelations $\hat\gamma_l, \cdots, \hat\gamma_{l\cdot k}$. Typically, $l=1$ when testing the independence of the series or $l=freq$ when testing seasonality.

The value of the test is defined by

$$ bp=n \sum_{i=1}^k\hat\gamma_{i \cdot l}^2$$

It is asymptotically distributed as a $\chi \left(k\right)$

##### Impementation in GUI 

##### Impementation in R

##### Java Library


This test is implemented in the class `demetra.stats.tests.BoxPierceTest`

```java
    int N=100;
    DataBlock sample=DataBlock.make(N);
    Random rnd=new Random();
    sample.set(rnd::nextDouble);
    BoxPierceTest bp=new BoxPierceTest(sample);
    StatisticalTest test = bp
              .lag(3)
              .autoCorrelationsCount(10)
              .build();
```


### Normality 
#### Doornik-Hansen test

The Doornik-Hansen test for multivariate normality (DOORNIK, J.A., and
HANSEN, H. (2008)) is based on the skewness and kurtosis of multivariate
data that is transformed to ensure independence. It is more powerful
than the Shapiro-Wilk test for most tested multivariate
distributions[^80].

The skewness and kurtosis are defined, respectively, as:
$$s = \frac{m_{3}}{\sqrt{m_{2}}^{3}}$$ and
$k = \frac{m_{4}}{m_{2}^{2}},\ $where:
$m_{i} = \frac{1}{n}\sum_{i = 1}^{n}{(x_{i}}{- \overline{x})}^{i}$
$\overline{x} = \frac{1}{n}\sum_{i = 1}^{n}x_{i}$ and $n$ is a number of
(non-missing) residuals.

The Doornik-Hansen test statistic derives from SHENTON, L.R., and
BOWMAN, K.O. (1977) and uses transformed versions of skewness and
kurtosis.

The transformation for the skewness $s$ into$\text{z}_{1}$ is as in
D\'AGOSTINO, R.B. (1970):

  $$
  \beta = \frac{3(n^{2} + 27n - 70)(n + 1)(n + 3)}{(n - 2)(n + 5)(n + 7)(n + 9)}
  $$ 
  
  $$
  \omega^{2} = - 1 + \sqrt{2(\beta - 1)}
  $$ 
  
  $$
  \delta = \frac{1}{\sqrt{\log{(\omega}^{2})}}
  $$                    
  
  $$
  y = s\sqrt{\frac{(\omega^{2} - 1)(n + 1)(n + 3)}{12(n - 2)}}
  $$                   
  
  $$
  z_{1} = \delta log(y + \sqrt{y^{2} - 1})
  $$                                    

The kurtosis $k$ is transformed from a gamma distribution to $\chi^{2}$,
which is then transformed into standard normal $z_{2}$ using the
Wilson-Hilferty cubed root transformation:

  $$
  \delta = (n - 3)(n + 1)(n^{2} + 15n - 4)
  $$                                               
  
  $$
  a = \frac{(n - 2)(n + 5)(n + 7)(n^{2} + 27n - 70)}{6\delta}
  $$                                
  
  $$
  c = \frac{(n - 7)(n + 5)(n + 7)(n^{2} + 2n - 5)}{6\delta}
  $$                                   
  
  $$
  l= \frac{(n + 5)(n + 7)({n^{3} + 37n}^{2} + 11n - 313)}{12\delta}
  $$                     
  
  $$
  \alpha = a + c \times s^{2}
  $$                                           
  
  $$
  \chi = 2l(k - 1 - s^{2})
  $$                                                                    
  
  $$
  z_{2} = \sqrt{9\alpha}\left( \frac{1}{9\alpha} - 1 + \sqrt[3]{\frac{\chi}{2\alpha}} \right)
  $$  

Finally, the Doornik-Hansen test statistic is defined as the sum of
squared transformations of the skewness and kurtosis. Approximately, the
test statistic follows a $\chi^{2}$distribution, i.e.:


  $$
  DH = z_{1}^{2} + z_{2}^{2}\sim\chi^{2}(2)
  $$   
  
  [^80]: The description of the test derives from DOORNIK, J.A., and
    HANSEN, H. (2008).




#### D'agostino ?

#### Jarque-Bera

## Forecasting

### Introduction 

The prediction errors are defined with a reference $i$ to the information set available at the time the forecast was made: 

$$ e_{t|i}=y_{t}-\hat{y}_{t|\mathcal{F}_{i}}$$ 
 
where $$ \mathcal{F}_{i} $$ need not only include lags of $$ y_{t} $$. 
In practice, the information that will be actually used may be a small subset of $$ \mathcal{F}_{i} $$.

 
The properties of these forecast errors can be assessed in isolation or relative to a benchmark, 
which we will define as $$ \breve{e}_{t|i} $$.  The benchmark may be a naive forecast, e.g. random walk, 
in which case  $$ \breve{y}_{t|\mathcal{F}_{i}} $$ would be equal to  $$ \breve{y}_{t|y_{t-1}}=y_{t-1} $$. 
However, the benchmark could also be a prediction regularly published by a forecasting institute or market analysts, 
i.e. Bloomberg, which is not necessarily model-based. In that case, $$ \breve{y}_{t|\mathcal{F}_{i}} $$ 
would be given by methods and a subset of $$ \mathcal{F}_{i} $$ which is unknown to us.

For model-based forecasts, we use the following notation:

$$ \hat{y}_{t|\mathcal{F}_{i}}=E_{\theta}[y_{t}|\mathcal{F}_{i}] $$ 
to highlight the fact that they are based on model-consistent 
expectations given by the parameter vector $$ \theta $$ .  
 

In forecasting comparisons involving competing forecasts resulting from the same information set, the subindex $i$ will be removed because it does not play a role. 
One could test the following hypothesis involving forecast errors: 


|   Test	|Null   Hypothesis	|  JDemetra*+* class `AccuracyTests` is extended by  |
|---	    |---	    |---      |
|   Unbiasedness	| $$ E[e_{t}]=0  $$	|      `BiasTest`                        |
|   Autocorrelation	| $$ E[e_{t}e_{t-1}]=0 $$    	|         `EfficiencyTest`         |
|   Equality  in  squared  errors	|  $$ E[e^2_{t}-\breve{e}^2_{t}]=0  $$  	|       `DieboldMarianoTest`     |
|   Forecast  $$ \hat{y}_{t} $$  encompases   $$ \breve{y}_{t} $$ |   $$ E[(e_{t}-\breve{e}_{t})e_{t}]=0  $$  	|   `EncompassingTest`         |
|   Forecast   $$\breve{y}_{t}$$   encompases   $$ \hat{y}_{t} $$ |   $$ E[(\breve{e}_{t}-e_{t})\breve{e}_{t}]=0 $$ |     `EncompassingTest`   |

The subsequent pages describe the implementation details of the various tests within *J*Demetra*+*
and examples of how to construct them.



### Encompassing Test 

Independently of whether the null hypothesis $$ E[e^2_{t}-\breve{e}^2_{t}]=0 $$  of the 
Diebold-Mariano test is rejected or not, it is relevant 
to understand the extent to which our model forecasts encompass those of the benchmark, 
or those are encompassed by the benchmark.  

Because of the obvious symmetry of both statements, we consider the first one alone in this document. 
If our forecasts $$ y_{t|\mathcal{F}_{i}} $$ 
encompasses a given benchmark 
$$ \breve{y}_{t|\mathcal{F}_{i}} $$ ,  
the difference between those benchmark forecasts and ours will not be a relevant factor 
at explaining our own forecast error. In other words, the regression coefficient $$ \lambda $$ 
will not be significantly different from zero in the following regression:

$$
\begin{equation}
\begin{array}{ccc}
  \underbrace{y_{t}-y_{t|\mathcal{F}_{i}}}_{e_{t}}  &   =   &   \lambda \underbrace{(\breve{y}_{t|\mathcal{F}_{i}}-y_{t|\mathcal{F}_{i}})}_{e_{t}-\breve{e}_{t}}+ \xi_{t}     \\
&   \Updownarrow    & \\
   y_{t}=  &   \lambda \breve{y}_{t|\mathcal{F}_{i}}  &    + (1-\lambda) y_{t|\mathcal{F}_{i}}+ \xi_{t}   \label{combi}
\end{array}
\end{equation}
$$


Following Harvey, Leybourne and Newbold (1998), the statistical significance 
of the $$ \lambda $$ coefficient in expression \ref{combi} can be used to reject 
the null hypothesis that our model encompasses the benchmark. In this case of rejection, 
the second expression in (\ref{combi}) suggests that a combination of the two forecast would yield a 
more informative forecast. 

By construction, the value of the coefficient of a regression 
$$ \breve{e_{t}} =\alpha\breve{e_{t}}-e_{t}+\xi_{t} $$ is equal to $$ 1-\lambda $$, 
but it is not necessarily true that the *rejection* of the 
null hypothesis in the first case implies the *acceptance*  of the symmetric statement.

The test-statistic is computed as follows. When the null hypothesis is that our model encompasses the benchmark, we define the sequence 
$$ \{d_{t}\}^{T}_{t=1} $$ , where $$ d_{t}=e_{t}(e_{t}-\breve{e_{t}}) $$ , and we 
compute $$ E1=\dfrac{\bar{d}}{\sqrt{\dfrac{2\pi\hat{f}_{d}(0)}{T}}} $$ , which is equivalent to the test statistic of the  in
[Diebold-Mariano Test](dmtest.md).  

#### Small samples <a name="fixedsmoothing"></a>
The small sample sizes that are typical 
in real-time forecasting applications lead to an over-rejection of the null hypothesis under standard asymptotics, 
so we follow the *fixed-smoothing asymptotics* proposed by Coroneo and Iacone (2015). The idea is to use the finite sample 
distributions of Kiefer and Vogelsang (2005). As a result, the distribution of the test statistic 
will depend on kernel and the bandwidth chosen, which is set by default equal to $T^{0.5}$. 
The results can be very different than those resulting from the traditional asymptotic theory, 
where the test statistic would have the same distribution under the null independently of the kernel and the bandwidth used. 

---


#### *J*Demetra*+* implementation

##### Class structure
*J*Demetra*+* exploits the same unified framework 
to conduct all forecasting accuracy tests.  

- The class `AccuracyTests` contains all methods required to perform the tests. All calculations are equivalent independently of the kind of test
because the calculation of the  loss function $ d_{t} $ is defined using abstraction, which is one of the features of the Java programming language. As a result, 
this class is extended by several classes that incorportate a  precise implementation of the method to calculate the loss
function: `BiasTest`, `EfficiencyTest`, `DieboldMarianoTest` and `EncompassingTest`. 
The constructor of each one of these classes can generate the tests 
when either the forecasts or the forecast errors are given as an input. Another input required is a boolean (`AsymptoticsType`) specifying whether 
standard asymptotics or 
[fixed-smoothing](#fixedsmoothing) asymptotics.

- All the tests contained in the class `AccuracyTests` will be constructed using the class `GlobalForecastingEvaluation`, which contains the various
tests as objects. This is
illustrated in the following example.

- The class `ForecastEvaluation` contains methods to quantify errors: 
Root Mean Squared Errors (RMSE), relative RMSE, Mean Absolute Errors (MAE), etc...  Those statistics could be
reported along with the test results.


#### A simple example
Suppose we want evaluate the forecasts of a model and compare them with 
those of a benchmark. The following points explain all the steps 
followed in the code below to run all the tests:

- First we need to initialize an array of time series  `TsData[]` that includes 
the two competing forecast (i.e. benchmark vs model) and the target. Next, we initialize the p-value corresponding 
to the test.
- Second, we initialize the `eval` object of the class `GlobalForecastingEvaluation`, 
which will contain all test results. The inputs needed to run the tests are three time series: our model's forecasts, 
those of the benchmark, and the actual data, which is the target. We also need to specify the kind of 
distribution of the various test statistics under the null, which is given by a normal distribution when 
`AccuracyTests.AsymptoticsType.STANDARD_FIXED_B` is used. By choosing the option 
`AccuracyTests.AsymptoticsType.HAR_FIXED_B`, the distribution tabulated by Kiefer and Vogelsang (2005) is used. 
- Since `eval` belongs to the class `GlobalForecastingEvaluation`, which contains all tests, the instruction `eval.getModelEncompassesBenchmarkTest()`
will trigger the necessary calculations.  In this example, we will be asking for the pvalues and weights of the test.
- Notice that two different hypothesis are tested at the same time: 
	..*Model forecasts encompasses Benchmark forecasts (`getModelEncompassesBenchmarkTest()`)
	..*Benchmark forecasts encompasses Model forecasts (`getBenchmarkEncompassesModelTest()`)
- For each type of test, the bandwidth used to estimate the variance needs to be specified. 
Otherwise, the default value will be used ($$ T^{1/2} $$). The relevant statistics for each test as well as the 
pvalues are obtained with a simple get command. Notice that `getPValue(twoSided)`  uses the logical argument 
`true` in order to get the p-values of the two-sided test.

- From this test, we can get the pvalues and also the weight defined in 
equation \ref{combi}. For example, if the pvalue obtained in 
`getModelEncompassesBenchmarkTest().getPValue(twoSided)` is very small and we reject 
that the Model encompasses the Benchmark, it is likely that the weight associated 
to the Benchmark (`getModelEncompassesBenchmarkTest().calcWeights()` ) will be very different from zero.

{% highlight java %}
	public void example() {
		TsData[] series = {benchmark, model, target};
			
		boolean twoSided = true;
		  
		double m_enc_bench = new double ;
		double m_enc_bench_Pval = new double ;
		   
		double bench_enc_m = new double ;
		double bench_enc_m_Pval = new double ;
			
		// squared root of T
		int bandwith = (int) Math.pow(series.getObsCount(), 1.0 / 2.0);
			
		GlobalForecastingEvaluation eval = new GlobalForecastingEvaluation(model, benchmark, target,
		AccuracyTests.AsymptoticsType.HAR_FIXED_B);
		   
		eval.getModelEncompassesBenchmarkTest().setBandwith(bandwith);
		   
		m_enc_bench = eval.getModelEncompassesBenchmarkTest().calcWeights();
		m_enc_bench_Pval = eval.getModelEncompassesBenchmarkTest().getPValue(twoSided);
		bench_enc_m = eval.getBenchmarkEncompassesModelTest().calcWeights();
		bench_enc_m_Pval = eval.getBenchmarkEncompassesModelTest().getPValue(twoSided);
	}
{% endhighlight %}


### Bias Evaluation 

In order to assess whether our forecasts are unbiased, 
we will simply test the statistical significance of the average error. 
In some cases, the time series of forecast errors 
$$ \{e_{t}\}^{T}_{t=1} $$ may be autocorrelated 
to some extent even when they are based on a model with IID innovations. 
In such cases, the variance associated to the estimate of the average forecast error may be large. 
The test statistic has exactly the same form as the previous tests discussed so far, 
and it follows a standard normal distribution under the null of unbiased forecasts:  
$$ BIAS=\dfrac{\bar{e}}{\sqrt{\dfrac{2\pi\hat{f}_{e}(0)}{T}}} $$


#### Efficiency 

We will test here a necessary condition for our forecasts to be efficient: absence of autocorrelation. 
In the same spirit as the tests described above, we will assess the statistical significance of the forecast errors' autocorrelation. 
Thus,  our sequence $$ \{d_{t}\}^{T}_{t=1} $$ will be defined with $$ d_{t}=e_{t}e_{t-1} $$.

#### Small samples <a name="fixedsmoothing"></a>
The small sample sizes that are typical 
in real-time forecasting applications lead to an over-rejection of the null hypothesis under standard asymptotics, 
so we follow the *fixed-smoothing asymptotics* proposed by Coroneo and Iacone (2015). 
The idea is to use the finite sample 
distributions of Kiefer and Vogelsang (2005). As a result, the distribution of the test statistic 
will depend on kernel and the bandwidth chosen, which is set by default equal to $T^{0.5}$. 
The results can be very different than those resulting from the traditional asymptotic theory, 
where the test statistic would have the same distribution under the null independently of the kernel and the bandwidth used. 

---


#### *J*Demetra*+* implementation

##### Class structure

*J*Demetra*+* exploits the same unified framework 
to conduct all forecasting accuracy tests.  

- The class `AccuracyTests` contains all methods required to perform the tests. All calculations are equivalent independently of the kind of test
because the calculation of the  loss function $ d_{t} $ is defined using abstraction, which is one of the features of the Java programming language. As a result, 
this class is extended by several classes that incorportate a  precise implementation of the method to calculate the loss
function: `BiasTest`, `EfficiencyTest`, `DieboldMarianoTest` and `EncompassingTest`. 
The constructor of each one of these classes can generate the tests 
when either the forecasts or the forecast errors are given as an input. Another input required is a boolean (`AsymptoticsType`) specifying whether 
standard asymptotics or 
[fixed-smoothing](#fixedsmoothing) asymptotics.

- All the tests contained in the class `AccuracyTests` will be constructed using the class `GlobalForecastingEvaluation`, which contains the various
tests as objects. This is
illustrated in the following example.

- The class `ForecastEvaluation` contains methods to quantify errors: 
Root Mean Squared Errors (RMSE), relative RMSE, Mean Absolute Errors (MAE), etc...  Those statistics could be
reported along with the test results.



#### A simple example
Suppose we want evaluate the forecasts of a model, but instead of comparing them with 
those of a benchmark, as in the [Diebold-Mariano](dmtest.md) or[Encompassing](encompassing.md) tests, we
want to assess: a) is the average forecast error  different from zero? b) are forecast errors autocorrelated? 
In the first case, estimating the average forecast error would help to adjust the forecast by removing the systematic bias. 
In the second case, a forecast error would be able to predict part of the subsequent error. The same questions may be 
asked about the benchmark forecasts themselves, although the tests do not involve any forecasting comparison.


- First we need to initialize an array of time series  `TsData[]` that includes 
the two competing forecast (i.e. benchmark vs model) and the target. Next, we initialize the p-value corresponding 
to the test, and both bias and autocorrelation coefficients. We consider both our own forecasts and the benchmark forecasts.
- Second, we initialize the `eval` object of the class `GlobalForecastingEvaluation`, 
which will contain all test results. The inputs needed to run the tests are three time series: our model's forecasts, 
those of the benchmark, and the actual data, which is the target. We also need to specify the kind of 
distribution of the various test statistics under the null, which is given by a normal distribution when 
`AccuracyTests.AsymptoticsType.STANDARD_FIXED_B` is used. By choosing the option 
`AccuracyTests.AsymptoticsType.HAR_FIXED_B`, the distribution tabulated by Kiefer and Vogelsang (2005) is used. 
- Since `eval` belongs to the class `GlobalForecastingEvaluation`, which contains all tests, the instructions `eval.getBiasTest()`
and `eval.getEfficiencyTest()` will trigger the necessary calculations.  In this example, we will be asking for the pvalues of the test as well as
the bias and autocorrelation coefficient of the errors.

- For each type of test, the bandwidth used to estimate the variance needs to be specified. 
Otherwise, the default value will be used ($$ T^{1/2} $$). The relevant statistics for each test as well as the 
pvalues are obtained with a simple get command. Notice that `getPValue(twoSided)`  uses the logical argument 
`true` in order to get the p-values of the two-sided test.

- Notice the results will be reported for the forecasts of the model and for those of the benchmark. For convenience, 
`GlobalForecastingEvaluation` contains the tests for both cases, so we can get them using the appropiate get instruction: `eval.getBiasBenchmarkTest()`
and `eval.getBiasTest()`.




``` java
    public void example() {
    
    TsData[] series = {benchmark, model, target};
    
    boolean twoSided = true;
    
	// our forecasts
    double bias = new double ;
    double biasPval = new double ;
    
    double arcorr = new double ;
    double arPval = new double ;

	// benchmark
	double bias_B = new double ;
    double biasPval_B = new double ;
    
    double arcorr_B = new double ;
    double arPval_B = new double ;
	
    
    // squared root of T
    int bandwith = (int) Math.pow(series.getObsCount(), 1.0 / 2.0);
    
   	// our forecasts
    eval.getBiasTest().setBandwith(bandwith);
    bias = eval.getBiasTest().getAverageLoss();
    biasPval = eval.getBiasTest().getPValue(twoSided);
    
    eval.getEfficiencyTest().setBandwith(bandwith);
    arcorr = eval.getEfficiencyTest().calcCorelation();
    arPval = eval.getEfficiencyTest().getPValue(twoSided);
   
    // benchmark
    eval.getBiasBenchmarkTest().setBandwith(bandwith);
    bias_B = eval.getBiasBenchmarkTest().getAverageLoss();
    biasPval_B = eval.getBiasBenchmarkTest().getPValue(twoSided);
    
    eval.getEfficiencyTest().setBandwith(bandwith);
    arcorr_B = eval.getEfficiencyBenchmarkTest().calcCorelation();
    arPval_B = eval.getEfficiencyBenchmarkTest().getPValue(twoSided);
  }

``` 


### Diebold-Mariano Test 

The test originally proposed by Diebold and Mariano (1995) considers 
a sample path of loss differentials $$ \{d_{t}\}^{T}_{t=1} $$. 
In the case of a squared loss function, we have $$ d_{t}=e^2_{t}-\breve{e}^2_{t} $$. 
Under the assumption that the loss differential is a covariance stationary series, 
the sample average, $$ \bar{d} $$, converges asymptotically to a normal distribution: 

$$ \sqrt{T} \bar{d}\:\:\: \underrightarrow{d}\:\:\: N(\mu, 2\pi f_{d}(0)) $$

In particular, they proposed to test the null hypothesis that 
the forecast errors coming from the two forecasts bring about the same loss:  
$$ E[e^2_{t}-\breve{e}^2_{t}]=0 $$ against the two-sided alternative. 
Thus, the resulting p-values represent the probability of 
obtaining the realized forecast error differential or 
a more extreme one in a new experiment if the null 
hypothesis was actually true. 


|   symbol	|definition   	|
|---	|---	|
|   $ e_{t}=y_{t}-\hat{y}_{t}  $		| forecast error|
|   $$ \breve{e}_{t}=y_{t}-\breve{y}_{t}  $$		| benchmark error|
|   $$ d_{t}=e^2_{t}-\breve{e}^2_{t} $$ 	| loss differential    	|


#### DM test-statistic

The test-statistic that will be used to calculate our 
p-values is computed as follows:

\begin{equation}
DM=\dfrac{\bar{d}}{\sqrt{\dfrac{2\pi\hat{f}_{d}(0)}{T}}}
\label{DMTEST}
\end{equation}

where $$ 2\pi\hat{f}_{d}(0) $$  is a consistent estimate. 
Consider $$ 2\pi\hat{f}_{d}(0)=\sum^{(T-1)}_{\tau=-(T-1)}w_{\tau}\gamma_{d}(\tau) $$ , 
where $$ \gamma_{d}(\tau)=\dfrac{1}{T}\sum^{T}_{t=|\tau|+1}(d_{t}-\bar{d})(d_{t-|\tau|}-\bar{d}) $$ . 
Under the assumption that  $$ \gamma_{d}(\tau)=0 $$  for $$ \tau\geq h $$ , 
we can use a rectangular lag window estimator by setting $$ w_{\tau}=0 $$  for  $$ \tau\geq h $$ . 
Another option is to use the Heteroschedasticity and Autocorrelation Consistent (HAC) 
estimator proposed by Newey and West (1987). In this case, the weights could be given 
by a triangular window, $$ w_{\tau}=1-\dfrac{\tau}{h} $$  for $$ \tau<h $$ . In this case, however, 
the consistency property only remains valid when the truncation lag $h$ or bandwidth is 
a function of the sample size $$ T $$. 

The idea is to test the statistical significance of the regression of 
$$ e^2_{t}-\breve{e}^2_{t} $$  on an intercept.  In order to determine the statistical 
significance of the intercept, its associated standard errors need to take into account 
the autocorrelation patterns of the regression error, which are considered in the denominator 
of equation (\ref{DMTEST}). 

## Small samples <a name="fixedsmoothing"></a>
The small sample sizes that are typical 
in real-time forecasting applications lead to an over-rejection of the null hypothesis under standard asymptotics, 
so we follow the *fixed-smoothing asymptotics* proposed by Coroneo and Iacone (2015). The idea is to use the finite sample 
distributions of Kiefer and Vogelsang (2005). As a result, the distribution of the test statistic (\ref{DMTEST}) 
will depend on kernel and the bandwidth chosen, which is set by default equal to $T^{0.5}$. 
The results can be very different than those resulting from the traditional asymptotic theory, 
where the test statistic would have the same distribution under the null independently of the kernel and the bandwidth used. 

---


#### *J*Demetra*+* implementation

##### Class structure
*J*Demetra*+* exploits the same unified framework 
to conduct all forecasting accuracy tests.  

- The class `AccuracyTests` contains all methods required to perform the tests. All calculations are equivalent independently of the kind of test
because the calculation of the  loss function $ d_{t} $ is defined using abstraction, which is one of the features of the Java programming language. As a result, 
this class is extended by several classes that incorportate a  precise implementation of the method to calculate the loss
function: `BiasTest`, `EfficiencyTest`, `DieboldMarianoTest` and `EncompassingTest`. 
The constructor of each one of these classes can generate the tests 
when either the forecasts or the forecast errors are given as an input. Another input required is a boolean (`AsymptoticsType`) specifying whether 
standard asymptotics or 
[fixed-smoothing](#fixedsmoothing) asymptotics.

- All the tests contained in the class `AccuracyTests` will be constructed using the class `GlobalForecastingEvaluation`, which contains the various
tests as objects. This is
illustrated in the following example.

- The class `ForecastEvaluation` contains methods to quantify errors: 
Root Mean Squared Errors (RMSE), relative RMSE, Mean Absolute Errors (MAE), etc...  Those statistics could be
reported along with the test results.




#### A simple example
Suppose we want evaluate the forecasts of a model and compare them with 
those of a benchmark. The following points explain all the steps 
followed in the code below to run all the tests:

- First we need to initialize an array of time series  `TsData[]` that includes 
the two competing forecast (i.e. benchmark vs model) and the target. Next, we initialize the p-value corresponding 
to the test, and the RMSE, which will be calculated at the end. 	
- Second, we initialize the `eval` object of the class `GlobalForecastingEvaluation`, 
which will contain all test results. The inputs needed to run the tests are three time series: our model's forecasts, 
those of the benchmark, and the actual data, which is refered to as the target. We also need to specify the kind of 
distribution of the various test statistics under the null, which is given by a normal distribution when 
`AccuracyTests.AsymptoticsType.STANDARD_FIXED_B` is used. By choosing the option 
`AccuracyTests.AsymptoticsType.HAR_FIXED_B`, the distribution tabulated by Kiefer and Vogelsang (2005) is used. 
- Since `eval` belongs to the class `GlobalForecastingEvaluation`, which contains all tests, the instruction `eval.getDieboldMarianoTest()`
will trigger the necessary calculations.  In this example, we will be asking for the pvalue of the Diebold-Mariano test.
- For each type of test, the bandwidth used to estimate the variance needs to be specified. 
Otherwise, the default value will be used ($$ T^{1/2} $$). The relevant statistics for each test as well as the 
pvalues are obtained with a simple get command. Notice that `getPValue(twoSided)`  uses the logical argument 
`true` in order to get the p-values of the two-sided test.

- Finally, the `feval` object of the class `ForecastEvaluation` is initialized in order to calculate the RMSE.

``` java
	public void example() {  
		TsData[] series = {benchmark, model, target};

		double rmse = new double ;
		double dmPval = new double ;
		boolean twoSided = true;
			
		int bandwith = (int) Math.pow(series.getObsCount(), 1.0 / 2.0);
			
		GlobalForecastingEvaluation eval = new GlobalForecastingEvaluation(model, benchmark, target,
		AccuracyTests.AsymptoticsType.HAR_FIXED_B);
		eval.getDieboldMarianoTest().setBandwith(bandwith);    
		dmPval = eval.getDieboldMarianoTest().getPValue(twoSided); 
			
		ForecastEvaluation feval = new ForecastEvaluation(model, benchmark, target);
		rmse = feval.calcRMSE();   
	}
```
