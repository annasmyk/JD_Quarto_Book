
# Seasonal Adjustment

## Motivation

The primary aim of the seasonal adjustment process is to remove seasonal
fluctuations from the time series. To achieve this goal, seasonal
adjustment methods decompose the original time series into components
that capture specific movements. These components are: trend-cycle,
seasonality and irregularity. The trend-cycle component includes
long-term and medium-term movements in the data. For seasonal adjustment
purposes there is no need to divide this component into two parts.
JDemetra+ refers to the trend-cycle as trend and consequently this
convention is used here.

This section presents the options of the seasonal adjustment processes
performed by the methods implemented in JDemetra+
(X-12-ARIMA/X-13ARIMA-SEATS and TRAMO/SEATS) and discusses the output
displayed by JDemetra+. As these seasonal adjustment methods use
different approach to the decomposition, the output produced for both of
them has different structure and content. Therefore, the results for
both methods are discussed separately. However, in contrast to the
original programs, in JDemetra+ some quality indicators have been
implemented for both methods, allowing for an easier compaison of the
results.

## Unobserved Components (UC)

The main components, each representing the impact of certain types of
phenomena on the time series ($X_{t}$), are:

-   The trend ($T_{t}$) that captures long-term and medium-term
    behaviour;

-   The seasonal component ($S_{t}$) representing intra-year
    fluctuations, monthly or quarterly, that are repeated more or less
    regularly year after year;

-   The irregular component ($I_{t}$) combining all the other more or
    less erratic fluctuations not covered by the previous components.

In general, the trend consists of 2 sub-components:

-   The long-term evolution of the series;

-   The cycle, that represents the smooth, almost periodic movement
    around the long-term evolution of the series. It reveals a
    succession of phases of growth and recession.

For seasonal adjustment purposes both TRAMO-SEATS and X-13ARIMA-SEATS do
not separate the long-term trend from the cycle as these two components
are usually too short to perform their reliable estimation.
Consequently, hereafter TRAMO-SEATS and X-13ARIMA-SEATS estimate the
trend component. However, the original TRAMO-SEATS may separate the
long-term trend from the cycle through the Hodrick-Precsott filter using
the output of the standard decomposition. It should be remembered that
JDemetra+ refers to the trend-cycle as trend ($T_{t}$), and consequently
this convention is used in this document.

TRAMO-SEATS considers two decomposition models:

-   The additive model: $X_{t} = T_{t} + S_{t} + I_{t}$;

-   The log additive model:
    $log(X_{t}) = log(T_{t}) + log(S_{t}) + log(I_{t})$.

Apart from these two decomposition types X-13ARIMA-SEATS allows the user
to apply also the multiplicative model:
$X_{t} = T_{t} \times S_{t} \times I_{t}$.

A time series $x_{t}$, which is a subject to a decomposition, is assumed
to be a realisation of a discrete-time stochastic, covariance-stationary
linear process, which is a collection of random variables $x_{t}$, where
$t$ denotes time. It can be shown that any stochastic,
covariance-stationary process can be presented in the form:

$x_{t} = \mu_{t} + {\widetilde{x}}_{t}$, $$1$$

where $\mu_{t}$ is a linearly deterministic component and
${\widetilde{x}}_{t}$ is a linearly interderministic component, such as:

$${\widetilde{x}}_{t} = {\sum_{j = 0}^{\infty}\psi_{j}a}_{t - j}
  $$, $$2$$

where $\sum_{j = 0}^{\infty}\psi_{i}^{2} < \infty$ (coefficients
$\psi_{j}$ are absolutely summable), $\psi_{0} = 1$ and $a_{t}$ is the
white noise error with zero mean and constant variance $V_{a}$. The
error term $a_{t}$ represents the one-period ahead forecast error of
$x_{t}$, that is:

$$
  a_{t} = {\widetilde{x}}_{t} - {\widehat{x}}_{t|t - 1}
  $$, $$3$$

where $${\widehat{x}}_{t|t - 1}$$ is the forecast of
$${\widetilde{x}}_{t}$$ made at period $t - 1$. As $a_{t}$ represents
what is new in $${\widetilde{x}}_{t}$$ in point $t$, i.e., not contained
in the past values of $${\widetilde{x}}_{t}$$, it is also called
innovation of the process. From $$3$$ $${\widetilde{x}}_{t}$$ can be
viewed as a linear filter applied to the innovations.

The equation 7.1 is called a Wold representation. It presents a process
as a sum of linearly deterministic component $\mu_{t}$ and linearly
interderministic component $\sum_{j = 0}^{\infty}\psi_{j}a_{t - j}$, the
first one is perfectly predictable once the history of the process
$x_{t - 1}$ is known and the second one is impossible to predict
perfectly. This explains why the stochastic process cannot be perfectly
predicted.

Under suitable conditions $${\widetilde{x}}_{t}$$ can be presented as a
weighted sum of its past values and $a_{t}$, i.e.:

$$
  { {\widetilde{x}}_{t} = \sum_{j = 0}^{\infty}\pi_{j}{\widetilde{x}}_{t - j} + a}_{t}
  $$, $$4$$

In general, for the observed time series, the assumptions concerning the
nature of the process $$1$$ do not hold for various reasons. Firstly,
most observed time series display a mean that cannot be assumed to be
constant due to the presence of a trend and the seasonal movements.
Secondly, the variance of the time series may vary in time. Finally, the
observed time series usually contain outliers, calendar effects and
regression effects, which are treated as deterministic. Therefore, in
practice a prior transformation and an adjustment need to be applied to
the time series. The constant variance is usually achieved through
taking a logarithmic transformation and the correction for the
deterministic effects, while stationarity of the mean is achieved by
applying regular and seasonal differencing. These processes, jointly
referred to as preadjustment or linearization, can be performed with the
TRAMO or RegARIMA models. Besides the linearisation, forecasts and
backcasts of stochastic time series are estimated with the ARIMA model,
allowing for later application of linear filters at both ends of time
series. The estimation performed with these models delivers the
stochastic part of the time series, called the linearised series, which
is assumed to be an output of a linear stochastic
process.[^seasonal-adjustment-1] The deterministic effects are removed
from the time series and used to form the final components.

[^seasonal-adjustment-1]: When the series are non-stationary
    differentiation is performed before the seasonality tests.

In the next step the linearised series is decomposed into its
components. There is a fundamental difference in how this process is
performed in TRAMO-SEATS and X-13ARIMA-SEATS. In TRAMO-SEATS the
decomposition is performed by the SEATS procedure, which follows a so
called ARIMA model based approach. In principle, it aims to derive the
components with statistical models. More information is given in the
[SEATS](../theory/SA_SEATS.html) section. X-13ARIMA-SEATS offers two
algorithms for decomposition: SEATS and X-11. The X-11 algorithm, which
is described in the [X-11 section](../theory/SA_X11.html) section,
decomposes a series by means of linear filters. Finally, in both methods
the final components are derived by the assignment of the deterministic
effects to the stochastic components. Consequently, the role of the
ARIMA models is different in each method. TRAMO-SEATS applies the ARIMA
models both in the preadjustment step and in the decomposition
procedure. On the contrary, when the X-11 algorithm is used for
decomposition, X-13ARIMA-SEATS uses the ARIMA model only in the
preadjustment step. In summary, the decomposition procedure that results
in an estimation of the seasonal component requires prior identification
of the deterministic effects and their removal from the time series.
This is achieved through the linearisation process performed by the
TRAMO and the RegARIMA models, shortly discussed in the [Linearisation
with the TRAMO and RegARIMA models](../theory/SA_lin.html) section.The
linearised series is then decomposed into the stochastic components with
[SEATS](../theory/SA_SEATS.html) or [X-11](../theory/SA_X11.html)
algorithms.

## Seasonality tests

### Overview

### F-test on seasonal dummies

The F-test on seasonal dummies checks for the presence of deterministic
seasonality. The model used here uses seasonal dummies (mean effect and
11 seasonal dummies for monthly data, mean effect and 3 for quarterly
data) to describe the (possibly transformed) time series behaviour. The
test statistic checks if the seasonal dummies are jointly statistically
not significant. When this hypothesis is rejected, it is assumed that
the deterministic seasonality is present and the test results are
displayed in green.

This test refers to Model-Based $\chi^{2}\$and F-tests for Fixed
Seasonal Effects proposed by LYTRAS, D.P., FELDPAUSCH, R.M., and BELL,
W.R. (2007) that is based on the estimates of the regression dummy
variables and the corresponding t-statistics of the RegARIMA model, in
which the ARIMA part of the model has a form (0,1,1)(0,0,0). The
consequences of a misspecification of a model are discussed in LYTRAS,
D.P., FELDPAUSCH, R.M., and BELL, W.R. (2007).

For a monthly time series the RegARIMA model structure is as follows:

$$\left( 1 - B \right)\left( y_{t} - \beta_{1}M_{1,t} - \ldots - \beta_{11}M_{11,t} - \gamma X_{t} \right) = \mu + (1 - B)a_{t}
$$

where:

$$
M_{j,t} =
\begin{cases}
1 & \text{ in month } j = 1, \ldots, 11 \\
- 1 & \text{ in December}\\
0 & \text{ otherwise}
\end{cases} \text{ - dummy variables;}
$$

$y_{t}$ -- the original time series;

$B$ -- a backshift operator;

$X_{t}$ -- other regression variables used in the model (e.g. outliers,
calendar effects, user-defined regression variables, intervention
variables);

$\mu$ -- a mean effect;

$a_{t}$ -- a white-noise variable with mean zero and a constant
variance.

In the case of a quarterly series the estimated model has a form:

$$\left( 1 - B \right)\left( y_{t} - \beta_{1}M_{1,t} - \ldots - \beta_{3}M_{3,t} - \gamma X_{t} \right) = \mu + (1 - B)a_{t}$$,
$$2$$ <!--- \[7.156\]     -->

where:

$$
M_{j,t} =
\begin{cases}
1 & \text{ in quarter} j = 1, \ldots, 3 \\
- 1 & \text{ in the fourth quarter}\\
0 & \text{ otherwise}
\end{cases} \text{ - dummy variables;}
$$

One can use the individual t-statistics to assess whether seasonality
for a given month is significant, or a chi-squared test statistic if the
null hypothesis is that the parameters are collectively all zero. The
chi-squared test statistic is
${\widehat{\chi}}^{2} = {\widehat{\beta}}^{'}{\lbrack Var(\widehat{\beta})}^{\ })^{- 1}\rbrack{\widehat{\beta}}^{\ }$
in this case compared to critical values from a
$\chi^{2}\left( \text{df} \right)$-distribution, with degrees of freedom
$df = 11\$(monthly series) or $df = 3$ (quarterly series). Since the
${Var(\widehat{\beta})}^{\ }$ computed using the estimated variance of
$\alpha_{t}$ may be very different from the actual variance in small
samples, this test is corrected using the proposed $\text{F}$ statistic:

$$
  F = \frac{ {\widehat{\chi}}^{2}}{s - 1} \times \frac{n - d - k}{n - d}
  $$*,* $$3$$ <!--- \[7.157\]     -->

where $n$ is the sample size, $d$ is the degree of differencing, s is
time series frequency (12 for a monthly series, 4 for a quarterly
series) and $k$ is the total number of regressors in the RegARIMA model
(including the seasonal dummies $\text{M}_{j,t}$ and the intercept).

This statistic follows a $$F_{s - 1,n - d - k}$$ distribution under the
null hypothesis.

### QS Test on autocorrelation at seasonal lags

The QS test is a variant of the [Ljung-Box](../theory/Tests_LB.html)
test computed on seasonal lags, where we only consider positive
auto-correlations

More exactly,

$$ QS=n \left(n+2\right)\sum_{i=1}^k\frac{\left[ \max  \left(0, \hat\gamma_{i \cdot l}\right)\right]^2}{n-i \cdot l}$$

where $$k=2$$, so only the first and second seasonal lags are
considered. Thus, the test would checks the correlation between the
actual observation and the observations lagged by one and two years.
Note that $$l=12$$ when dealing with monthly observations, so we
consider the autocovariances $$\hat\gamma_{12}$$ and $$\hat\gamma_{24}$$
alone. In turn, $$k=4$$ in the case of quarterly data.

Under H0, which states that the data are independently distributed, the
statistics follows a $$\chi \left(k\right)$$ distribution. However, the
elimination of negative correlations makes it a bad approximation. The
p-values would be given by $P(\chi^{2}\left( k \right) > Q)$ for
$k = 2$. As ${P(\chi}^{2}(2)) > 0.05 = 5.99146$ and
${P(\chi}^{2}(2)) > 0.01 = 9.21034$, $QS > 5.99146$ and $QS > 9.21034$
would suggest rejecting the null hypothesis at $95\%$ and $99\%$
significance levels, respecively.

##### Modification

Maravall (2012) proposes approximate the correct distribution (p-values)
of the QS statistic using simulation techniques. Using 1000K
replications of sample size 240, the correct critical values would be
3.83 and 7.09 with confidence levels of $95\%$ and $99\%$, respectively
(lower than the 5.99146 and 9.21034 shown above). For each of the
simulated series, he obtains the distribution by assuming $QS=0$ when
$$\hat\gamma_{12}$$, so in practice this test will detect seasonality
only when any of these conditions hold: - Statistically significant
positive autocorrelation at lag 12 - Nonnegative sample autocorrelation
at lag 12 and statistically significant positive autocorrelation at lag
24

#### Implementation

##### In the graphical user interface (GUI)

The test can be applied directly to any series by selecting the option
*Statistical Methods \>\> Seasonal Adjustment \>\> Tools \>\>
Seasonality Tests*. This is an example of how results are displayed for
the case of a monthly series:

![qs](All_images/qs.png)

The test can be applied to the input series before any seasonal
adjustment method has been applied. It can also be applied to the
seasonally adjusted series or to the irreguar component.

##### Via R package: RJD3toolkit (blank)

##### Java Library

This test is implemented in the class `ec.satoolkit.diagnostics.QsTest`

##### References

-   LJUNG G. M. and G. E. P. BOX (1978). "On a Measure of a Lack of Fit
    in Time Series Models". Biometrika 65 (2): 297--303.
    <doi:10.1093/biomet/65.2.297>
-   MARAVALL, A. (2011). "Seasonality Tests and Automatic Model
    Identification in Tramo-Seats". Manuscript
-   MARAVALL, A. (2012). "Update of Seasonality Tests and Automatic
    Model Identification in TRAMO-SEATS". Bank of Spain (November 2012)

### QS Test for seasonality (BIS : solve this)

<!-- The QS test is a variant of the [Ljung-Box](../wn/ljungbox.md) test computed on seasonal lags, where we only consider positive auto-correlations -->

More exactly,

$$ qs=n \left(n+2\right)\sum_{i=1}^k\frac{\left[ \max  \left(0, \hat\gamma_{i \cdot l}\right)\right]^2}{n-i \cdot l}$$

The current implementation still considers that the statistics is
distributed as a $$\chi \left(k\right)$$ even if it is obvioulsly
incorrect.

### Kurskall-Wallis

The Kruskal-Wallis test is a non-parametric test used for testing
whether samples originate from the same distribution. The parametric
equivalent of the Kruskal-Wallis test is the one-way analysis of
variance (ANOVA). When rejecting the null hypothesis of the
Kruskal-Wallis test, then at least one sample stochastically dominates
at least one other sample. The test does not identify where this
stochastic dominance occurs or for how many pairs of groups stochastic
dominance obtains. The null hypothesis states that all months (or
quarters, respectively) have the same mean. Under this hypothesis the
test statistic follows a $$ \chi^2 $$ distribution. When this hypothesis
is rejected, it is assumed that time series values differ significantly
between periods and the test results are displayed in green

The test is typically applied to $$ k  $$ groups of data
$$ \left\{x_{i}\right\}_{j} $$. Each group $$ j=1,…,k $$ is composed of
$$ n_j $$ observations, which are indexed by $$ i=1,…,n_j $$. Each month
(or quarter) groups all the observations available for a certain number
of years.

As opposed to the notation used in the [Friedman
test](../theory/Tests_Friedman.html), number of observations here is not
necessarily equal for each group. The ranking of each data point,
represented by variable $$ r_{ij} $$., is now defined different than in
Friedman test, since it considers all observables
$$ N=n_1+ \dots + n_g $$, thereby ignoring group membership.

The test statistic is given by

$$
Q=\frac{SS_t}{SS_e}
$$

where $$ SS_t=(N-1)\sum_{j=1}^{g}n_i(\bar{r}_{.j}-\bar{r})^2 $$ and
$$ SS_e=\sum_{j=1}^{g}\sum_{i=1}^{n_j}(r_{ij}-\bar{r})^2 $$ - $$ n_j $$
is the number of observations in group $$ j  $$ - $$ \bar{r}_{.j} $$ is
the average of the absolute ranks of the data in group $$ j  $$ - The
average rank is $$ \bar{r} =\frac{1}{2}(N+1) $$

Under the null hypothesis that all groups are generated from the same
distribution, the test statistic Q is approximated by a chi-squared
distribution. Thus, the p-value is given by $$ P( \chi^2_{g-1}>Q) $$.
This approximation can be misleading if some of the groups are very
small (i.e. less than five elements). If the statistic is not
significant, then there is no evidence of stochastic dominance between
the samples. However, if the test is significant then at least one
sample [stochastically
dominates](http://en.wikipedia.org/wiki/Stochastic_dominance) another
sample.

#### Use

The test can be applied directly to any series by selecting the option
*Statistical Methods \>\> Seasonal Adjustment \>\> Tools \>\>
Seasonality Tests*. This is an example of how results are displayed for
the case of a monthly series:

![kwResults](All_images/kw.png)

The test can be applied to the input series before any seasonal
adjustment method has been applied. It can also be applied to the
seasonally adjusted series or to the irreguar component.

#### Implementation

This test is implemented in the class
`ec.satoolkit.diagnostics.KruskallWallisTest`

#### References

-   Kruskal; Wallis (1952). "Use of ranks in one-criterion variance
    analysis". Journal of the American Statistical Association 47 (260):
    583--621. <doi:10.1080/01621459.1952.10483441>.

### Friedman test (stable seasonality test)

The Friedman test is a non-parametric method for testing that samples
are drawn from the same population or from populations with equal
medians. The significance of the month (or quarter) effect is tested.
The Friedman test requires no distributional assumptions. It uses the
rankings of the observations. If the null hypothesis of no stable
seasonality is rejected at the 0.10% significance level then the series
is considered to be seasonal and the test's outcome is displayed in
green.

The test statistic is constructed as follows. Consider first the matrix
of data $$ \left\{x_{ij}\right\}_{n \times k} $$ with $$ n $$ rows (the
blocks, i.e. number of years in the sample), $$ k $$ columns (the
treatments, i.e. either 12 months or 4 quarters, depending on the
frequency of the data).\
The data matrix needs to be replaced by a new matrix
$$ \left\{r_{ij}\right\}_{n \times k} $$, where the entry $$ r_{ij} $$
is the rank of $$ x_{ij} $$ within block $$ i $$ .

The test statistic is given by

$$
Q=\frac{SS_t}{SS_e}
$$

where $$ SS_t=n \sum_{j=1}^{k}(\bar{r}_{.j}-\bar{r})^2 $$ and
$$ SS_e=\frac{1}{n(k-1)} \sum_{i=1}^{n}\sum_{j=1}^{k}(r_{ij}-\bar{r})^2 $$
It represents the variance of the average ranking across treatments j
relative to the total.

Under the hypothesis of no seasonality, all months can be equally
treated. For the sake of completeness: - $$ \bar{r}_{.j} $$ is the
average ranks of each treatment (month) j within each block (year) - The
average rank is given by
$$ \bar{r}= \frac{1}{nk}\sum_{i=1}^{n}\sum_{j=1}^{k}(r_{ij})$$

For large $$ n $$ or $$ k $$ , i.e. n \> 15 or k \> 4, the probability
distribution of $$ Q $$ can be approximated by that of a chi-squared
distribution. Thus, the p-value is given by $$ P( \chi^2_{k-1}>Q) $$ .

#### Use

The test can be applied directly to any series by selecting the option
*Statistical Methods \>\> Seasonal Adjustment \>\> Tools \>\>
Seasonality Tests*. This is an example of how results are displayed for
the case of a monthly series:

![friedman](All_images/friedman.png)

If the null hypothesis of no stable seasonality is rejected at the 1%
significance level, then the series is considered to be seasonal and the
outcome of the test is displayed in green.

The test can be applied to the input series before any seasonal
adjustment method has been applied. It can also be applied to the
seasonally adjusted series or to the irreguar component. In the case of
X-13ARIMA-SEATS, the test is applied to the preliminary estimate of the
unmodified Seasonal-Irregular component[^seasonal-adjustment-2] (time
series shown in Table B3). In this estimate, the number of observations
is lower than in the final estimate of the unmodified Seasonal-Irregular
component. Thus, the number of degrees of freedom in the stable
seasonality test is lower than the number of degrees of freedom in the
test for the [presence of seasonality assuming
stability](../theory/Tests_presence_stability.html). For example,
X-13ARIMA-SEATS uses a centred moving average of order 12 to calculate
the preliminary estimation of trend. Consequently, the first six and
last six points in the series are not computed at this stage of
calculation. The preliminary estimation of the trend is then used for
the calculation of the preliminary estimation of the unmodified
Seasonal-Irregular.

[^seasonal-adjustment-2]: The unmodified Seasonal-Irregular component
    corresponds to the Seasonal-Irregular factors with the extreme
    values.

#### Related tests

-   When using this kind of design for a binary response, one instead
    uses the Cochran's Q test.
-   Kendall's W is a normalization of the Friedman statistic between 0
    and 1.
-   The Wilcoxon signed-rank test is a nonparametric test of
    non-independent data from only two groups.

#### Implementation

This test is implemented in the class
`ec.satoolkit.diagnostics.FriedmanTest`

#### References

-   Friedman, Milton (December 1937). "The use of ranks to avoid the
    assumption of normality implicit in the analysis of variance".
    Journal of the American Statistical Association (American
    Statistical Association) 32 (200): 675--701. <doi:10.2307/2279372>.
    JSTOR 2279372.

-   Friedman, Milton (March 1939). "A correction: The use of ranks to
    avoid the assumption of normality implicit in the analysis of
    variance". Journal of the American Statistical Association (American
    Statistical Association) 34 (205): 109. <doi:10.2307/2279169>. JSTOR
    2279169.

-   Friedman, Milton (March 1940). "A comparison of alternative tests of
    significance for the problem of m rankings". The Annals of
    Mathematical Statistics 11 (1): 86--92.
    <doi:10.1214/aoms/1177731944>. JSTOR 2235971.

### Stable seasonality test (missing)

### Moving seasonality test

The evolutive seasonality test is based on a two-way analysis of
variance model. The model uses the values from complete years only.
Depending on the decomposition type for the Seasonal -- Irregular
component it uses $$1$$ (in the case of a multiplicative model) or $$2$$
(in the case of an additive model):

$$
  \left|\text{SI}_{\text{ij}} - 1 \right| = X_{\text{ij}} = b_{i} + m_{j} + e_{\text{ij}}
  $$, $$1$$ <!---  \[7.148\]     -->

$$
  \left| \text{SI}_{\text{ij}} \right| = X_{\text{ij}} = b_{i} + m_{j} + e_{\text{ij}}
  $$, $$2$$ <!---  \[7.149\]     -->

where:

$m_{j}$ -- the monthly or quarterly effect for $j$-th period,
$j = (1,\ldots,k)$, where $k = 12$ for a monthly series and $k = 4$ for
a quarterly series;

$b_{j}$ -- the annual effect $i$, $(i = 1,\ldots,N)$ where $N$ is the
number of complete years;

$e_{\text{ij}}$ -- the residual effect.

The test is based on the following decomposition:

$$S^{2} = S_{A}^{2} + S_{B}^{2} + S_{R}^{2},$$ $$3$$
<!---  \[7.150\]     -->

where:

$$
S^{2} = \sum_{j = 1}^{k}{\sum_{i = 1}^{N}\left( {\overline{X}}_{\text{ij}} - {\overline{X}}_{\bullet \bullet} \right)^{2}}\ 
$$ --the total sum of squares;

$$
S_{A}^{2} = N\sum_{j = 1}^{k}\left( {\overline{X}}_{\bullet j} - {\overline{X}}_{\bullet \bullet} \right)^{2}
$$ -- the inter-month (inter-quarter, respectively) sum of squares,
which mainly measures the magnitude of the seasonality;

$$
S_{B}^{2} = k\sum_{i = 1}^{N}\left( {\overline{X}}_{i \bullet} - {\overline{X}}_{\bullet \bullet} \right)^{2}
$$ -- the inter-year sum of squares, which mainly measures the
year-to-year movement of seasonality;

$$
S_{R}^{2} = \sum_{i = 1}^{N}{\sum_{j = 1}^{k}\left( {\overline{X}}_{\text{ij}} - {\overline{X}}_{i \bullet} - {\overline{X}}_{\bullet j} - {\overline{X}}_{\bullet \bullet} \right)^{2}}
$$ -- the residual sum of squares.

The null hypothesis $H_{0}\$is that $b_{1} = b_{2} = ... = b_{N}$ which
means that there is no change in seasonality over the years. This
hypothesis is verified by the following test statistic:

$$
   F_{M} = \frac{\frac{S_{B}^{2}}{(n - 1)}}{\frac{S_{R}^{2}}{(n - 1)(k - 1)}}
   $$, $$4$$ <!---   \[7.151\]     -->

which follows an $F$-distribution with $k - 1$ and $n - k$ degrees of
freedom.

### Identifiable seasonality

This test combines the values of the $F$-statistic of the parametric
test for stable seasonality and the values of the moving seasonality
test, which was described above.

The test statistic is:

$$
  T = \left( \frac{\frac{7}{F_{S}} + \frac{3F_{M}}{F_{S}}}{2} \right)^{\frac{1}{2}}
  $$, $$1$$ <!---\[7.152\]     -->

where $F_{S}$ is a stable seasonality test statistic and $F_{M}$ is
moving seasonality test statistic. The test checks if the stable
seasonality is not dominated by moving seasonality. In such a case the
seasonality is regarded as identifiable. This test statistic is used in
the combined seasonality tests (see section [Combined seasonality
test](../theory/Tests_combined.html). The detailed description of the
test is available in LOTHIAN, J., and MORRY, M. (1978).

### Combined seasonality test

This test combines the Kruskal-Wallis test along with test for the
presence of seasonality assuming stability ($F_{S}$), and evaluative
seasonality test for detecting the presence of identifiable seasonality
($F_{M}$). Those three tests are calculated using the final unmodified
SI component. The main purpose of the combined seasonality test is to
check whether the seasonality of the series is identifiable. For
example, the identification of the seasonal pattern is problematic if
the process is dominated by highly moving
seasonality[^seasonal-adjustment-3]. The testing procedure is shown in
the figure below.

[^seasonal-adjustment-3]: DAGUM, E.B. (1987).

![Text](All_images/UG_A_image18.png)

**Combined seasonality test, source: LADIRAY, D., QUENNEVILLE, B.
(2001)**

### Spectral analysis

In order to decide whether a series has a seasonal component that is
predictable (stable) enough, these tests use visual criteria and formal
tests for the periodogram. The periodogram is calculated using complete
years, so that the set of Fourier frequencies contains exactly all
seasonal frequencies[^seasonal-adjustment-4].

[^seasonal-adjustment-4]: For definition of the periodogram and Fourier
    frequencies see section [Spectral Analysis](..\theory\spectral.html)

The tests rely on two basic principles:

-   The peaks associated with seasonal frequencies should be larger than
    \> the median spectrum for all frequencies and;

-   The peaks should exceed the spectrum of the two adjacent values by
    \> more than a critical value.

> JDemetra+ performs this test on the original series. If these two
> requirements are met, the test results are displayed in green. The
> statistical significance of each of the seasonal peaks (i.e.
> frequencies
> $\frac{\pi}{6},\ \frac{\pi}{3},\ \frac{\pi}{2},\ \frac{2\pi}{3}\text{ and } \frac{5\pi}{6}\$corresponding
> to 1, 2, 3, 4 and 5 cycles per year) is also displayed. The seasonal
> and trading days frequencies depends on the frequency of time series.
> They are shown in the table below. The symbol $d$ denotes a default
> frequency and is described below the table.

**The seasonal and trading day frequencies by time series frequency**

| **Number of months per full period** | **Seasonal frequency**                                                              | **Trading day frequency (radians)** |
|------------------|-------------------------------------|------------------|
| 12                                   | $\frac{\pi}{6},\frac{\pi}{3},\ \frac{\pi}{2},\frac{2\pi}{3},\ \frac{5\pi}{6},\ \pi$ | $d$, 2.714                          |
| 6                                    | $\frac{\pi}{3},\frac{2\pi}{3}$, $\pi$                                               | $$d$$                               |
| 4                                    | $\frac{\pi}{2}$, $\pi$                                                              | $d$, 1.292, 1.850, 2.128            |
| 3                                    | $$\pi$$                                                                             | $$d$$                               |
| 2                                    | $$\pi$$                                                                             | $$d$$                               |

The calendar (trading day or working day) effects, related to the
variation in the number of different days of the week per period, can
induce periodic patterns in the data that can be similar to those
resulting from pure seasonal effects. From the theoretical point of
view, trading day variability is mainly due to the fact that the average
number of days in the months or quarters is not equal to a multiple of
$7$ (the average number of days of a month in the year of $365.25\$days
is equal to $\frac{365.25}{12} = 30.4375$ days). This effect occurs
$\frac{365.25}{12} \times \frac{1}{7} = 4.3482$ times per month: one
time for each one of the four complete weeks of each month, and a
residual of $0.3482$ cycles per month, i.e.
$0.3482 \times 2\pi = 2.1878\ radians$. This turns out to be a
fundamental frequency for the effects associated with monthly data. In
JDemetra+ the fundamental frequency corresponding to $0.3482$ cycles per
month is used in place of the closest frequency$\ \frac{\text{πk}}{60}$.
Thus, the quantity $\frac{\pi \times 42}{60}$ is replaced by
$$\omega_{42} = 0.3482 \times 2\pi = 2.1878$$. The frequencies
neighbouring $\omega_{42}$, i.e. $$\omega_{41}$$ and $$\omega_{43}$$ are
set to, respectively, $$2.1865 - \frac{1}{60}$$ and
$$2.1865 + \frac{1}{60}$$.

The default frequencies ($d)\$for calendar effect are: 2.188 (monthly
series) and 0.280 (quarterly series). They are computed as:

$\omega_{\text{ce}} = \frac{2\pi}{7}\left( n - 7 \times \left\lbrack \frac{n}{7} \right\rbrack \right)$,
$$1$$ <!---\[7.158\]      --> where:

$n = \frac{365.25}{s}$, $s = 4$ for quarterly series and $s = 12$ for
monthly series.

Other frequencies that correspond to trading day frequencies are: 2.714
(monthly series) and 1.292, 1.850, 2.128 (quarterly series).

In particular, the calendar frequency in monthly data (marked in red on
the figure below) is very close to the seasonal frequency corresponding
to 4 cycles per year $\text{ω}_{40} = \frac{2}{3}\pi = 2.0944$.

![Text](All_images/UG_A_image19.png)

**Periodogram with seasonal (grey) and calendar (red) frequencies
highlighted**

This implies that it may be hard to disentangle both effects using the
frequency domain techniques.

#### Defining a F-test

link to the definition of the periodogram in the methods part

Brockwell and Davis (1991, section 10.2) exploit the fact that the
periodogram can be expressed as the projection on the orthonormal basis
defined above to derive a test. Thus, under the null hypothesis:

-   $$ 2I(\omega_{k})= \| P_{\bar{sp}_{\left\{ c_{k},s_{k} \right\}}} \mathbf{X} \|^{2}  \sim \sigma^{2} \chi^{2}(2) $$,
    for Fourier frequencies $$ 0 < \omega_{k}=2\pi k/n < \pi $$
-   $$ I(\pi)= \| P_{\bar{sp}_{\left\{ e_{n/2} \right\}}} \mathbf{X} \|^{2}  \sim \sigma^{2} \chi^{2}(1) $$,
    for $$ \pi $$

Because $$ I(\omega_{k}) $$ is independent from the projection error sum
of squares, we can define our F-test statistic as follows:

-   $$ \frac{ 2I(\omega_{k})}{\|\mathbf{X}-P_{\bar{sp}_{\left\{ e_0,c_{k},s_{k} \right\}}} \mathbf{X}\|^2} \frac{n-3}{2} \sim F(2,n-3) $$,
    for Fourier frequencies $$ 0 < \omega_{k}=2\pi k/n < \pi $$
-   $$ \frac{ I(\pi)}{\|\mathbf{X}-P_{\bar{sp}_{\left\{ e_0,e_{n/2} \right\}}} \mathbf{X}\|^2} \frac{n-2}{1} \sim F(1,n-2)$$,
    for $$ \pi $$

where -
$$ \|\mathbf{X}-P_{\bar{sp}_{\left\{ e_0,c_{k},s_{k} \right\}}} \mathbf{X}\|^2  = \sum_{i=1}^{n}\mathbf{X^2_i}-I(0)-2I(\omega_{k}) \sim \sigma^{2} \chi^{2}(n-3)$$
for Fourier frequencies $$ 0 < \omega_{k}=2\pi k/n < \pi $$ -
$$ \|\mathbf{X}-P_{\bar{sp}_{\left\{ e_0,e_{n/2} \right\}}} \mathbf{X}\|^2 = \sum_{i=1}^{n}\mathbf{X^2_i}-I(0)-I(\pi) \sim \sigma^{2} \chi^{2}(n-2)  $$
for $$ \pi $$

Thus, we reject the null if our F-test statistic computed at a given
seasonal frequency (different from $$ \pi $$) is larger than
$$ F_{1-α}(2,n-3)$$. If we consider $$ \pi  $$, our test statistic
follows a $$ F_{1-α}(1,n-2)$$ distribution.

#### Implementation of F-test

The implementation of JDemetra+ considers simultaneously the whole set
of seasonal frequencies (1, 2, 3, 4, 5 and 6 cycles per year). Thus, the
resulting test-statistic is:

$$
 \frac{ 2I(\pi/6)+ 2I(\pi/3)+ 2I(2\pi/3)+ 2I(5\pi/6)+ \delta I(\pi)}{\left\|\mathbf{X}-P_{\bar{sp}_{\left\{ e_0,c_{1},s_{1},c_{2},s_{2},c_{3},s_{3},c_{4},s_{4},c_{5},s_{5}, \delta e_{n/2} \right\}}} \mathbf{X} \right\|^2} \frac{n-12}{11} \sim F(11-\delta,n-12+\delta) 
$$ where $$ \delta=1 $$ if $$ n $$ is even and 0 otherwise.

In small samples, the test performs better when the periodogram is
evaluated as the exact seasonal frequencies. JDemetra+ modifies the
sample size to ensure the seasonal frequencies belong to the set of
Fourier frequencies. This strategy provides a very simple and effective
way to eliminate the leakage problem.

Example of how results are displayed:

![periodtest](All_images/periodogram.png) \#### Identification of
seasonal peaks in a Tukey periodogram and in an autoregressive spectrum
In order to decide whether a series has a seasonal component that is
predictable (stable) enough, these tests use visual criteria and formal
tests for the periodogram. The periodogram is calculated using complete
years, so that the set of Fourier frequencies contains exactly all
seasonal frequencies[^seasonal-adjustment-5].

[^seasonal-adjustment-5]: For definition of the periodogram and Fourier
    frequencies see section [Spectral Analysis](..\theory\spectral.html)

The tests rely on two basic principles:

-   The peaks associated with seasonal frequencies should be larger than
    the median spectrum for all frequencies and;

-   The peaks should exceed the spectrum of the two adjacent values by
    more than a critical value.

> JDemetra+ performs this test on the original series. If these two
> requirements are met, the test results are displayed in green. The
> statistical significance of each of the seasonal peaks (i.e.
> frequencies
> $\frac{\pi}{6},\ \frac{\pi}{3},\ \frac{\pi}{2},\ \frac{2\pi}{3}$ and
> \$\frac{5\pi}{6}\$ corresponding to 1, 2, 3, 4 and 5 cycles per year)
> is also displayed. The seasonal and trading days frequencies depends
> on the frequency of time series. They are shown in the table below.
> The symbol $d$ denotes a default frequency and is described below the
> table.

**The seasonal and trading day frequencies by time series frequency**

| **Number of months per full period** | **Seasonal frequency**                                                              | **Trading day frequency (radians)** |
|------------------|-------------------------------------|------------------|
| 12                                   | $\frac{\pi}{6},\frac{\pi}{3},\ \frac{\pi}{2},\frac{2\pi}{3},\ \frac{5\pi}{6},\ \pi$ | $d$, 2.714                          |
| 6                                    | $\frac{\pi}{3},\frac{2\pi}{3}$, $\pi$                                               | $$d$$                               |
| 4                                    | $\frac{\pi}{2}$, $\pi$                                                              | $d$, 1.292, 1.850, 2.128            |
| 3                                    | $$\pi$$                                                                             | $$d$$                               |
| 2                                    | $$\pi$$                                                                             | $$d$$                               |

The calendar (trading day or working day) effects, related to the
variation in the number of different days of the week per period, can
induce periodic patterns in the data that can be similar to those
resulting from pure seasonal effects. From the theoretical point of
view, trading day variability is mainly due to the fact that the average
number of days in the months or quarters is not equal to a multiple of 7
(the average number of days of a month in the year of 365.25 days is
equal to $\frac{365.25}{12} =$ 30.4375 days). This effect occurs
$\frac{365.25}{12} \times \frac{1}{7} =$ 4.3482 times per month: one
time for each one of the four complete weeks of each month, and a
residual of 0.3482 cycles per month, i.e. $0.3482 \times 2\pi = 2.1878$
radians. This turns out to be a fundamental frequency for the effects
associated with monthly data. In JDemetra+ the fundamental frequency
corresponding to 0.3482 cycles per month is used in place of the closest
frequency $\frac{\text{πk}}{60}$. Thus, the quantity
$\frac{\pi \times 42}{60}$ is replaced by
$\omega_{42} = 0.3482 \times 2\pi = 2.1878$. The frequencies
neighbouring $\omega_{42}$, i.e. $\omega_{41}$ and $\omega_{43}$ are set
to, respectively, $2.1865 - \frac{1}{60}$ and $2.1865 + \frac{1}{60}$.

The default frequencies ($d)\$for calendar effect are: 2.188 (monthly
series) and 0.280 (quarterly series). They are computed as:

$$
 \omega_{\text{ce}} = \frac{2\pi}{7}\left( n - 7 \times \left\lbrack \frac{n}{7} \right\rbrack \right)
 $$, $$1$$ <!---\[7.158\]      -->

where:

$n = \frac{365.25}{s}$, $s = 4$ for quarterly series and $s = 12$ for
monthly series.

Other frequencies that correspond to trading day frequencies are: 2.714
(monthly series) and 1.292, 1.850, 2.128 (quarterly series).

In particular, the calendar frequency in monthly data (marked in red on
the figure below) is very close to the seasonal frequency corresponding
to 4 cycles per year $\text{ω}_{40} = \frac{2}{3}\pi = 2.0944$.

![Text](All_images/UG_A_image19.png)

**Periodogram with seasonal (grey) and calendar (red) frequencies
highlighted**

This implies that it may be hard to disentangle both effects using the
frequency domain techniques.

#### Graphical Test based on AR spectrum

for AR spectrum definition link to methods part

![Text](All_images/UG_A_image19.png)

**Periodogram with seasonal (grey) and calendar (red) frequencies
highlighted**

The statistical significance of the peaks associated to a given
frequency can be informally tested using a visual criterion, which has
proved to perform well in simulation experiments. Visually significant
peaks for a frequency $$\lambda_{j}$$ satisfy both conditions:

-   $$ \frac{f_{x}(\lambda_{j})- \max \left\{f_{x}(\lambda_{j+1}),f_{x}(\lambda_{j-1}) \right\}}{\left[ \max_{k}f_{x}(\lambda_{k})-\min_{i}f_{x}(\lambda_{i}) \right]}\ge CV(\lambda_{j}) $$,
    where $$ CV(\lambda_{j})$$ can be set equal to $$6/52 $$ for all
    $$j$$
-   $$ f_{x}(\lambda_{j})> median_{j} \left\{ f_{x}(\lambda_{j}) \right\}$$,
    which guarantees $$ f_{x}(\lambda_{j}) $$ it is not a local peak.

The first condition implies that if we divide the range
$$\max_{k}f_{x}(\lambda_{k})-\min_{i}f_{x}(\lambda_{i})$$ in 52 parts
(traditionally represented by stars) the height of each pick should be
at least 6 stars.

#### Graphical Test based on Tukey spectrum

link to methods/spectral analysis section for Tukeys definition

The current JDemetra+ implementation of the seasonality test is based on
a $$F(d_{1},d_{2})$$ approximation that has been originally proposed by
Maravall (2012) for TRAMO-SEATS. This test is has been designed for a
Blackman-Tukey window based on a particular choices of the truncation
lag $$r$$ and sample size. Following this approach, we determine
visually significant peaks for a frequency $$\omega_{j}$$ when

$$ 
\frac{2 f_{x}(\omega_{j})}{\left[ f_{x}(\omega_{j+1})+ f_{x}(\omega_{j-1}) \right]} \ge CV(\omega_{j}) 
$$

where $$ CV(\omega_{j})$$ is the critical value of a $$F(d_{1},d_{2})$$
distribution, where the degrees of freedom are determined using
simulations. For $$\omega_{j}= \pi$$, we have a significant peak when
$$\frac{f_{x}(\omega_{[n/2]})}{\left[ f_{x}(\omega_{[(n-1)/2]})\right]} \ge CV(\omega_{j}) $$

Two significant levels for this test are considered: $$\alpha=0.05$$
(code "t") and $$\alpha=0.01$$ (code "T").

As opposed to the [AR
spectrum](%7B%7B%20site.baseurl%20%7D%7D/pages/theory/Tests_ARspectrum.html),
which is computed on the basis of the last $$120$$ data points, we will
use here all available observations. Those critical values have been
calculated given the recommended truncation lag $$r=79$$ for a sample
size within the interval $$n \in [80,119]$$ and $$r=112$$ for
$$n \in [120,300]$$ . The $$F$$ approximation is less accurate for
sample sizes larger than $$300$$. For quarterly data, $$r=44 $$, but
there are no recommendations regarding the required sample size

JDemetra+ considers critical values for $$ \alpha=1\%$$ (code "T") and
$$ \alpha=5\%$$ (code "t") at each one of the seasonal frequencies
represented in the table below, e.g. frequencies
$\frac{\pi}{6},\ \frac{\pi}{3},\ \frac{\pi}{2},\ \frac{2\pi}{3}\text{ and } \frac{5\pi}{6}\$
corresponding to 1, 2, 3, 4, 5 and 6 cycles per year in this example,
since we are dealing with monthly data. The codes "a" and "A" correpond
to the so-called [AR
spectrum](%7B%7B%20site.baseurl%20%7D%7D/pages/theory/Tests_ARspectrum.html),
so ignore them for the moment.

**The seasonal and trading day frequencies by time series frequency**

| **Number of months per full period** | **Seasonal frequency**                                                              | **Trading day frequency (radians)** |
|------------------|-------------------------------------|------------------|
| 12                                   | $\frac{\pi}{6},\frac{\pi}{3},\ \frac{\pi}{2},\frac{2\pi}{3},\ \frac{5\pi}{6},\ \pi$ | $d$, 2.714                          |
| 6                                    | $\frac{\pi}{3},\frac{2\pi}{3}$, $\pi$                                               | $$d$$                               |
| 4                                    | $\frac{\pi}{2}$, $\pi$                                                              | $d$, 1.292, 1.850, 2.128            |
| 3                                    | $$\pi$$                                                                             | $$d$$                               |
| 2                                    | $$\pi$$                                                                             | $$d$$                               |

Currently, only seasonal frequencies are tested, but the program allows
you to manually plot the Tukey spectrum and focus your attention on both
seasonal and trading day frequencies.

#### References

Tukey, J. (1949). The sampling theory of power spectrum estimates.,
Proceedings Symposium on Applications of Autocorrelation Analysis to
Physical Problems, NAVEXOS-P-735, Office of Naval Research, Washington,
47-69

Brockwell, P.J., and R.A. Davis (1991). Times Series: Theory and
Methods. Springer Series in Statistics.

### Example of non seasonal series

The [ESS Guidelines on Seasonal Adjustment
(2015)](https://ec.europa.eu/eurostat/documents/3859598/6830795/KS-GQ-15-001-EN-N.pdf/d8f1e5f5-251b-4a69-93e3-079031b74bd3)
recommend to apply seasonal adjustment only to those time series for
which the seasonal and/or calendar effects can be properly explained,
identified and estimated. Therefore, seasonal adjustment of non-seasonal
time series is an inappropriate treatment. This case study explains how
to recognize a non-seasonal time series using the tools and
functionalities implemented in JDemetra+.

1.  The picture below shows the results from the seasonal adjustment of
    a stock market turnover series from Greece using the RSA4c
    specification. The test diagnostics do not indicate any problems in
    the modelling phase (residual seasonality statistics and
    out-of-sample tests are displayed in green). The seasonality seems
    to be removed from the time series, but the overall assessment is
    uncertain, due to the failure of the m-statistics and the visual
    spectral analysis.

    ![Text](All_images/UG_SA_image5.jpg)

    **The diagnostic results for stock market turnover in Greece**

2.  The inspection of a graph hints at the source of the problem. The
    original time series does not manifest any seasonal movements (left
    panel). It should be noted that when the X-13ARIMA-SEATS method is
    used for seasonal adjustment, the seasonal component is estimated
    regardless of the properties of the original time series (right
    panel). It means that the seasonal component is estimated even if
    there are no signs of the presence of seasonal fluctuations in the
    time series. In the picture below the seasonal component (blue line)
    is moving rather than being stable and the averages for the specific
    months (red lines) are not at the same level, suggesting some
    intra-year differences between seasons. Nevertheless, the SI ratios
    (dots) are rather far from the seasonal component, indicating that
    the irregular movements dominate over the seasonal ones.

    ![Text](All_images/UG_SA_image6.jpg)

    **Original and seasonally adjusted time series and the trend-cycle
    component (left) and SI ratios (right)**

3.  The seasonality tests performed for the original time
    series[^seasonal-adjustment-6] are ambiguous. Some suggest that
    seasonality is not present (the outcomes of three tests: the
    auto-correlation at seasonal lags, the spectral peaks test and the
    seasonal dummies test all indicate no seasonality in the original
    time series). These tests are available in the *Diagnostic* section
    of the output tree. The seasonality tests can be executed
    independently from the seasonal adjustment proces. The descriptions
    of these tests are given in the [*Seasonality
    tests*](../case-studies/seasonalitytests.html) scenario.

    ![Text](All_images/UG_SA_image7.jpg)

    **Seasonality test for the original (transformed) series**

4.  Another sign indicating that the presence of seasonality is
    uncertain should be addressed : the non-seasonal ARIMA model chosen
    by the automatic model identification procedure. The details of the
    RegARIMA model are available in the *Pre-processing* node.

    ![Text](All_images/UG_SA_image8.jpg)

    **Estimation results for the RegARIMA model**

5.  For X-13ARIMA-SEATS the most relevant tool to assess the presence of
    seasonal movement in the time series is a combined seasonality test.
    For the series presented in this case study the result of the
    combined seasonality test confirms that the movements observed in
    the time series are not stable and regular enough to be recognized
    as seasonal ones.

    ![Text](All_images/UG_SA_image9.jpg)

    **Combined seasonality test result**

6.  Regardless of the presence and/or significance of seasonal movements
    in the original time series the seasonal component is always
    estimated by X-13ARIMA-SEATS, as shown in the picture below (from
    the panel on the left choose *Main results* → *Table*). Therefore
    X-13ARIMA-SEATS users should always check the outcome of the
    combined seasonality test.

    ![Text](All_images/UG_SA_image10.jpg)

    **Decomposition's results**

7.  In general, in the case of a non-seasonal time series the
    TRAMO-SEATS method produces more coherent results than
    X-13ARIMA-SEATS. When no seasonal movements are detected the
    non-seasonal ARIMA model is used and the seasonal component is not
    estimated.

    ![Text](All_images/UG_SA_image11.jpg)

    **Decomposition result for a non-seasonal time series -
    TRAMO-SEATS**

8.  Consequently, the *SI ratios* (dots) estimated by TRAMO-SEATS are
    equal to the irregular component and for each month the seasonal
    component is equal to the mean (red, horizontal line), which is one.

    ![Text](All_images/UG_SA_image12.jpg)

    **SI ratios for a non-seasonal time series - TRAMO-SEATS**

[^seasonal-adjustment-6]: When the series are non-stationary
    differentiation is performed before the seasonality tests.

## Calendar correction

here definitions, test in the pre adj part ?

### Overview of Calendar effects in JDemetra+

The following description of the calendar effects in JDemetra+ is
strictly based on PALATE, J. (2014).

A natural way for modelling calendar effects consists of distributing
the days of each period into different groups. The regression variable
corresponding to a type of day (a group) is simply defined by the number
of days it contains for each period. Usual classifications are:

-   Trading days (7 groups): each day of the week defines a group
    (Mondays,\...,Sundays);

-   Working days (2 groups): week days and weekends.

The definition of a group could involve partial days. For instance, we
could consider that one half of Saturdays belong to week days and the
second half to weekends.

Usually, specific holidays are handled as Sundays and they are included
in the group corresponding to \"non-working days\". This approach
assumes that the economic activity on national holidays is the same (or
very close to) the level of activity that is typical for Sundays.
Alternatively, specific holidays can be considered separately, e.g. by
the specification that divided days into three groups:

-   Working days (Mondays to Fridays, except for specific holidays),

-   Non-working days (Saturdays and Sundays, except for specific
    holidays),

-   Specific holidays.

### Summary of the method used in JDemetra+ to compute trading day and working day effects

The computation of trading day and working days effects is performed in
four steps:

1.  Computation of the number of each weekday performed for all periods.

2.  Calculation of the usual contrast variables for trading day and
    working day.

3.  Correction of the contrast variables with specific holidays (for
    each holiday add +1 to the number of Sundays and subtract 1 from the
    number of days of the holiday). The correction is not performed if
    the holiday falls on a Sunday, taking into account the validity
    period of the holiday.

4.  Correction of the constant variables for long term mean effects,
    > taking into account the validity period of the holiday; see below
    > for the different cases.

The corrections of the constant variables may receive a weight
corresponding to the part of the holiday considered as a Sunday.

An example below illustrates the application of the above algorithm for
the hypothetical country in which three holidays are celebrated:

-   New Year (a fixed holiday, celebrated on 01 January);

-   Shrove Tuesday (a moving holiday, which falls 47 days before Easter
    Sunday, celebrated until the end of 2012);

-   Freedom day (a fixed holiday, celebrated on 25 April).

The consecutive steps in calculation of the calendar for 2012 and 2013
years are explained below.

First, the number of each day of the week in the given month is
calculated as it is shown in table below.


**Number of each weekday in different months**


  |**Month**   | **Mon**  | **Tue**  | **Wed**  | **Thu**  | **Fri**  | **Sat**  | **Sun**|
  |----------- | ---------| ---------| ---------| ---------| ---------| ---------| ---------|
  |Jan-12      |5         |5         |4         |4         |4         |4         |5|
  |Feb-12      |4         |4         |5         |4         |4         |4         |4|
  |Mar-12      |4         |4         |4         |5         |5         |5         |4|
  |Apr-12      |5         |4         |4         |4         |4         |4         |5|
  |May-12      |4         |5         |5         |5         |4         |4         |4|
  |Jun-12      |4         |4         |4         |4         |5         |5         |4|
  |Jul-12      |5         |5         |4         |4         |4         |4         |5|
  |Aug-12      |4         |4         |5         |5         |5         |4         |4|
  |Sep-12      |4         |4         |4         |4         |4         |5         |5|
  |Oct-12      |5         |5         |5         |4         |4         |4         |4|
  |Nov-12      |4         |4         |4         |5         |5         |4         |4|
  |Dec-12      |5         |4         |4         |4         |4         |5         |5|
  |Jan-13      |4         |5         |5         |5         |4         |4         |4|
  |Feb-13      |4         |4         |4         |4         |4         |4         |4|
  |Mar-13      |4         |4         |4         |4         |5         |5         |5|
  |Apr-13      |5         |5         |4         |4         |4         |4         |4|
  |May-13      |4         |4         |5         |5         |5         |4         |4|
  |Jun-13      |4         |4         |4         |4         |4         |5         |5|
  |Jul-13      |5         |5         |5         |4         |4         |4         |4|
  |Aug-13      |4         |4         |4         |5         |5         |5         |4|
  |Sep-13      |5         |4         |4         |4         |4         |4         |5|
  |Oct-13      |4         |5         |5         |5         |4         |4         |4|
  |Nov-13      |4         |4         |4         |4         |5         |5         |4|
  |Dec-13      |5         |5         |4         |4         |4         |4         |5|

Next, the contrast variables are calculated (table below) as a result of
the linear transformation applied to the variables presented in table below.

**Contrast variables (series corrected for leap year effects)**


  |**Month**  |**Mon**   |**Tue**   |**Wed**  | **Thu**  | **Fri**   |**Sat**   |**Length**|
  |-----------| ---------| ---------|---------| ---------| --------- |--------- |------------|
  |Jan-12     | 0        |0         |-1       | -1       | -1        |-1        |0|
  |Feb-12     | 0        | 0        | 1       |  0       |  0        | 0        | 0.75|
  |Mar-12     | 0        | 0        | 0       |  1       |  1        | 1        | 0|
  |Apr-12     | 0        | -1       | -1      |  -1      |  -1       | -1       | 0|
  |May-12     | 0        | 1        | 1       |  1       |  0        | 0        | 0|
  |Jun-12     | 0        | 0        | 0       |  0       |  1        | 1        | 0|
  |Jul-12     | 0        | 0        | -1      |  -1      |  -1       | -1       | 0|
  |Aug-12     | 0        | 0        | 1       |  1       |  1        | 0        | 0|
  |Sep-12     | -1       | -1       | -1      |  -1      |  -1       | 0        | 0|
  |Oct-12     | 1        | 1        | 1       |  0       |  0        | 0        | 0|
  |Nov-12     | 0        | 0        | 0       |  1       |  1        | 0        | 0|
  |Dec-12     | 0        | -1       | -1      |  -1      |  -1       | 0        | 0|
  |Jan-13     | 0        | 1        | 1       |  1       |  0        | 0        | 0|
  |Feb-13     | 0        | 0        | 0       |  0       |  0        | 0        | -0.25|
  |Mar-13     | -1       | -1       | -1      |  -1      |  0        | 0        | 0|
  |Apr-13     | 1        | 1        | 0       |  0       |  0        | 0        | 0|
  |May-13     | 0        | 0        | 1       |  1       |  1        | 0        | 0|
  |Jun-13     | -1       | -1       | -1      |  -1      |  -1       | 0        | 0|
  |Jul-13     | 1        | 1        | 1       |  0       |  0        | 0        | 0|
  |Aug-13     | 0        | 0        | 0       |  1       |  1        | 1        | 0|
  |Sep-13     | 0        | -1       |-1       | -1       | -1        |-1        | 0|
  |Oct-13     | 0        | 1        | 1       |  1       |  0        | 0        | 0|
  |Nov-13     | 0        | 0        | 0       |  0       |  1        | 1        | 0|
  |Dec-13     | 5        | 5        | 4       |  4       | 4         |4         | 0|

In the next step the corrections for holidays is done in the following
way:

-   New Year: In 2012 it falls on a Sunday. Therefore no correction is
    applied. In 2013 it falls on a Tuesday. Consequently, the following
    corrections are applied to the number of each weekday in January:
    Tuesday -1, Sunday +1, so the following corrections are applied to
    the contrast variables: -2 for Tuesday and -1 for the other contrast
    variables.

-   Shrove Tuesday: It is a fixed day of the week holiday that always
    falls on Tuesday. For this reason in 2012 the following corrections
    are applied to the number of each weekday in February: Tuesday -1,
    Sunday +1, so the following corrections are applied to the contrast
    variables: -2 for the contrast variable associated with Tuesday,
    and -1 for the other contrast variables. The holiday expires at the
    end of 2012. Therefore no corrections are made for 2013.

-   Freedom Day: In 2012 it falls on a Wednesday. Consequently, the
    following corrections are applied to the number of each weekday in
    April: Wednesday -1, Sunday +1, so the following corrections are
    applied to the contrast variables: -2 for Wednesday and -1 for the
    other contrast variables. In 2013 it falls on Thursday. Therefore,
    the following corrections are applied to the number of each weekday
    in April: Thursday -1, Sunday +1, so the following corrections are
    applied to the contrast variables: -2 for Thursday, and -1 for the
    other contrast variables.

The result of these corrections is presented in table below.

**Contrast variables corrected for holidays**


  |**Month**  | **Mon**   |**Tue**  | **Wed**  | **Thu**  | **Fri** |  **Sat** |  **Length**|
  |-----------| --------- |---------| ---------| ---------|---------| ---------| -----------|
  |Jan-12     | 0         |0        | -1       | -1       | -1      |  -1      |  0|
  |Feb-12     | -1        |-2       | 0        | -1       | -1      |  -1      |  0.75|
  |Mar-12     | 0         |0        | 0        | 1        | 1       | 1        |  0|
  |Apr-12     | -1        |-2       | -3       | -2       | -2      |  -2      |  0|
  |May-12     | 0         |1        | 1        | 1        | 0       |  0       |  0|
  |Jun-12     | 0         |0        | 0        | 0        | 1       |  1       |  0|
  |Jul-12     | 0         |0        | -1       | -1       | -1      |  -1      |  0|
  |Aug-12     | 0         |0        | 1        | 1        | 1       |  0       |  0|
  |Sep-12     | -1        |-1       | -1       | -1       | -1      |  0       |  0|
  |Oct-12     | 1         |1        | 1        | 0        | 0       |  0       |  0|
  |Nov-12     | 0         |0        | 0        | 1        | 1       |  0       |  0|
  |Dec-12     | 0         |-1       | -1       | -1       | -1      |  0       |  0|
  |Jan-13     | -1        |-1       | 0        | 0        | -1      |  -1      |  0|
  |Feb-13     | 0         |0        | 0        | 0        | 0       |  0       |  -0.25|
  |Mar-13     | -1        |-1       | -1       | -1       | 0       |  0       |  0|
  |Apr-13     | 0         |0        | -1       | -2       | -1      |  -1      |  0|
  |May-13     | 0         |0        | 1        |1         | 1       |  0       |  0|
  |Jun-13     |-1         |-1       | -1       | -1       |-1       | 0        |  0|
  |Jul-13     | 1         |1        | 1        | 0        | 0       |  0       |  0|
  |Aug-13     | 0         |0        | 0        | 1        | 1       |  1       |  0|
  |Sep-13     | 0         |-1       | -1       | -1       | -1      |  -1      |  0|
  |Oct-13     | 0         |1        | 1        | 1        | 0       |  0       |  0|
  |Nov-13     | 0         |0        | 0        | 0        | 1       |  1       |  0|
  |Dec-13     | 0         |0        | -1       | -1       | -1      |  -1      |  0|

Finally, the long term corrections are applied on each year of the
validity period of the holiday.

-   New Year: Correction on the contrasts: +1, to be applied to January
    of 2012 and 2013.

-   Shrove Tuesday: It may fall either in February or in March. It will
    fall in March if Easter is on or after 17 April. Taking into account
    the theoretical distribution of Easter, it gives: prob(March) =
    +0.22147, prob(February) = +0.77853. The correction of the contrasts
    will be +1.55707 for Tuesday in February 2012 and +0.77853 for the
    other contrast variables. The correction of the contrasts will be
    +0.44293 for Tuesday in March 2012, +0.22147 for the other contrast
    variables.

-   Freedom Day: Correction on the contrasts: +1, to be applied to April
    of 2012 and 2013.

The modifications due to the corrections described above are presented
in table below.

**Trading day variables corrected for the long term effects**


  |**Month**  | **Mon**   | **Tue**   | **Wed**   | **Thu**   |  **Fri**  | **Sat**  |  **Length**|
  |-----------| ----------| ----------| ----------| ----------| ----------|----------| ------------|
  |Jan-12     | 1         | 1         | 0         | 0         | 0         |0         | 0|
  |Feb-12     | -0.22115  | -0.44229  | 0.778853  |-0.22115   |-0.22115   |-0.22115  | 0.75|
  |Mar-12     | 0.221147  | 0.442293  | 0.221147  |1.221147   |1.221147   |1.221147  | 0|
  |Apr-12     | 0         | -1        | -2        | -1        | -1        | -1       |  0|
  |May-12     | 0         | 1         | 1         | 1         | 0         | 0        |  0|
  |Jun-12     | 0         | 0         | 0         | 0         | 1         | 1        |  0|
  |Jul-12     | 0         | 0         | -1        | -1        | -1        | -1       |  0|
  |Aug-12     | 0         | 0         | 1         | 1         | 1         | 0        |  0|
  |Sep-12     | -1        | -1        | -1        | -1        | -1        | 0        |  0|
  |Oct-12     | 1         | 1         | 1         | 0         | 0         | 0        |  0|
  |Nov-12     | 0         | 0         | 0         | 1         | 1         | 0        |  0|
  |Dec-12     | 0         | -1        | -1        | -1        | -1        | 0        |  0|
  |Jan-13     | 0         | 0         | 1         | 1         | 0         | 0        |  0 |
  |Feb-13     | 0         | 0         | 0         | 0         | 0         | 0        |  -0.25|
  |Mar-13     | -1        | -1        | -1        |-1         | 0         | 0        |  0|
  |Apr-13     | 1         | 1         | 0         | -1        | 0         | 0        |  0|
  |May-13     | 0         | 0         | 1         | 1         | 1         | 0        |  0|
  |Jun-13     | -1        | -1        | -1        | -1        | -1        | 0        |  0|
  |Jul-13     | 1         | 1         | 1         | 0         | 0         | 0        |  0|
  |Aug-13     | 0         | 0         | 0         | 1         | 1         | 1        |  0|
  |Sep-13     | 0         | -1        | -1        |-1         |-1         |-1        |  0|
  |Oct-13     | 0         | 1         | 1         | 1         | 0         | 0        |  0|
  |Nov-13     | 0         | 0         | 0         | 0         | 1         |1         |  0|
  |Dec-13     | 0         | 0         | -1        |-1         |-1         |-1        |  0|
  

### Mean and seasonal effects of calendar variables

The calendar effects produced by the regression variables that fulfil
the definition presented above include a mean effect (i.e. an effect
that is independent of the period) and a seasonal effect (i.e. an effect
that is dependent of the period and on average it is equal to 0). Such
an outcome is inappropriate, as in the usual decomposition of a series
the mean effect should be allocated to the trend component and the fixed
seasonal effect should be affected to the corresponding component.
Therefore, the actual calendar effect should only contain effects that
don\'t belong to the other components.

In the context of JDemetra+ the mean effect and the seasonal effect are
long term theoretical effects rather than the effects computed on the
time span of the considered series (which should be continuously
revised).

The mean effect of a calendar variable is the average number of days in
its group. Taking into account that one year has on average 365.25 days,
the monthly mean effects for a working days are, as shown in the table below,
21.7411 for week days and 8.696 for weekends.

**Monthly mean effects for the Working day variable**

 
 |**Groups of Working day effect**|   **Mean effect**            |
 |--------------------------------|------------------------------|
 |Week days                       |  365.25/12\*5/7 = **21.7411**|
 |Weekends                        |  365.25/12\*2/7 = **8.696**  |
 |Total                           | 365.25/12 = **30.4375**      |

The number of days by period is highly seasonal, as apart from
February, the length of each month is the same every year. For this
reason, any set of calendar variables will contain, at least in some
variables, a significant seasonal effect, which is defined as the
average number of days by period (Januaries\..., first quarters\...)
outside the mean effect. Removing that fixed seasonal effects consists
of removing for each period the long term average of days that belong to
it. The calculation of a seasonal effect for the working days
classification is presented in the table below.

**The mean effect and the seasonal effect for the calendar periods**

 
  |**Period**   |**Average number of days**   |**Average number of week days**   |**Mean effect**   |**Seasonal effect** |
  |------------ |---------------------------- |--------------------------------- |----------------- |--------------------|
  |January      |31                           |31\*5/7=22.1429                   |21.7411           |0.4018|
  |February     |28.25                        |28.25\*5/7=20.1786                |21.7411           |-1.5625|
  |March        |31                           |31\*5/7=22.1429                   |21.7411           |0.4018|
  |April        |30                           |30\*5/7=21.4286                   |21.7411           |-0.3125|
  |May          |31                           |31\*5/7=22.1429                   |21.7411           |0.4018|
  |June         |30                           |30\*5/7=21.4286                   |21.7411           |-0.3125|
  |July         |31                           |31\*5/7=22.1429                   |21.7411           |0.4018|
  |August       |31                           |31\*5/7=22.1429                   |21.7411           |0.4018|
  |September    |30                           |30\*5/7=21.4286                   |21.7411           |-0.3125|
  |October      |31                           |31\*5/7=22.1429                   |21.7411           |0.4018|
  |November     |30                           |30\*5/7=21.4286                   |21.7411           |-0.3125|
  |December     |31                           |31\*5/7=22.1429                   |21.7411           |0.4018|
  |Total        |365.25                       |260.8929                          |260.8929          |0|

For a given time span, the actual calendar effect for week days can be
easily calculated as the difference between the number of week days in a
specific period and the sum of the mean effect and the seasonal effect
assigned to this period, as it is shown in the table below for the period
01.2013 -- 06.2013.

**The calendar effect for the period 01.2013 - 06.2013**

 
  |**Time period (t)**   |**Week days**   |**Mean effect**   |**Seasonal effect**   |**Calendar effect**|
  |--------------------- |--------------- |----------------- |--------------------- |---------------------|
  |Jan-2013              |23              |21.7411           |0.4018                |0.8571|
  |Feb-2013              |20              |21.7411           |-1.5625               |-0.1786|
  |Mar-2013              |21              |21.7411           |0.4018                |-1.1429|
  |Apr-2013              |22              |21.7411           |-0.3125               |0.5714|
  |May-2013              |23              |21.7411           |0.4018                |0.8571|
  |Jun-2013              |20              |21.7411           |-0.3125               |-1.4286|
  |Jul-2013              |23              |21.7411           |0.4018                |0.8571|

The distinction between the mean effect and the seasonal effect is
usually unnecessary. Those effects can be considered together (simply
called mean effects) and be computed by removing from each calendar
variable its average number of days by period. These global means effect
are considered in the next section.

### Impact of the mean effects on the decomposition

When the ARIMA model contains a seasonal difference -- something that
should always happen with calendar variables -- the mean effects
contained in the calendar variables are automatically eliminated, so
that they don\'t modify the estimation. The model is indeed estimated on
the series/regression variables after differencing. However, they lead
to a different linearised series ($y_{\text{lin}})$. The impact of other
corrections (mean and/or fixed seasonal) on the decomposition is
presented in the next paragraph. Such corrections could be obtained, for
instance, by applying other solutions for the long term corrections or
by computing them on the time span of the series.

Now the model with \"correct\" calendar effects (denoted as $C$), i.e.
effects without mean and fixed seasonal effects, can be considered. To
simplify the problem, the model has no other regression effects.

For such a model the following relations hold:

$$y_{\text{lin}} = \ y - C$$                       
$$T = \ F_{T}\left( y_{\text{lin}} \right)$$       
$$S = \ F_{S}\left( y_{\text{lin}} \right) + C$$   
$$I = \ F_{I}\left( y_{\text{lin}} \right)$$       

where:

T - the trend;

S - the seasonal component;

I - the irregular component;

$F_{X}$ - the linear filter for the component X.

Consider next other calendar effects ($\widetilde{C}$) that contain some
mean ($\text{cm}$, integrated to the final trend) and fixed seasonal
effects ($\text{cs}$, integrated to the final seasonal). The modified
equations are now:

  $$\widetilde{C} = C + cm + cs$$                                                       
  $${\widetilde{y}}_{\text{lin}} = \ y - \widetilde{C} = \ y_{\text{lin}} - cm - cs$$   
  $$\widetilde{T} = \ F_{T}\left( {\widetilde{y}}_{\text{lin}} \right) + cm$$           
  $$\widetilde{S} = \ F_{S}\left( {\widetilde{y}}_{\text{lin}} \right) + C + cs$$       
  $$\widetilde{I} = \ F_{I}\left( {\widetilde{y}}_{\text{lin}} \right)$$                

Taking into account that $F_{X}$ is a linear transformation and
that[^64]

  $$F_{T}\left( \text{cm} \right) = cm$$   
  $$F_{T}\left( \text{cs} \right) = 0$$     
  $$F_{S}\left( \text{cm} \right) = 0\ $$   
  $$F_{S}\left( \text{cs} \right) = cs$$    
  $$F_{I}\left( \text{cm} \right) = 0$$     
  $$F_{I}\left( \text{cs} \right) = 0$$     

The following relationships hold:

  $$\widetilde{T} = \ F_{T}\left( {\widetilde{y}}_{\text{lin}} \right) + cm = F_{T}\left( y_{\text{lin}} \right) - cm + cm = T$$           
  $$\widetilde{S} = \ F_{S}\left( {\widetilde{y}}_{\text{lin}} \right) + C + cs = F_{S}\left( y_{\text{lin}} \right) - cs + C + cs = S$$   
  $$\widetilde{I} = \ I$$                                                                                                                 

If we don't take into account the effects and apply the same approach
as in the "correct" calendar effects, we will get:

  $$\breve{T} = \ F_{T}\left( {\widetilde{y}}_{\text{lin}} \right) = T - cm$$                   
  $$\breve{S} = \ F_{S}\left( {\widetilde{y}}_{\text{lin}} \right) + \widetilde{C} = S + cm$$   
  $$\breve{I} = \ F_{I}\left( {\widetilde{y}}_{\text{lin}} \right) = I$$                        

The trend, seasonal and seasonally adjusted series will only differ by a
(usually small) constant.

In summary, the decomposition does not depend on the mean and fixed
seasonal effects used for the calendar effects, provided that those
effects are integrated in the corresponding final components. If these
corrections are not taken into account, the main series of the
decomposition will only differ by a constant.


[^64]: In case of SEATS the properties can be trivially derived from the matrix formulation of signal extraction. They are also valid for X-11 (additive).


### Linear transformations of the calendar variables



As far as the RegARIMA and the TRAMO models are considered, any
non-degenerated linear transformation of the calendar variables can be
used. It will produce the same results (likelihood, residuals,
parameters, joint effect of the calendar variables, joint F-test on the
coefficients of the calendar variables...). The linearised series that
will be further decomposed is invariant to any linear transformation of
the calendar variables.

However, it should be mentioned that choices of calendar corrections
based on the tests on the individual t statistics are dependent on the
transformation, which is rather arbitrary. This is the case in old
versions of TRAMO-SEATS. That is why the joint F-test (as in the version
of TRAMO-SEATS implemented in TSW+) should be preferred.

An example of a linear transformation is the calculation of the contrast
variables. In the case of the usual trading day variables, they are
defined by the following transformation: the 6 contrast variables
($\text{No.}\left( \text{Mondays} \right) - No.\left( \text{Sundays} \right),\ldots No.\left( \text{Saturdays} \right) - No.(Sundays)$)
used with the length of period.


  $$\begin{bmatrix}                 
  1 & 0 & 0 & 0 & 0 & 0 & - 1 \\    
  0 & 1 & 0 & 0 & 0 & 0 & - 1 \\    
  0 & 0 & 1 & 0 & 0 & 0 & - 1 \\    
  0 & 0 & 0 & 1 & 0 & 0 & - 1 \\    
  0 & 0 & 0 & 0 & 1 & 0 & - 1 \\    
  0 & 0 & 0 & 0 & 0 & 1 & - 1 \\    
  1 & 1 & 1 & 1 & 1 & 1 & 1 \\      
  \end{bmatrix}\begin{bmatrix}      
  \text{Mon} \\                     
  \text{Tue} \\                     
  \text{Wed} \\                     
  \text{Thu} \\                     
  \text{Fri} \\                     
  \text{Sat} \\                     
  \text{Sun} \\                     
  \end{bmatrix} = \begin{bmatrix}   
  Mon - Sun \\                      
  Tue - Sun \\                      
  Wed - Sun \\                      
  Thu - Sun \\                      
  Fri - Sun \\                      
  Sat - Sun \\                      
  \text{Length of period} \\      
  \end{bmatrix}$$                   
  
For the usual working day variables, two variables are used: one
contrast variable and the length of period

  $$\begin{bmatrix}                  
  1 & - \frac{5}{2} \\              
  1 & 1 \\                          
  \end{bmatrix}\begin{bmatrix}      
  \text{Week} \\                    
  \text{Weekend} \\                 
  \end{bmatrix} = \begin{bmatrix}   
  \text{Contrast week} \\          
  \text{Length of period} \\      
  \end{bmatrix}$$

The $\text{Length of period}$ variable is defined as a deviation from
the length of the month (in days) and the average month length, which is
equal to $30.4375.$ Instead, the leap-year variable can be used here
(see Regression sections in [RegARIMA](../reference-manual/modelling-spec-arima.html#regression) or 
[Tramo](../reference-manual/modelling-spec-tramo.html#regression))[^62].

Such transformations have several advantages. They suppress from the
contrast variables the mean and the seasonal effects, which are
concentrated in the last variable. So, they lead to fewer correlated
variables, which are more appropriate to be included in the regression
model. The sum of the effects of each day of the week estimated with the
trading (working) day contrast variables cancel out.


[^62]: GÓMEZ, V., and MARAVALL, A (2001b).

### Handling of specific holidays

check vs GUI (v3) and rjd3 modelling



Three types of holidays are implemented in JDemetra+:

-   Fixed days, corresponding to the fixed dates in the year (e.g. New
    Year, Christmas).

-   Easter related days, corresponding to the days that are defined in
    relation to Easter (e.g. Easter +/- n days; example: Ascension,
    Pentecost).

-   Fixed week days, corresponding to the fixed days in a given week of
    a given month (e.g. Labor Day celebrated in the USA on the first
    Monday of September).

From a conceptual point of view, specific holidays are handled in
exactly the same way as the other days. It should be decided, however,
to which group of days they belong. Usually they are handled as Sundays.
This convention is also used in JDemetra+. Therefore, except if the
holiday falls on a Sunday, the appearance of a holiday leads to
correction in two groups, i.e. in the group that contains the weekday,
in which holiday falls, and the group that contains the Sundays.

Country specific holidays have an impact on the mean and the seasonal
effects of calendar effects. Therefore, the appropriate corrections to
the number of particular days (which are usually the basis for the
definition of other calendar variables) should be applied, following the
kind of holidays. These corrections are applied to the period(s) that
may contain the holiday. The long term corrections in JDemetra+ don\'t
take into account the fact that some moving holidays could fall on the
same day (for instance the May Day and the Ascension). However, those
events are exceptional, and their impact on the final result is usually
not significant.

#### Fixed day

The probability that the holiday falls on a given day of the week is
1/7. Therefore, the probability to have 1 day more that is treated like
Sunday is 6/7. The effect on the means for the period that contains the
fixed day is presented in the table below (the correction on the calendar
effect has the opposite sign).

**The effect of the fixed holiday on the period, in which it occurred**


  | **Sundays** |  **Others days**  | **Contrast variables** |
  |-------------| ----------------- |------------------------|
  |+ 6/7        |- 1/7              | 1/7 - (+ 6/7)= -1      |

#### Easter related days

Easter related days always fall the same week day (denoted as Y in the table below: The effects of the Easter Sunday on the seasonal means). However, they can fall during different periods (months or
quarters). Suppose that, taking into account the distribution of the
dates for Easter and the fact that this holiday falls in one of two
periods, the probability that Easter falls during the period
$m$ is $p$, which implies that the probability that it falls in
the period $m + 1$ is $1 - p$. The effects of Easter on the seasonal
means are presented in the table below.

**The effects of the Easter Sunday on the seasonal means**


  |**Period**  | **Sundays**   **Days X**   **Others days**   **Contrast Y**      **Other contrasts**
  |------------| ------------- ------------ ----------------- ------------------- ---------------------
  |m            |\+ p          \- p         0                 \- 2p               \- p
  |m+1        |  \+ (1-p)      \- (1-p)     0                 \- 2$\times$(1-p)   \- (1-p)

The distribution of the dates for Easter may be approximated in
different ways. One of the solutions consists of using some well-known
algorithms for computing Easter on a very long period. JDemetra+
provides the Meeus/Jones/Butcher\'s and the Ron Mallen\'s algorithms
(they are identical till year 4100, but they slightly differ after that
date). Another approach consists in deriving a raw theoretical
distribution based on the definition of Easter. It is the solution used
for Easter related days. It is shortly explained below.

The date of Easter in the given year is the first Sunday after the full
moon (the Paschal Full Moon) following the northern hemisphere\'s vernal
equinox. The definition is influenced by the Christian tradition,
according to which the equinox is reckoned to be on 21 March[^63] and
the full moon is not necessarily the astronomically correct date.
However, when the full moon falls on Sunday, then Easter is delayed by
one week. With this definition, the date of Easter Sunday varies between
22 March and 25 April. Taking into account that an average lunar month
is $29.530595$ days the approximated distribution of Easter can be
derived. These calculations do not take into account the actual
ecclesiastical moon calendar.

For example, the probability that Easter Sunday falls on 25 March is
0.004838 and results from the facts that the probability that 25 March
falls on a Sunday is $1/7$ and the probability that the full moon is on
21 March, 22 March, 23 March or 24 March is $5/29.53059$. The
probability that Easter falls on 24 April is 0.01708 and results from
the fact that the probability that 24 April is Sunday is $1/7$ and takes
into account that 18 April is the last acceptable date for the full
moon. Therefore the probability that the full moon is on 16 April or 17
April is $1/29.53059$ and the probability that the full moon is on 18
April is $1.53059/29.53059$.

**The approximated distribution of Easter dates**


  |**Day**    |**Probability**|
  |---------- |-------------------------------|
  |22 March   |1/7 \* 1/29.53059|
  |23 March   |1/7 \* 2/29.53059|
  |24 March   |1/7 \* 3/29.53059|
  |25 March   |1/7 \* 4/29.53059|
  |26 March   |1/7 \* 5/29.53059|
  |27 March   |1/7 \* 6/29.53059|
  |28 March   |1/29.53059|
  |29 March   |1/29.53059|
  |...        |...|
  |18 April   |1/29.53059|
  |19 April   |1/7 \* (6 + 1.53059)/29.53059|
  |20 April   |1/7 \* (5 + 1.53059)/29.53059|
  |21 April   |1/7 \* (4 + 1.53059)/29.53059|
  |22 April   |1/7 \* (3 + 1.53059)/29.53059|
  |23 April   |1/7 \* (2 + 1.53059)/29.53059|
  |24 April   |1/7 \* (1 + 1.53059)/29.53059|
  |25 April   |1/7 \* 1.53059/29.53059|

#### Fixed week days

Fixed week days always fall on the same week day (denoted as Y in the table below) and in the same period. Their effect on the seasonal means is
presented in the table below.

**The effect of the fixed week holiday on the period, in which
it occurred**


  |**Sundays**  | **Day Y**   |**Others days**|
  |-------------| ----------- |-----------------|
  |+ 1          |- 1          | 0|

The impact of fixed week days on the regression variables is zero
because the effect itself is compensated by the correction for the mean
effect.

### Holidays with a validity period

When a holiday is valid only for a given time span, JDemetra+ applies
the long term mean corrections only on the corresponding period.
However, those corrections are computed in the same way as in the
general case.

It is important to note that using or not using mean corrections will
impact in the estimation of the RegARIMA and TRAMO models. Indeed, the
mean corrections do not disappear after differencing. The differences
between the SA series computed with or without mean corrections will no
longer be constant.


[^63]: In fact, astronomical observations show that the equinox occurs
    on 20 March in most years.



### Different Kinds of calendars

see link with GUI 

This scenario presents how to define different kinds of calendars. These
calendars can be applied to the specifications that take into account
country-specific holidays and can be used for detecting and estimating
the calendar effects.

The calendar effects are those parts of the movements in the time series
that are caused by different number of weekdays in calendar months (or
quarters). They arise as the number of occurrences of each day of the
week in a month (or a quarter) differs from year to year. These
differences cause regular effects in some series. In particular, such
variation is caused by a leap year effect because of an extra day
inserted into February every four years. As with seasonal effects, it is
desirable to estimate and remove calendar effects from the time series.

The calendar effects can be divided into a mean effect, a seasonal part
and a structural part. The mean effect is independent from the period
and therefore should be allocated to the trend-cycle. The seasonal part
arises from the properties of the calendar that recur each year. For one
thing, the number of working days of months with 31 calendar days is on
average larger than that of months with 30 calendar days. This effect is
part of the seasonal pattern captured by the seasonal component (with
the exception of leap year effects). The structural part of the calendar
effect remains to be determined by the calendar adjustment. For example,
the number of working days of the same month in different years varies
from year to year.

Both X-12-ARIMA/X-13ARIMA-SEATS and TRAMO/SEATS estimate calendar
effects by adding some regressors to the equation estimated in the
pre-processing part (RegARIMA or TRAMO, respectively). Regressors
mentioned above are generated from the default calendar or the user
defined calendar.

The calendars of JDemetra+ simply correspond to the usual trading days
contrast variables based on the Gregorian calendar, modified to take
into account some specific holidays. Those holidays are handled as
\"Sundays\" and the variables are properly adjusted to take into account
the long term mean effects.



### Tests for residual trading days

We consider below tests on the seasonally adjusted series ($sa_t$) or on
the irregular component ($irr_t$). When the reasoning applies on both
components, we will use $y_t$. The functions $stdev$ stands for
"standard deviation" and $rms$ for "root mean squares"

The tests are computed on the log-transformed components in the case of
multiplicative decomposition.

TD are the usual contrasts of trading days, 6 variables (no specific
calendar).

#### Non significant irregular

When $irr_t$ is not significant, we don't compute the test on it, to
avoid irrelevant results. We consider that $irr_t$ is significant if
$stdev( irr_t)>0.01$ (multiplicative case) or if
$stdev(irr_t)/rms(sa_t) >0.01$ (additive case).

#### F test

The test is the usual joint F-test on the TD coefficients, computed on
the following models:

##### Autoregressive model (AR modelling option)

We compute by OLS:

$$y_t=\mu + \alpha y_{t-1} + \beta TD_t + \epsilon_t $$

##### Difference model

We compute by OLS:

$$\Delta y_t - \overline{\Delta y_t}=\beta TD_t + \epsilon_t $$

So, the latter model is a restriction of the first one
($\alpha =1, \mu =μ=\overline{\Delta y_t}$)

The tests are the usual joint F-tests on $\beta \quad (H_0:\beta=0)$.

By default, we compute the tests on the 8 last years of the components,
so that they might highlight moving calendar effects.

Remark:

In Tramo, a similar test is computed on the residuals of the Arima
model. More exactly, the F-test is computed on
$e_t=\beta TD_t + \epsilon_t$, where $e_t$ are the one-step-ahead
forecast errors.

## Outliers and intervention variables

here just definition and purpose use in the regression in the reg-arima
regression part

Outliers[\^2] are abnormal values of a time series. In general, they
cannot be properly explained by the ARIMA model and its underlying
normality assumption. They tend to be associated with irregular special
events that produce a distortion in the series. The presence of such
values disturbs the modelling of time series with methods like
X-13ARIMA-SEATS and TRAMO-SEATS because of the linear procedures (e.g.
moving averages and regression analysis) implemented by them. The
presence of outliers has an adverse effect on the quality of seasonal
adjustment because outliers can lead to model misspecification, biased
parameter estimation, poor forecasts and inappropriate decomposition of
a series. Therefore, it is vital to identify and include them in the
modelling step of seasonal adjustment. The aim is to remove the effect
of outliers from a time series before its decomposition into its
components. Both X-13ARIMA-SEATS and TRAMO-SEATS include automatic
procedure for the treatment of outliers (detection and correction).
However, a priori information about an event that may have caused the
abnormal observations (the date of its occurrence and type of an effect)
can be included in the model by the user. This case study explains how
to do it.

In the automatic outlier detection and correction procedures, three
outlier types are considered by default:

-   additive outlier (AO) -- an abnormal value in an isolated point of
    the series;

-   transitory change (TC) -- a series of outliers with a temporarily
    decreasing effect on the level of the series;

-   level shift (LS) -- series of innovation outliers with a constant
    long-term effect on the level of the series, where for an innovation
    outlier is meant an anomalous value in the innovation series.

Seasonal outliers, which are defined as an abrupt increase or decrease
of the seasonal component for a specific month or quarter and are of
permanent nature can be automatically detected once the user has chosen
the appropriate option. The relevant instructions are given in this case
study.

The user may also introduce into the model a ramp effect, which is
described as a smooth, linear transition between two time points unlike
the abrupt change associated with level shifts. This case study explains
how to add ramp effects into a specification.

The formulas that describe outliers are given
[here](../theory/SA_lin.html).

1.  The picture below presents the number of registered unemployed
    persons in Poland. It is clear that in the beginning of 1999 a
    sudden, permanent shift in the trend level took place as a result of
    the poor state of the economy. At the end of 2008 a single peak can
    be observed, which can be interpreted as a reaction by entrepreneurs
    to the beginning of the economic crisis.

    ![Text](All_images/UG_SA_image13.jpg)

    **Registered unemployed persons in Poland**

(feed from estp ? + info on SO from ...)

## Pre-adjustment

### Overview

(edit) \#### Modeliing part to adapt\
The algorithms implemented in JDemetra+ enable a modelling of the
original time series with the RegARIMA model, including estimation of
the regression effects such as outliers and calendar effects. These
procedures can be used just for modelling and forecasting of the
original time series but also as a pre-treatment before performing a
seasonal adjustement of the series. Hence, this pre-treatement will
allow for a more reliable estimation of the time series components
performed by the seasonal adjustment procedures.

This section is divided into two parts: \*
[Specifications](../reference-manual/modelling-specifications.html),
which presents parameters of the modelling procedure. \*
[Output](../reference-manual/output-modelling.html), which details a
typical output produced by the modelling procedure.

The specifications and output of the modelling procedure are displayed
in the [*Workspace* window](../reference-manual/workspace.html).

![Text](All_images/A_Ref_d1.jpg)

**The *Workspace* window with the nodes for the modelling procedure
marked**

### Transformation choices

The log transformation of the original data is an option that is often
applied to achieve a stationary autocovariance function. The decision
concerning logging (or not) of a time series has a great impact on
seasonal adjustment outcomes[^seasonal-adjustment-7]. JDemetra+ offers
two options: logging (which means that the multiplicative decomposition
is used) or no transformation (the additive decomposition is used). The
selection of the transformation type can be done automatically, on the
basis of the outcome of a log-level test.

[^seasonal-adjustment-7]: *ESS Guidelines on Seasonal Adjustment*
    (2015).

The test used by TRAMO-SEATS is based on the maximum likelihood
estimation of the parameter $\lambda$ in the Box-Cox transformations
(which is a power transformations such that the transformed values of
time series \$\text{y }\$are a monotonic function of the observations,
i.e.:

$$y_{i}^{\alpha} = \left\{ \frac{\left( y_{i}^{\alpha} - 1 \right)}{\begin{matrix}
\lambda \\
\log{y_{i}^{\alpha},\lambda = 0\ } \\
\end{matrix}} \right.\ ,\ \lambda \neq 0$$

The automatic procedure first fits two Airline models (i.e. ARIMA
(0,1,1)(0,1,1)) on the time series: one in logs ($\lambda = 0$), the
other without logs ($\lambda = 1$). The test compares the sum of squares
of the model without logs with the sum of squares multiplied by the
square of the geometric mean from the model in logs. Logs are taken in
case the last function is the maximum[^seasonal-adjustment-8]. The
parameter *fct* controls the bias in the log/level pre-test (the
function is active when **Function** is set to *Auto*); *fct* \> 1
favours levels, *fct* \< 1 favours logs. [This test is used for
modelling with the TRAMO
model](../reference-manual/modelling-spec-tramo.html#transformation).

[^seasonal-adjustment-8]: Gómez, V., and Maravall, A. (1998).

![Text](All_images/UG_SA_image37.jpg)

**The *Transformation* options for the TRAMO-SEATS method**

The test used by X-13ARIMA-SEATS is based on the AICC information
criteria[^seasonal-adjustment-9]. To choose the transformation type,
X-13ARIMA-SEATS fits the RegARIMA model to the untransformed and the
transformed series. X-13ARIMA-SEATS will choose the log transformation
except when[^seasonal-adjustment-10]:

[^seasonal-adjustment-9]: Formula and further information available in
    Grudkowska, S. (2015).

[^seasonal-adjustment-10]: Description from *Guide to seasonal
    adjustment with X-12-ARIMA* (2007).

$$\text{AICC}_{\log} - \text{AICC}_{\text{no\ log}} < \Delta_{\text{AICC}}$$

where:

$\text{AICC}_{\text{no\ log}}$ is the value of AICC from fitting the
RegARIMA model to the untransformed series;

$\text{AICC}_{\log}$ is the value of AICC from fitting the RegARIMA
model to the transformed series;

$\Delta_{\text{AICC}}$ is the threshold value; $\Delta_{\text{AICC}}$\>
0 favours levels and $\Delta_{\text{AICC}}$ \< 0 favours logs.

The RegARIMA model used in the test is the one specified in the ARIMA
part of the specification. If model is specified then the (0,1,1)(0,1,1)
model is used. [This test is used for modelling with the RegARIMA
model](../reference-manual/modelling-spec-arima.html#transformation).

![Text](All_images/UG_SA_image38.jpg)

**The *Transformation* options for the X-13ARIMA-SEATS method**

According to the [ESS Guidelines on Seasonal Adjustment
(2015)](https://ec.europa.eu/eurostat/documents/3859598/6830795/KS-GQ-15-001-EN-N.pdf/d8f1e5f5-251b-4a69-93e3-079031b74bd3),
the automatic procedures should be applied for the transformation
choice, however in case of the most problematic series the manual
selection is recommended. The manual selection of the transformation is
usually made in the specifications used for a regular data production.

1.  To determine the transformation choice first [create and open a new
    specification](../reference-manual/sa-specifications.html#pre-defined-specifications).

2.  For *tramo* and *tramoseats* specifications from the
    *Transformation* section choose the *function* option and input the
    *fct* parameter's value. Click *OK* to confirm your choice.

    ![Text](All_images/UG_SA_image39.jpg)

    **Transformation's option for the *tramoseats* specification**

3.  For the *regarima* and *x13* specifications from the
    *Transformation* section choose the *function* option and *Aic
    difference* parameter's value. Click *OK* to confirm your choice.

    ![Text](All_images/UG_SA_image40.jpg)

    **Transformation's option for *X13* specification**

    It is also possible to change the transformation's options in the
    currently used specification (see [*Defining and modifying a
    specification*](../case-studies/detailedsa-spec.html) scenario, step
    4).

### Reg-Arima linarization

Old page mixing \* general info, parameters \* gui display \* pasted in
GUI chapter \* here + trim the gui part + add link to R (rjdemetra, go
version 3 first but reference help pages)

The *Model* node includes basic information about the outcome of the
model identification procedure and checking the goodness of fit. The
summary information about the final model is available directly from the
main *Model* node. The content of this panel depends on the
[settings](../reference-manual/modelling-specifications.html) applied to
the modelling procedure.

![Text](All_images/image22_RMSB.jpg)

**The *Model* node in the navigation tree**

The first part contains fundamental information about the model.

![Text](All_images/image23_RMSB.jpg)

**The *Summary* section of the *Model* node**

*Estimation span* informs about the first and the last observation used
for modelling. The notation of the estimation span varies according to
the frequencies (for example, the span $$2-1993 : 10-2006$$ represents a
monthly time series and the span $$II-1994 : I-2011$$ represents a
quarterly time series). The message *Series has been log-transformed* is
only displayed if a logarithmic transformation has been applied.

In the case of [the pre-defined
specifications](../reference-manual/modelling-specifications.html): TR0,
TR1, TR3, RG0, RG1 and RG3 no trading day effect is estimated. For TR2,
RG2c, TR4 and RG4c pre-defined specifications, working day effects and
the leap year effect are pre-tested and estimated if present. If the
working day effect is significant, the pre-processing part includes the
message *Working days effect (1 regressor)*. The message *Working days
effect (2 regressors)* means that the leap year effect has also been
estimated. For TR5 and RG5c the trading day effect and the leap year
effect are pre-tested. If the trading day effect has been detected,
either of the messages *Trading days effect (6 regressors)* or *Trading
days effect (7 regressors)* are displayed, depending whether the leap
year effect has been detected or not. If the Easter effect is
statistically significant, *Easter effect detected* is displayed.

In this section the total number of detected outliers is displayed. The
additional information on detected outliers, i.e. type, location and
coefficients' values, can be found in the *Arima model* subsection of
the *Model* node.

The *Final model* section informs about the outcome of the estimation
process. *Number of effective observations* is the number of
observations used to estimate the model, i.e. the number of observations
of the transformed series (regularly and/or seasonally differenced)
reduced by the *Number of estimated parameters*, which is the sum of
regular and seasonal parameters for both autoregressive and moving
average processes, mean effect, trading/working day effect, outliers,
regressors and one.

*Likelihood* is a maximized value of a
Likelihood[^seasonal-adjustment-11] function after the iterations
processed in Exact Maximum Likelihood Estimation, which is a method used
to estimate the model. This value is used by the model selection
criteria: *AIC*, *AICC*, *BIC (corrected by length)* and
*Hannan-Quinn*[^seasonal-adjustment-12]. *Standard error of the
regression (ML estimate)* is the standard error of the regression from
Maximum Likelihood Estimation[^seasonal-adjustment-13].

[^seasonal-adjustment-11]: The likelihood function is the joint
    probability (density) function of observable random variables. It is
    viewed as the function of the parameters given the realized random
    variables. Therefore, this function measures the probability of
    observing the particular set of dependent variable values that occur
    in the sample.

[^seasonal-adjustment-12]: [AIC, AICC, BIC and Hannan-Quinn criteria are
    used by X-13ARIMA-SEATS while BIC (TRAMO
    definition)](../theory/SA_lin.html#model-selection-criteria) by
    TRAMO/SEATS. These criteria are used in seasonal adjustment
    procedures for the selection of the optimum ARIMA model. The model
    with the smaller value of the model selection criteria is preferred.

[^seasonal-adjustment-13]: Maximum Likelihood Estimation is a
    statistical method for estimating the coefficients of a model. This
    method determines the parameters that maximize the probability
    (likelihood) of the sample data.

The *scores at the solution* section presents the gradient of the
loglikelihood. The different items of the scores are related to the
different parameters of the ARIMA model. The output indicates to which
extent the optimization procedure reached the maximum. At the maximum of
the likelihood, it should be 0. However, it is never exactly the case,
due to numerical approximations. Usually, the scores can be improved by
using a higher precision (smaller tolerance). This precision is
controlled by the **Tolerance** parameter in the **Estimate** section of
the *Specifications* window (see how to use this parameter [for
Tramo](../reference-manual/modelling-spec-tramo.html#estimate) and [for
Arima](../reference-manual/modelling-spec-arima.html#estimate)).

An example of the output is presented in the chart below.

![Text](All_images/image24_RMSB.jpg)

**The content of the *Final model* section**

Next, the estimated values of model parameters (*Coefficients*),
t-statistics (*T-Stat*) and corresponding p-values (*P*$$\|T\|\>t$$) are
displayed. JDemetra+ uses the following notation:

-   *Phi(p)* -- the \\(p\^{\text{th}}\\) term in the non-seasonal
    autoregressive polynomial;

-   *Theta(q)* -- the $q^{\text{th}}$ term in the non-seasonal moving
    average polynomial;

-   *BPhi(P)* -- the $P^{\text{th}}$ term in the seasonal autoregressive
    polynomial;

-   *BTheta(Q)* -- the $Q^{\text{th}}$ term in the seasonal moving
    average polynomial.

In the example below, the ARIMA model (0,1,1)(0,1,1) was chosen, which
means that one regular and one seasonal moving average parameter were
identified and estimated. The p-values indicate that *BTheta(1)*
parameter is significant in contrast to the *Theta(1)*, which is not
significant [^seasonal-adjustment-14].

[^seasonal-adjustment-14]: The variable is called statistically
    significant if it is so extreme that such a result would be expected
    to arise simply by chance only in rare circumstances (with the
    probability equal to p-value). Generally, the regressor is thought
    to be significant if p-value is lower than 5%.

![Text](All_images/image25_RMSB.jpg)

**The estimation's results of the ARIMA model**

For the [fixed ARIMA
parameters](../reference-manual/modelling-spec-tramo.html#tramo-specification--options-for-manual-identification-of-the-arima-model),
JDemetra+ shows only the values of the parameters. Figure below presents
the output from the manually chosen ARIMA model (2,0,0)(0,1,1) with a
fixed parameter *BTheta(1).* For the fixed parameter the *T-Stat* and
*(P\|T\|\>t)* are not displayed as no estimation is done for this
parameter.

![Text](All_images/image26_RMSB.jpg)

**The results of the estimation of the ARIMA model with a fixed
coefficient**

If the ARIMA model contains a constant term (detected automatically or
introduced by the user), the estimated value and related statistics are
reported.

![Text](All_images/image27_RMSB.jpg)

**The results of the estimation of a mean effect**

JDemetra+ presents estimated values of the coefficients of one or six
regressors depending on the type of a calendar effect specification. For
a working days effect one regressor is estimated.

![Text](All_images/image28_RMSB.jpg)

**The results of the estimation of a working day effect**

When a trading days effect is estimated the *Joint F-test* value is
reported under the table that presents estimated values. When the result
of *Joint F-test* indicates that the trading day variables are jointly
not significant the test result is displayed in red.

![Text](All_images/image29_RMSB.jpg)

**The results of the estimation of a trading day effect: the case of
jointly not significant variables**

In the example below the RSA5c specification has been used and a trading
day effect has been detected. In spite of the fact that some trading day
regressors are not significant at the 5% significance level, the outcome
of the joint F-test indicates that the trading day regressors are
jointly significant (the F-test statistic is lower than 5%).

![Text](All_images/image30_RMSB.jpg)

**The results of the estimation of a trading day effect: the case of
jointly significant variables**

If a leap year regressor has been used in the model specification, the
value of the estimated leap year coefficient is also reported with the
corresponding t-statistics and p-value. As the p-value presented on the
picture below is greater than 0.05, it indicates that the leap year
effect is not significant.

![Text](All_images/image31_RMSB.jpg)

**The results of the estimation of a leap year effect**

When the option
[*UserDefined*](../reference-manual/modelling-spec-tramo.html#regression)
is used, JDemetra+ displays the *User-defined calendar variables*
section with variables and corresponding estimation results (the values
of the parameters, corresponding t-statistics and p-values). The outcome
of the joint F-test is displayed when more than one user-defined
calendar variable is used.

![Text](All_images/image32_RMSB.jpg)

**The results of the estimation of user-defined calendar variables**

When the Easter effect is estimated, the following table is displayed in
the output. In the case presented below Easter has a negative,
significant effect on the time series.

![Text](All_images/image33_RMSB.jpg)

**The results of the estimation of the Easter effect**

JDemetra+ also presents the results of an outlier detection procedure.
The table includes the type of outlier, its date, the value of the
coefficient and corresponding t-statistics and p-values.

![Text](All_images/image34_RMSB.jpg)

**The results of the outlier identification procedure**

In all pre-defined specifications, except for TR0 and RG0, only additive
outliers, temporary changes and level shifts are considered in the
automatic outlier identification procedure. When seasonal outliers are
also enabled, they appear in the same table as other outliers.

![Text](All_images/image35_RMSB.jpg)

**The results of the outlier identification procedure that enables
seasonal outliers**

Results for [pre-specified
outliers](../reference-manual/modelling-spec-tramo.html#regression) are
displayed in a separate table.

![Text](All_images/image36_RMSB.jpg)

**The results of the estimation of the pre-specified outliers**

Regression variables, like ramps and intervention variables, are not
identified automatically. They need to be defined by the user. The
results of an estimation of ramps that are pre-defined types of
regression variables are displayed in a separate table. All information
concerning ramps, including spans, estimated coefficients and related
statistics, is shown in a separate table.

![Text](All_images/image37_RMSB.jpg)

**The results of the estimation of the ramp effect**

All other intervention variables with corresponding statistics are shown
under the *Intervention variable(s)* table.

![Text](All_images/image38_RMSB.jpg)

**The results of the estimation of the intervention variable**

User-defined variables are marked as *Vars-1.x_1, Vars-1.x_2*, ...,
*Vars-1.x_n* and displayed in the separate tables.

![Text](All_images/image39_RMSB.jpg)

**The results of the estimation of the user-defined variables**

JDemetra+ also reports a list of missing observations, if any. JDemetra+
applies the AO approach to the estimation of the missing
observations[^seasonal-adjustment-15].

[^seasonal-adjustment-15]: Missing observations are treated as additive
    outliers and interpolated accordingly.

![Text](All_images/image40_RMSB.jpg)

**The results of the estimation of the missing observations**

Detailed results are divided into several sections and are investigated
in the following sections: -
[Forecasts](../reference-manual/forecasts.html) -
[Regressors](../reference-manual/regressors.html) -
[ARIMA](../reference-manual/arima.html) - [Pre-adjustment
series](../reference-manual/pre-adjustment-series.html) -
[Residuals](../reference-manual/residuals.html) -
[Likelihood](../reference-manual/likelihood.html)


### Foreasts Out-of sample


One of the important steps in of the a validation of the model is an
analysis of the out-of-sample forecasts[^1]. Using the identified TRAMO
model JDemetra+ produces the point forecasts, the forecast standard
errors, and a prediction interval. The prediction interval on the
transformed scale is denoted 
as$\ point\ forecast \pm K \times forecast\ standard\ error$, where $K$ is
the standard error multiplier taken from a table of the normal
distribution, corresponding to the specified coverage probability.
JDemetra+ displays 95% prediction interval, which corresponds to
$K = 1.96.$ The forecasting procedure assumes that no outliers appear in
the forecasting period.

The graph presenting time series together with the values forecasted for
the next year and corresponding prediction interval can be found
directly in the *Forecast* node.


![Text](All_images/image41_RMSB.png)



**The *Forecast* panel with visual presentation of the estimated forecasts**

The local menu offers the copy and export options, including sending the
graph to the printer and save the graph as clipboard or a file in the
PNG format. The *Show precision gradient* option highlights the
precision of the estimation using different shades of orange. As a rule,
the precision decreases in time, which is depicted by gradually more
intense orange. The *Copy all series* option enables the user to export
time series together with the forecasts and the prediction intervals to
the another application.


![Text](All_images/image42_RMSB.jpg)



**The *Forecast* panel with visual presentation of the precision of the estimated forecasts**

The *Forecast* node is divided into two subsections: *Table* and
*Out-of-sample test*. The *Table* section presents the forecasts as a
table.


![Text](All_images/image43_RMSB.png)



**A local menu for the content of the *Table* panel**

A standard local menu, which is available for this table, includes:

-   **Select all** -- copies series and allows it to be pasted to the
    another application e.g. into Excel.

-   **Transpose** -- changes the orientation of the table from
    horizontal to vertical.

-   **Reverse chronology** -- displays the series from last to first
    observation.

-   **Single time series** -- when it is marked observations are divided
    by calendar's periods. Otherwise, data are presented as a standard
    time series.

-   **Edit format** -- allows changing the format used for displaying
    dates and values.

-   **Use color scheme** -- allows series values to be displayed in
    colour.

-   **Color scheme** -- allows for a choice of colour scheme from a
    pre-specified list.

-   **Show bars** -- presents values in a table as horizontal bars.

-   **Show crosshair** -- highlights an active cell.

-   **Zoom** -- an option for modifying the chart size.

**Paste** and **Clear** are disabled as they are not relevant for this
view.

When a time series is marked, a local menu offers the following options:

-   **Open** – opens selected time series in the new window that contain
    *Chart* and *Grid* panels.

-   **Open with** – opens the time series in a separate window according
    to the user choice (*Chart* *& grid* or *Simple chart*). The *All ts
    views* option is not currently available.

-   **Save** – saves the marked series in a spreadsheet file or in a
    text file.

-   **Rename** – enables the user to change the time series name.

-   **Copy** – copies series and allows it to be pasted to another
    application e.g. into Excel.

-   **Paste** – pastes the time series previously marked.

-   **Select all** – selects all time series presented in the graph.

-   **Transpose** – changes the orientation of the table from horizontal
    to vertical.

-   **Reverse chronology** – displays the series from the last to the
    first observation.

-   **Single time series** – removes from the table all time series
    apart from the selected one.
	
-   **Clear** – removes all time series from the chart.

-   **Edit format** – enables the user to change a data format.

-   **Use color scheme** – allows the series to be displayed in colour.

-   **Color scheme** – allows the colour scheme used in the graph to be
    changed.

-   **Show bars** – presents values in a table as horizontal bars.

-   **Show crosshair** – highlights an active cell.

-   **Zoom** – option for modifying the chart size.


![Text](All_images/image44_RMSB.png)



**A local menu for a selected series in the *Table* panel**

The *Out-of-sample test* section presents two tests. These tests check
the performance of the out-of-sample forecasts, by comparison of
forecast limits and the data. To perform this exercise, first the
historical data sample is divided into a fit period (whole estimation
span excluding last 18 observations) and a test period (last 18
observations). For the fit period the model is automatically selected
and its parameters are fixed. The model is used to produce a
one-period-ahead forecast (i.e. the forecast for $\mathbf{n + 1}$, where
$\mathbf{n}$ is the length of the time series). This estimation is
performed 18 times for the gradually extended time series.

The first test compares forecast errors with the standard error of the
in-sample residuals. The goodness of fit is assessed by checking if the
mean of the forecast errors can be assumed zero.

The second test compares mean squared of forecast error with mean
squared of in-sample residuals. The result of the test is accepted when
these two indicators are close to each other. The lack of consistency is
a clear evidence of a model inadequacy.


![Text](All_images/image45_RMSB.jpg)



**An example of out-of-sample test results**




[^1]: PEÑA, D. (2001).


## Decomposition

### X-11 moving average based decomposition

A complete documentation of the X-11 method is available in LADIRAY, D.,
and QUENNEVILLE, B. (2001). The X-11 program is the result of a long
tradition of non-parametric smoothing based on moving averages, which
are weighted averages of a moving span of a time series (see hereafter).
Moving averages have two important drawbacks:

-   They are not resistant and might be deeply impacted by outliers;

-   The smoothing of the ends of the series cannot be done except with
    asymmetric moving averages which introduce phase-shifts and delays
    in the detection of turning points.

These drawbacks adversely affect the X-11 output and stimulate the
development of this method. To overcome these flaws first the series are
modelled with a RegARIMA model that calculates forecasts and estimates
the regression effects. Therefore, the seasonal adjustment process is
divided into two parts.

-   In a first step, the RegARIMA model is used to clean the series from
    \> non-linearities, mainly outliers and calendar effects. A global
    \> ARIMA model is adjusted to the series in order to compute the \>
    forecasts.

-   In a second step, an enhanced version of the X-11 algorithm is used
    \> to compute the trend, the seasonal component and the irregular \>
    component.

![Text](All_images/UG_A_image13.png)

**The flow diagram for seasonal adjustment with X-13ARIMA-SEATS using
the X-11 algorithm.**

#### Moving averages

The moving average of coefficient $\theta_{i}$ is defined as:

$$M\left( X_{t} \right) = \sum_{k = - p}^{+ f}\theta_{k}X_{t + k}$$
$$1$$<!---\[7.61\]-->

The value at time $t$ of the series is therefore replaced by a weighted
average of $p$ "past" values of the series, the current value, and $f$
"future" values of the series. The quantity $p + f + 1\$is called the
moving average order. When $p$ is equal to $f$, that is, when the number
of points in the past is the same as the number of points in the future,
the moving average is said to be centred. If, in addition,
$\theta_{- k} = \theta_{k}$ for any $k$, the moving average $M$ is said
to be symmetric. One of the simplest moving averages is the symmetric
moving average of order $P = 2p + 1$ where all the weights are equal
to$\ \frac{1}{P}$.

This moving average formula works well for all time series observations,
except for the first $p$ values and last $f$ values. Generally, with a
moving average of order $p + f + 1\$calculated for instant $t$nwith
points $p$ in the past and points $f$ in the future, it will be
impossible to smooth out the first $p$ values and the last $f$ values of
the series because of lack of input to the moving average formula.

In the X-11 method, symmetric moving averages play an important role as
they do not introduce any phase-shift in the smoothed series. But, to
avoid losing information at the series ends, they are either
supplemented by *ad hoc* asymmetric moving averages or applied on the
series extended by forecasts.

For the estimation of the seasonal component, X-13ARIMA-SEATS uses
$P \times Q$ composite moving averages, obtained by composing a simple
moving average of order $P$, which coefficients are all equal to
$\frac{1}{P}$, and a simple moving average of order $Q$, which
coefficients are all equal to $\frac{1}{Q}$.

The composite moving averages are widely used by the X-11 method. For an
initial estimation of trend X-11 method uses a $2 \times 4$ moving
average in case of a quarterly time series while for a monthly time
series a $2 \times 12\$moving average is applied. The $2 \times 4$
moving average is an average of order 5 with coefficients
$$\frac{1}{8}\left\{1, 2, 2, 2, 1\right\}$$. It eliminates frequency
$\frac{\pi}{2}$ corresponding to period 4 and therefore it is suitable
for seasonal adjustment of the quarterly series with a constant
seasonality. The $2 \times 12$ moving average, with coefficients
$$\frac{1}{24}\left\{1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1\right\} $$that
retains linear trends, eliminates order-$12$ constant seasonality and
minimises the variance of the irregular component. The $2 \times 4$ and
$2 \times 12$ moving averages are also used in the X-11 method to
normalise the seasonal factors. The composite moving averages are also
used to extract the seasonal component. These, which are used in the
purely automatic run of the X-11 method (without any intervention from
the user) are $3 \times 3$, $3 \times 5$ and $3 \times 9$.

In the estimation of the trend also Henderson moving averages are used.
These filters have been chosen for their smoothing properties. The
coefficients of a Henderson moving average of order $2p + 1$ may be
calculated using the formula:

$\theta_{i} = \frac{315\left\lbrack \left( n - 1 \right)^{2} - i^{2} \right\rbrack\left\lbrack n^{2} - i^{2} \right\rbrack\left\lbrack \left( n + 1 \right)^{2} - i^{2} \right\rbrack\left\lbrack {3n}^{2} - 16 - 11i^{2} \right\rbrack}{8n\left( n^{2} - 1 \right)\left( {4n}^{2} - 1 \right)\left( {4n}^{2} - 9 \right)\left( 4n^{2} - 25 \right)}$,
$$2$$<!---\[7.62\]-->

where: $n = p + 2$$n = p + 2$.

#### The basic algorithm of the X-11 method

The X-11 method is based on an iterative principle of estimation of the
different components using appropriate moving averages at each step of
the algorithm. The successive results are saved in tables. The list of
the X-11 tables displayed in JDemetra+ is included at the end of this
section.

The basic algorithm of the X-11 method will be presented for a monthly
time series $X_{t}$ that is assumed to be decomposable into trend,
seasonality and irregular component according to an additive model
$X_{t} = TC_{t} + S_{t} + I_{t}$.

A simple seasonal adjustment algorithm can be thought of in eight steps.
The steps presented below are designed for the monthly time series. In
the algorithm that is run for the quarterly time series the $2 \times 4$
moving average instead of the $2 \times 12$ moving average is used.

***Step 1: Estimation of Trend by*** $\mathbf{2 \times 12}$ ***moving
average:***

$TC_{t}^{(1)} = M_{2 \times 12}(X_{t})$ $$3$$<!---\[7.63\]-->

***Step 2: Estimation of the Seasonal-Irregular component:***

$\left( S_{t} + I_{t} \right)^{(1)} = X_{t} - \text{TC}_{t}^{(1)}$
$$4$$<!---\[7.64\]-->

***Step 3: Estimation of the Seasonal component by***
$\mathbf{3 \times 3}$ ***moving average over each month:***

$S_{t}^{(1)} - M_{3 \times 3}\left\lbrack \left( S_{t} + I_{t} \right)^{(1)} \right\rbrack$
$$5$$<!---\[7.65\]-->

The moving average used here is a $3 \times 3$ moving average over $5$
terms, with coefficients $$\frac{1}{9} \left\{1, 2, 3, 2, 1 \right\}$$.
The seasonal component is then centred using a $2 \times 12$ moving
average.

$$
   \widetilde{S}_{t}^{(1)} = S_{t}^{(1)} - M_{2 \times 12}\left( S_{t}^{(1)} \right)
  $$ $$6$$<!---\[7.66\]-->

***Step 4: Estimation of the seasonally adjusted series:***

$$
  SA_{t}^{\left( 1 \right)} = \left( \text{TC}_{t} + I_{t} \right)^{(1)} = X_{t} - {\widetilde{S}}_{t}^{(1)}
  $$ $$7$$<!---\[7.67\]-->

This first estimation of the seasonally adjusted series must, by
construction, contain less seasonality. The X-11 method again executes
the algorithm presented above, changing the moving averages to take this
property into account.

***Step 5: Estimation of Trend by 13-term Henderson moving average:***

$$
  TC_{t}^{(2)} = H_{13}\left( \text{SA}_{t}^{\left( 1 \right)} \right)
  $$ $$8$$<!---\[7.68\]-->

Henderson moving averages, while they do not have special properties in
terms of eliminating seasonality (limited or none at this stage), have a
very good smoothing power and retain a local polynomial trend of degree
$2$ and preserve a local polynomial trend of degree $3$.

***Step 6: Estimation of the Seasonal-Irregular component:***

$$
  \left( S_{t} + I_{t} \right)^{(2)} = X_{t} - \text{TC}_{t}^{(2)}
  $$ $$9$$<!---\[7.69\]-->

***Step 7: Estimation of the Seasonal component by***
$\mathbf{3 \times 5}$ ***moving average over each month:***

$$S_{t}^{(2)} - M_{3 \times 3}\left\lbrack \left( S_{t} + I_{t} \right)^{(2)} \right\rbrack$$
$$10$$<!---\[7.70\]-->

The moving average used here is a $3 \times 5$ moving average over $7$
terms, of coefficients
$$\frac{1}{15} \left\{ 1,\ 2,\ 3,\ 3,\ 3,\ 2,\ 1 \right\}$$ and retains
linear trends. The coefficients are then normalised such that their sum
over the whole $12$-month period is approximately cancelled out:

$${ \widetilde{S}}_{t}^{(2)} = S_{t}^{(2)} - M_{2 \times 12}\left( S_{t}^{(2)} \right)$$
$$11$$<!---\[7.71\]-->

***Step 8: Estimation of the seasonally adjusted series:***

$$SA_{t}^{\left( 2 \right)} = \left(TC_{t} + I_{t} \right)^{(2)} = X_{t} - {\widetilde{S}}_{t}^{(2)}$$
$$12$$<!---\[7.72\]-->

The whole difficulty lies, then, in the choice of the moving averages
used for the estimation of the trend in steps $1$ and $5$ on the one
hand, and for the estimation of the seasonal component in steps $3$ and
$5$. The course of the algorithm in the form that is implemented in
JDemetra+ is presented in the figure below. The adjustment for trading
day effects, which is present in the original X-11 program, is omitted
here, as since calendar correction is performed by the RegARIMA model,
JDemetra+ does not perform further adjustment for these effects in the
decomposition step.

**A workflow diagram for the X-11 algorithm based upon training material
from the Deutsche Bundesbank**

##### **The iterative principle of X-11**

To evaluate the different components of a series, while taking into
account the possible presence of extreme observations, X-11 will proceed
iteratively: estimation of components, search for disruptive effects in
the irregular component, estimation of components over a corrected
series, search for disruptive effects in the irregular component, and so
on.

The Census X-11 program presents four processing stages (A, B, C, and
D), plus 3 stages, E, F, and G, that propose statistics and charts and
are not part of the decomposition per se. In stages B, C and D the basic
algorithm is used as is indicated in the figure below.

**A workflow diagram for the X-11 algorithm implemented in JDemetra+.
Source: Based upon training material from the Deutsche Bundesbank**

-   **Part A: Pre-adjustments**

This part, which is not obligatory, corresponds in X-13ARIMA-SEATS to
the first cleaning of the series done using the RegARIMA facilities:
detection and estimation of outliers and calendar effects (trading day
and Easter), forecasts and backcasts[\^61] of the series. Based on these
results, the program calculates prior adjustment factors that are
applied to the raw series. The series thus corrected, Table B1 of the
printouts, then proceeds to part B.

-   **Part B: First automatic correction of the series**

This stage consists of a first estimation and down-weighting of the
extreme observations and, if requested, a first estimation of the
calendar effects. This stage is performed by applying the basic
algorithm detailed earlier. These operations lead to Table B20,
adjustment values for extreme observations, used to correct the
unadjusted series and result in the series from Table C1.

-   **Part C: Second automatic correction of the series**

Applying the basic algorithm once again, this part leads to a more
precise estimation of replacement values of the extreme observations
(Table C20). The series, finally "cleaned up", is shown in Table D1 of
the printouts.

-   **Part D: Seasonal adjustment**

This part, at which our basic algorithm is applied for the last time, is
that of the seasonal adjustment, as it leads to final estimates:

-   of the seasonal component (Table D10);

-   of the seasonally adjusted series (Table D11);

-   of the trend component (Table D12);

-   of the irregular component (Table D13).

<!-- -->

-   **Part E: Components modified for large extreme values**

Parts E includes:

-   Components modified for large extreme values;

-   Comparison the annual totals of the raw time series and seasonally
    adjusted time series;

-   Changes in the final seasonally adjusted series;

-   Changes in the final trend;

-   Robust estimation of the final seasonally adjusted series.

The results from part E are used in part F to calculate the quality
measures.

-   **Part F: Seasonal adjustment quality measures**

Part F contains statistics for judging the quality of the seasonal
adjustment. JDemetra+ presents selected output for part F, i.e.:

-   M and Q statistics;

-   Tables.

<!-- -->

-   **Part G: Graphics**

Part G presents spectra estimated for:

-   Raw time series adjusted a priori (Table B1);

-   Seasonally adjusted time series modified for large extreme values
    (Table E2);

-   Final irregular component adjusted for large extreme values (Table
    E3).

Originally, graphics were displayed in character mode. In JDemetra+,
these graphics are replaced favourably by the usual graphics software.

**The Henderson moving average and the trend estimation**

In iteration B (Table B7), iteration C (Table C7) and iteration D (Table
D7 and Table D12) the trend component is extracted from an estimate of
the seasonally adjusted series using Henderson moving averages. The
length of the Henderson filter is chosen automatically by
X-13ARIMA-SEATS in a two-step procedure.

It is possible to specify the length of the Henderson moving average to
be used. X-13ARIMA-SEATS provides an automatic choice between a 9-term,
a 13-term or a 23-term moving average. The automatic choice of the order
of the moving average is based on the value of an indicator called
$\frac{\overline{I}}{\overline{C}}\$ratio which compares the magnitude
of period-on-period movements in the irregular component with those in
the trend. The larger the ratio, the higher the order of the moving
average selected. Moreover, X-13ARIMA-SEATS allows the user to choose
manually any odd‑numbered Henderson moving average. The procedure used
in each part is very similar; the only differences are the number of
options available and the treatment of the observations in the both ends
of the series. The procedure below is applied for a monthly time series.

In order to calculate $\frac{\overline{I}}{\overline{C}}\$ ratio a first
decomposition of the SA series (seasonally adjusted) is computed using a
13-term Henderson moving average.

For both the trend ($C$) and irregular ($I$) components, the average of
the absolute values for monthly growth rates (multiplicative model) or
for monthly growth (additive model) are computed. They are denoted as
$\overline{C}\$and $\overline{I}$, receptively, where
$\overline{C} = \frac{1}{n - 1}\sum_{t = 2}^{n}\left| C_{t} - C_{t - 1} \right|$
and
$\overline{I} = \frac{1}{n - 1}\sum_{t = 2}^{n}\left| I_{t} - I_{t - 1} \right|$.

Then the value of $\frac{\overline{I}}{\overline{C}}\$ ratio is checked
and in iteration B:

-   If the ratio is smaller than 1, a 9-term Henderson moving average is
    selected;

-   Otherwise, a 13-term Henderson moving average is selected.

Then the trend is computed by applying the selected Henderson filter to
the seasonally adjusted series from Table B6. The observations at the
beginning and at the end of the time series that cannot be computed by
means of symmetric Henderson filters are estimated by ad hoc asymmetric
moving averages.

In iterations C and D:

-   If the ratio is smaller than 1, a 9-term Henderson moving average is
    selected;

-   If the ratio is greater than 3.5, a 23-term Henderson moving average
    is selected.

-   Otherwise, a 13-term Henderson moving average is selected.

The trend is computed by applying selected Henderson filter to the
seasonally adjusted series from Table C6, Table D7 or Table D12,
accordingly. At the both ends of the series, where a central Henderson
filter cannot be applied, the asymmetric ends weights for the 7 term
Henderson filter are used.

##### **Choosing the composite moving averages when estimating the seasonal component**

In iteration D, Table D10 shows an estimate of the seasonal factors
implemented on the basis of the modified SI (Seasonal -- Irregular)
factors estimated in Tables D4 and D9bis. This component will have to be
smoothed to estimate the seasonal component; depending on the importance
of the irregular in the SI component, we will have to use moving
averages of varying length as in the estimate of the Trend/Cycle where
the $\frac{\overline{I}}{\overline{C}}\$ ratio was used to select the
length of the Henderson moving average. The estimation includes several
steps.

***Step 1: Estimating the irregular and seasonal components***

An estimate of the seasonal component is obtained by smoothing, month by
month and therefore column by column, Table D9bis using a simple 7-term
moving average, i.e. of coefficients
$$\frac{1}{7} \left\{1,\ 1,\ 1,\ 1,\ 1,\ 1,\ 1\right\}$$. In order not
to lose three points at the beginning and end of each column, all
columns are completed as follows. Let us assume that the column that
corresponds to the month is composed of $N$ values $$
\left\{ x_{1},\ x_{2},\ x_{3},\ \ldots x_{N - 1},\ x_{N} \right\}.
$$ It will be transformed into a series
$$\left\{ {x_{- 2},x_{- 1}{,x}_{0},x}_{1},\ x_{2},\ x_{3},\ \ldots x_{N - 1},\ x_{N},x_{N + 1},\ x_{N + 1},\ x_{N + 2},\ x_{N + 3} \right\}\$$
with $$x_{- 2} = x_{- 1} = x_{0} = \frac{x_{1} + x_{2} + x_{3}}{3}$$ and
$$x_{N + 1} = x_{N + 2} = x_{N + 3} = \frac{x_{N} + x_{N - 1} + x_{N - 2}}{3}$$.
We then have the required estimates: $S = M_{7}(D9bis)$ and
$I = D9bis - S$.

***Step 2: Calculating the Moving Seasonality Ratios***

For each $i^{\text{th}}$ month the mean annual changes for each
component is obtained by calculating
$${\overline{S}}_{i} = \frac{1}{N_{i} - 1}\sum_{t = 2}^{N_{i}}\left| S_{i,t} - S_{i,t - 1} \right|$$
and
$${\overline{I}}_{i} = \frac{1}{N_{i} - 1}\sum_{t = 2}^{N_{i}}\left| I_{i,t} - I_{i,t - 1} \right|$$,
where $N_{i}$ refers to the number of months $\text{i}$in the data, and
the moving seasonality ratio of month $i$:
$$MSR_{i} = \frac{\ {\overline{I}}_{i}}{ {\overline{S}}_{i}}$$. These
ratios are presented in *Details* of the *Quality Measures* node under
the *Decomposition (X11)* section. These ratios are used to compare the
year-on-year changes in the irregular component with those in the
seasonal component. The idea is to obtain, for each month, an indicator
capable of selecting the appropriate moving average for the removal of
any noise and providing a good estimate of the seasonal factor. The
higher the ratio, the more erratic the series, and the greater the order
of the moving average should be used. As for the rest, by default the
program selects the same moving average for each month, but the user can
select different moving averages for each month.

***Step 3: Calculating the overall Moving Seasonality Ratio***

The overall Moving Seasonality Ratio is calculated as follows:

$$\text{MSR}_{i} = \frac{\sum_{i}^{}{N_{i}\ }\ {\overline{I}}_{i}}{\sum_{i}^{}N_{i}{\overline{S}}_{i}}$$
$$13$$<!---\[7.73\]-->

***Step 4: Selecting a moving average and estimating the seasonal
component***

Depending on the value of the ratio, the program automatically selects a
moving average that is applied, column by column (i.e. month by month)
to the Seasonal/Irregular component in Table D8 modified, for extreme
values, using values in Table D9.

The default selection procedure of a moving average is based on the
Moving Seasonality Ratio in the following way:

-   If this ratio occurs within zone A (MSR \< 2.5), a $3 \times 3$
    moving average is used; if it occurs within zone C (3.5 \< MSR \<
    5.5), a $3 \times 5$ moving average is selected; if it occurs within
    zone E (MSR \> 6.5), a $3 \times 9$ moving average is used;

-   If the MSR occurs within zone B or D, one year of observations is
    removed from the end of the series, and the MSR is recalculated. \>
    If the ratio again occurs within zones B or D, we start over \>
    again, removing a maximum of five years of observations. If this \>
    does not work, i.e. if we are again within zones B or D, a \>
    $3 \times 5$ moving average is selected.

The chosen symmetric moving average corresponds, as the case may be 5
($3 \times 3$), 7 $(3 \times 5)\$or 11 ($3 \times 9$$3 \times 9)$ terms,
and therefore does not provide an estimate for the values of seasonal
factors in the first 2 (or 3 or 5) and the last 2 (or 3 or 5) years.
These are then calculated using associated asymmetric moving averages.

**Moving average selection procedure, source: DAGUM, E. B.(1999)**

##### **Identification and replacement of extreme values**

X-13ARIMA-SEATS detects and removes outliers in the RegARIMA part.
However, if there is a seasonal heteroscedasticity in a time series i.e.
the variance of the irregular component is different in different
calendar months. Examples for this effect could be the weather and
snow-dependent output of the construction sector in Germany during
winter, or changes in Christmas allowances in Germany and resulting from
this a transformation in retail trade turnover before Christmas. The
ARIMA model is not on its own able to cope with this characteristic. The
practical consequence is given by the detection of additional extreme
values by X-11. This may not be appropriate if the seasonal
heteroscedasticity is produced by political interventions or other
influences. The ARIMA models assume a constant variance and are
therefore not by themselves able to cope with this problem. Choosing
longer (in the case of diverging weather conditions in the winter time
for the construction sector) or shorter filters (in the case of a
changing pattern of retail trade turnover in the Christmas time) may be
reasonable in such cases. It may even be sensible to take into account
the possibility of period-specific (e.g. month-specific) standard
deviations, which can be done by changing the default settings of the
**calendarsigma** parameter (see
[Specifications-X13](../reference-manual/sa-spec-X13.html) section). The
value of the **calendarsigma** parameter will have an impact on the
method of calculation of the moving standard deviation in the procedure
for extreme values detection presented below.

***Step 1: Estimating the seasonal component***

The seasonal component is estimated by smoothing the SI component
separately for each period using a $3 \times 3$ moving average, i.e.:

$$
  \frac{1}{9} \times \begin{Bmatrix}   
  1,0,0,0,0,0,0,0,0,0,0,0, \\           
  2,0,0,0,0,0,0,0,0,0,0,0, \\           
  3,0,0,0,0,0,0,0,0,0,0,0, \\           
  2,0,0,0,0,0,0,0,0,0,0,0, \\           
  1,0,0,0,0,0,0,0,0,0,0,0, \\           
  \end{Bmatrix}
  $$ $$14$$<!---\[7.74\]-->

***Step 2: Normalizing the seasonal factors***

The preliminary seasonal factors are normalized in such a way that for
one year their average is equal to zero (additive model) or to unity
(multiplicative model).

***Step 3: Estimating the irregular component***

The initial normalized seasonal factors are removed from the
Seasonal-Irregular component to provide an estimate of the irregular
component.

***Step 4: Calculating a moving standard deviation***

By default, a moving standard deviation of the irregular component is
calculated at five-year intervals. Each standard deviation is associated
with the central year used to calculate it. The values in the central
year, which in the absolute terms deviate from average by more than the
**Usigma** parameter are marked as extreme values and assigned a zero
weight. After excluding the extreme values the moving standard deviation
is calculated once again.

***Step 5: Detecting extreme values and weighting the irregular***

The default settings for assigning a weight to each value of irregular
component are:

-   Values which are more than **Usigma** (2.5, by default) standard
    deviations away (in the absolute terms) from the 0 (additive) or 1
    (multiplicative) are assigned a zero weight;

-   Values which are less than 1.5 standard deviations away (in the
    absolute terms) from the 0 (additive) or 1 (multiplicative) are
    assigned a full weight (equal to one);

-   Values which lie between 1.5 and 2.5 standard deviations away (in
    the absolute terms) from the 0 (additive) or 1 (multiplicative) are
    assigned a weight that varies linearly between 0 and 1 depending on
    their position.

The default boundaries for the detection of the extreme values can be
changed with **LSigma** and **USigma** parameters

***Step 6: Adjusting extreme values of the seasonal-irregular
component***

Values of the SI component are considered extreme when a weight less
than 1 is assigned to their irregular. Those values are replaced by a
weighted average of five values:

-   The value itself with its weight;

-   The two preceding values, for the same period, having a full
    weight(if available);

-   The next two values, for the same period, having full a weight (if
    available).

When the four full-weight values are not available, then a simple
average of all the values available for the given period is taken.

This general algorithm is used with some modification in parts B and C
for detection and replacement of extreme values.

##### **X-11 tables**

The list of tables produced by JDemetra+ is presented below. It is not
identical to the output produced by the original X-11 program.

**Part A -- Preliminary Estimation of Outliers and Calendar Effects.**

This part includes prior modifications to the original data made in the
RegARIMA part:

-   Table A1 -- Original series;

-   Table A1a -- Forecast of Original Series;

-   Table A2 -- Leap year effect;

-   Table A6 -- Trading Day effect (1 or 6 variables);

-   Table A7 -- The Easter effect;

-   Table A8 -- Total Outlier Effect;

-   Table A8i -- Additive outlier effect;

-   Table A8t -- Level shift effect;

-   Table A8s -- Transitory effect;

-   Table A9 -- Effect of user-defined regression variables assigned to
    the seasonally adjusted series or for which the component has not
    been defined;

-   Table 9sa -- Effect of user-defined regression variables assigned to
    the seasonally adjusted series;

-   Table9u -- Effect of user-defined regression variables for which the
    component has not been defined.

**Part B -- Preliminary Estimation of the Time Series Components:**

-   Table B1 -- Original series after adjustment by the RegARIMA model;

-   Table B2 -- Unmodified Trend (preliminary estimation using composite
    moving average);

-   Table B3 -- Unmodified Seasonal -- Irregular Component (preliminary
    estimation);

-   Table B4 -- Replacement Values for Extreme SI Values;

-   Table B5 -- Seasonal Component;

-   Table B6 -- Seasonally Adjusted Series;

-   Table B7 -- Trend (estimation using Henderson moving average);

-   Table B8 -- Unmodified Seasonal -- Irregular Component;

-   Table B9 -- Replacement Values for Extreme SI Values;

-   Table B10 -- Seasonal Component;

-   Table B11 -- Seasonally Adjusted Series;

-   Table B13 -- Irregular Component;

-   Table B17 -- Preliminary Weights for the Irregular;

-   Table B20 -- Adjustment Values for Extreme Irregulars.

**Part C -- Final Estimation of Extreme Values and Calendar Effects:**

-   Table C1 -- Modified Raw Series;

-   Table C2 -- Trend (preliminary estimation using composite moving
    average);

-   Table C4 -- Modified Seasonal -- Irregular Component;

-   Table C5 -- Seasonal Component;

-   Table C6 -- Seasonally Adjusted Series;

-   Table C7 -- Trend (estimation using Henderson moving average);

-   Table C9 -- Seasonal -- Irregular Component;

-   Table C10 -- Seasonal Component;

-   Table C11 -- Seasonally Adjusted Series;

-   Table C13 -- Irregular Component;

-   Table C20 -- Adjustment Values for Extreme Irregulars.

**Part D -- Final Estimation of the Different Components:**

-   Table D1 -- Modified Raw Series;

-   Table D2 -- Trend (preliminary estimation using composite moving
    average);

-   Table D4 -- Modified Seasonal -- Irregular Component;

-   Table D5 -- Seasonal Component;

-   Table D6 -- Seasonally Adjusted Series;

-   Table D7 -- Trend (estimation using Henderson moving average);

-   Table D8 -- Unmodified Seasonal -- Irregular Component;

-   Table D9 -- Replacement Values for Extreme SI Values;

-   Table D10 -- Final Seasonal Factors;

-   Table D10A -- Forecast of Final Seasonal Factors;

-   Table D11 -- Final Seasonally Adjusted Series;

-   Table D11A -- Forecast of Final Seasonally Adjusted Series;

-   Table D12 -- Final Trend (estimation using Henderson moving
    average);

-   Table D12A -- Forecast of Final Trend Component;

<!-- -->

-   Table D13 -- Final Irregular Component;

-   Table D16 -- Seasonal and Calendar Effects;

-   Table D16A -- Forecast of Seasonal and Calendar Component;

-   Table D18 -- Combined Calendar Effects Factors.

**Part E -- Components Modified for Large Extreme Values:**

-   Table E1 -- Raw Series Modified for Large Extreme Values;

-   Table E2 -- SA Series Modified for Large Extreme Values;

-   Table E3 -- Final Irregular Component Adjusted for Large Extreme
    Values;

-   Table E11 -- Robust Estimation of the Final SA Series.

**Part F -- Quality indicators:**

-   Table F2A -- Changes, in the absolute values, of the principal
    components;

-   Table F2B -- Relative contribution of components to changes in the
    raw series;

-   Table F2C -- Averages and standard deviations of changes as a
    function of the time lag;

-   Table F2D -- Average duration of run;

-   Table F2E -- I/C ratio for periods span;

-   Table F2F -- Relative contribution of components to the variance of
    the stationary part of the original series;

-   Table F2G -- Autocorrelogram of the irregular component.

#### Filter length choice

A seasonal filter is a weighted average of a moving span of fixed length
within a time series that can be used to remove a fixed seasonal
pattern. X-13ARIMA-SEATS uses several of these filters, according to the
needs of the different stages of the program. As only X-13ARIMA-SEATS
allows the user to manually select seasonal filters, this case study can
be applied only to the X-13ARIMA-SEATS specifications.

The automatic seasonal adjustment procedure uses the default options to
select the most appropriate moving average. However there are occasions
when the user will need to specify a different seasonal moving\
average to that identified by the program. For example, if the SI values
do not closely follow the seasonal component, it may be appropriate to
use a shorter moving average. Also the presence of sudden breaks in the
seasonal pattern -- e.g. due to changes in the methodology -- can
negatively impact on the automatic selection of the most appropriate
seasonal filter. In such cases the usage of short seasonal filters in
the selected months or quarters can be considered. Usually, a shorter
seasonal filter $(3 \times 1)$ allows seasonality to change very rapidly
over time. However, a very short seasonal filter should not normally be
used, as it might often lead to large revisions as new data becomes
available. If a short filter is to be used it will usually be limited to
one month/quarter with a known reason for wanting to capture a rapidly
changing seasonality.

In the standard situation one seasonal filter is applied to all
individual months/quarters. The estimation of seasonal movements is
therefore based on the sample windows of equal lengths for each
individual month/quarter (i.e. for each month/quarter the seasonal
filter length or the number of years representing the major part of the
seasonal filter weights is identical). This approach relies on the
assumption that the number of past periods in which the conditions
causing seasonal behaviour are sufficiently homogenous is the same in
all months/quarters. However, this assumption does not always hold.
Seasonal causes may change in one month, while staying the same in
others[^seasonal-adjustment-16]. For instance, seasonal
heteroskedasticity might require different filter lengths in different
months or quarters.

[^seasonal-adjustment-16]: When the series are non-stationary
    differentiation is performed before the seasonality tests.

Another interesting example is industrial production in Germany. It can
be influenced by school holidays, since many employees have school-age
children, which interrupt their working pattern during these school
holidays. Consequently, businesses may temporarily suspend or lower
production during these periods. Since school holidays do not occur at
the same time throughout Germany and their timing varies from year to
year in the individual federal states, the effect is not completely
captured by seasonal adjustment. And since school holidays are treated
as usual working days, these effects are not captured by calendar
adjustment either. The majority of school holidays in Germany can take
place either in July or in August. This yields higher variances in the
irregular component for these months compared to the rest of the year.
Therefore, in this case a longer seasonal filter is used for these
months to account for this.

Another example might be given by German retail trade. Due to changes in
the consumers' behaviour around Christmas -- possibly more gifts of
money -- the seasonal peak in December has become steadily less
pronounced. To account for this moving seasonality, shorter seasonal
filters in December than during the rest of the year need to be applied.

JDemetra+ offers the options to assign a different seasonal filter
length to each period (month or quarter). The program offers these
options in the *single spec* mode as well as in the *multispec* mode,
albeit they are available only in the *Specifications* window, after a
document is created.

### Model based decomposition

SEATS is a program for estimating unobserved components in a time
series. It follows the ARIMA-model-based (AMB) method, developed from
the work of CLEVELAND, W.P., and TIAO, G.C. (1976), BURMAN, J.P. (1980),
HILLMER, S.C., and TIAO, G.C. (1982), BELL, W.R., and HILLMER, S.C.
(1984) and MARAVALL, A., and PIERCE, D.A. (1987).

In JDemetra+ the input for the model based signal extraction procedure
is always provided by TRAMO and includes the original series $y_{t}$,
the linearized series $x_{t}$ (i.e. the original series $y_{t}\$with the
deterministic effects removed), the ARIMA model for the stochastic
(linearized) time series $x_{t}$ and the deterministic effects (calendar
effects, outliers and other regression variable
effects)[^seasonal-adjustment-17]. SEATS decomposes the linearized
series (and the ARIMA model) into trend, seasonal, transitory and
irregular components, provides forecasts for these components, together
with the associated standard errors, and finally assign the
deterministic effects to each component yielding the *final*
components[^seasonal-adjustment-18]. The Minimum Mean Square Error
(MMSE) estimators of the components are computed with a
Wiener-Kolmogorov filter applied to the finite series extended with
forecasts and backcasts[^seasonal-adjustment-19].

[^seasonal-adjustment-17]: In the original software SEATS can be used
    either with TRAMO, operating on the input received from the latter,
    or alone, fitting an ARIMA model to the series.

[^seasonal-adjustment-18]: GÓMEZ, V., and MARAVALL, A. (1998).

[^seasonal-adjustment-19]: GÓMEZ, V., and MARAVALL, A. (1997).

One of the fundamental assumptions made by SEATS is that the linearized
time series $x_{t}$ follows the ARIMA model

$$\phi(B)\delta\left( B \right)x_{t} = \theta(B)a_{t}$$ $$1$$
<!---\[7.25\]-->

where:

$B$ -- the backshift operator $(Bx_{t} = x_{t - 1})$;

$\delta\left( B \right)$ -- a non-stationary autoregressive (AR)
polynomial in $B$ (unit roots);

$\theta\left( B \right)$ -- an invertible moving average (MA) polynomial
in $B$ and in $B^{S}$, which can be expressed in the multiplicative form

$\left( 1 + \vartheta_{1}B + \ldots{+ \ \vartheta}{q}B^{q} \right)\left( \ 1 + \Theta{1}B^{s} + \ldots{+ \ \Theta}_{Q}B^{\text{sQ}} \right)$
;

$\phi(B)$ -- a stationary autoregressive (AR) polynomial in $B$ and in
$B^{S}\$containing regular and seasonal unit roots, with *s*
representing the number of observations per year;

$a_{t}$ -- a white-noise variable with the variance$\ V(a)$.

It should be noted that the stochastic time series can be predicted
using its past observations and making an error. The variable $a_{t}$,
which is assumed to be white noise, is the fundamental *innovation* to
the series at time *t*, that is the part that cannot be predicted based
on the past history of the series.

Denoting
$\varphi\left( B \right) = \phi\left( B \right)\delta\left( B \right),\$
$$1$$ <!---\[7.25\]--> can be written in a more concise form as

$$\varphi\left( B \right)x_{t} = \theta(B)a_{t}$$, $$2$$
<!---\[7.26\]-->

where $\varphi\left( B \right)$ contains both the stationary and the
nonstationary roots.

#### Derivation of the models for the components

Let us consider the additive decomposition model

$$x_{t} = \sum_{i = 1}^{k}x_{\text{it}}$$, $$3$$ <!---\[7.27\]-->

where *i* refers to the orthogonal components: trend, seasonal,
transitory or irregular. Apart from the irregular component, supposed to
be a white noise, it is assumed that each component follows the ARIMA
model which can be represented, using the notation of $$2$$
<!---\[7.26\]-->, as:

$$\varphi_{i}\left( B \right)\ x_{\text{it}} = \theta_{i}(B)a_{\text{it}}$$,
$$4$$ <!---\[7.28\]-->

where
$\varphi_{i}\left( B \right) = \phi_{i}\left( B \right)\delta_{i}\left( B \right),\ \ x_{\text{it}}$
is the *i*-*th* unobserved component, $\varphi_{i}\left( B \right)$ and
$\theta_{i}\left( B \right)$ are finite polynomials of order $p_{i}$ and
$q_{i}$, respectively, and $a_{\text{it}},$ the disturbance associated
with such component, is a white noise process with zero mean and
constant variance $V(a_{i})$ and $a_{\text{it}}$ and $a_{\text{jt}}\$are
not correlated for $i \neq j\$and for any $t$.. These disturbances are
functions of the innovations in the series and are called
"pseudo-innovations" in the literature concerning the AMB decomposition
as they refer to the components that are never observed
[^seasonal-adjustment-20]. In the JDemetra+ documentation the term
"innovations" is used to refer to the "pseudo-innovations".

[^seasonal-adjustment-20]: GÓMEZ, V., and MARAVALL, A. (2001a).

The following assumptions hold for $$4$$ <!---\[7.28\]-->. For each
$\text{i}$ the polynomials $\phi_{i}\left( B \right)$,
$\delta_{i}\left( B \right)$ and $\theta_{i}(B)$ are prime and of finite
order. The roots of $\delta_{i}\left( B \right)$ lies on the unit
circle; those of $\phi_{i}\left( B \right)$ lie outside, while all the
roots of $\theta_{i}\left( B \right)\$are on or outside the unit circle.
This means that nonstationary and noninvertible components are allowed.
Since different roots of the AR polynomial induce peaks in the
spectrum[^seasonal-adjustment-21] of the series at different
frequencies, and given that different components are associated with the
spectral peaks for different frequencies, it is assumed that for
$i \neq j$ the polynomials$\ \phi_{i}\left( B \right)$ and
$\phi_{j}\left( B \right)$ do not share any common root (they are
coprime). Finally, it is assumed that the polynomials
$\theta_{i}\left( B \right),\ i = 1,\ldots,k$ are prime share no unit
root in common, guaranteeing the invertibility of the overall series. In
fact, since the unit root of $\theta_{i}\left( B \right)$ induce a
spectral zero, when the polynomials
$\theta_{i}\left( B \right),\ i = 1,\ldots,k$ share no unit root in
common, there is no frequency for which all component spectra become
zero[^seasonal-adjustment-22].

[^seasonal-adjustment-21]: For description of the spectrum see section
    [Spectral Analysis](..\theory\spectral.html).

[^seasonal-adjustment-22]: MARAVALL, A. (1995).

Since aggregation of ARIMA models yields ARIMA models, the series
$x_{t}\$will also follow an ARIMA model, as in $$2$$ <!---\[7.26\]-->,
and consequently the following identity can be derived:

$$\frac{\theta(B)}{\varphi(B)}a_{t} = \sum_{i = 1}^{k}{\frac{\theta_{i}(B)}{\varphi_{i}(B)}a_{\text{it}}}$$.
$$5$$ <!---\[7.29\]-->

In the ARIMA model based approach implemented in SEATS, the ARIMA model
identified and estimated for the observed series $x_{t}$ is decomposed
to derive the models for the components. In particular, the AR
polynomials for the components, $\varphi_{i}\left( B \right),$ are
easily derived through the factorization of the AR polynomial
$\varphi\left( B \right)$:

$$\varphi\left( B \right) = \prod_{i = 1}^{k}{\varphi_{i}\left( B \right)}$$,
$$6$$ <!---\[7.30\]-->

while the MA polynomials for the components, together with the
innovation variances $V(a_{i})$, cannot simply be obtained through the
relationship:

$$\theta(B)a_{t} = \sum_{i = 1}^{k}{\varphi_{\text{ni}}\left( B \right)}\theta_{i}(B)a_{\text{it}}$$,
$$7$$ <!---\[7.31\]-->

where $\varphi_{\text{ni}}\left( B \right)$ is the product of all
$\varphi_{j}\left( B \right),\ j = 1,\ldots,k$, except from
$\varphi_{i}\left( B \right)$. Further assumptions are therefore needed
to cope with the underidentification problem: i) $p_{i} \geq q_{i}$ and
ii) the canonical decomposition, i.e. the decomposition that allocate
all additive white noise to the irregular component (yielding
noninvertible components except the irregular).

To understand how SEATS factorizes the AR polynomials, first a concept
of a root will be explored[^seasonal-adjustment-23].

[^seasonal-adjustment-23]: Description based on KAISER, R., and
    MARAVALL, A. (2000) and MARAVALL, A. (2008c).

The equation $$2$$ <!---\[7.26\]--> can be expressed as:

$$\psi^{- 1}(B)x_{t} = a_{t}(1 + \varphi_{1}B + \ldots\varphi_{p}B^{p})x_{t} =(1 + \theta_{1}B + \ldots\theta_{q}B^{q})a_{t}$$,
$$8$$ <!---\[7.32\]-->

Let us now consider $$2$$ <!---\[7.26\]--> in the inverted form:

$$\theta\left( B \right)y_{t} = \varphi(B)a_{t}$$, $$9$$
<!---\[7.33\]-->

If both sides of $$8$$ <!---\[7.32\]--> are multiplied by $x_{t - k}$
with $k > q$, and expectations are taken, the right hand side of the
equation vanishes and the left hand side becomes:

$$\varphi(B)\gamma_{k} = \gamma_{k} + \varphi_{1}\gamma_{k - 1} + \ldots\varphi_{p}\gamma_{k - p} = 0 $$,
$$10$$ <!---\[7.34\]-->

where $B$ operates on the subindex $k$.

The autocorrelation function $\gamma_{k}$ is a solution of $$10$$
<!---\[7.34\]--> with the characteristic equation:

$$z^{p} + \varphi_{1}z^{p - 1} + \ldots\varphi_{p - 1}z + \varphi_{p} = 0$$.
$$11$$ <!---\[7.35\]-->

If $z_{1}$,...,$\ z_{p}$ are the roots of $$11$$ <!---\[7.35\]-->, the
solutions of $$10$$ <!---\[7.34\]--> can be expressed as:

$\gamma_{k} = \sum_{i = 1}^{p}z_{i}^{k}$, $$12$$ <!---\[7.36\]-->

and will converge to zero as $k \rightarrow \infty$ when
$\left| r_{i} \right| < 1,\ i = 1,\ldots,p$. From $$10$$
<!---\[7.34\]--> and $$12$$ <!---\[7.36\]--> it can be noticed that
$z_{1} = B_{i}^{- 1}$, meaning that $z_{1}$,...,$\ z_{p}$ are the
inverses of the roots $B_{1},\ldots,B_{p}$ of the polynomial
$\varphi(B)$. The convergence of $\gamma_{k}$ implies that the roots of
the $\varphi(B)$ are larger than 1 in modulus (lie outside the unit
circle). Therefore, from the equation

$$
  {\varphi(B)}^{- 1} = \frac{1}{(1 - z_{1})\ldots(1 - z_{1})}
  $$ $$13$$ <!---\[7.37\]-->

it can be derived that ${\varphi(B)}^{- 1}$ is convergent and all its
inverse roots are less than 1 in modulus.

Equation $$11$$ <!---\[7.35\]--> has real and complex roots (solutions).
Complex number $x = a + bi$, with $a$ and $\text{b}$ both real numbers,
can be represented as
$x = r\left( cos(\omega) + i\ sin(\omega \right))$, where $i$ is the
imaginary unit${\ (i}^{2} = - 1)$, $r$ is the modulus of $x$, that is
$\ r = \left| x \right| = \sqrt{a^{2} + b^{2}}$ and $\omega$ is the
argument (frequency). When roots are complex, they are always in pairs
of complex conjugates. The representation of the complex number
$x = a + bi$ has a geometric interpretation in the complex plane
established by the real axis and the orthogonal imaginary axis.

![Text](All_images/UG_A_image3.png)

**Geometric representation of a complex number and of its conjugate**

Representing the roots of the characteristic equation $$11$$
<!---\[7.35\]--> in the complex plane enhances understanding how they
are allocated to the components. When the modulus $r$ of the roots in
$\text{z}$ are greater than 1 (i.e. modulus of the roots in
$\varphi(B)\  < 1$), the solution of the characteristic equation has a
systematic explosive process, which means that the impact of the given
impulse on the time series is more and more pronounced in time. This
behaviour is not in line with the developments that can be identified in
actual economic series. Therefore, the models estimated by TRAMO-SEATS
(and X-13ARIMA-SEATS) have never inverse roots in $B$ with modulus
greater than 1.

The characteristic equations associated with the regular and the
seasonal differences have roots in $\varphi(B)$ with modulus $r = 1$.
They are called non-stationary roots and can be represented on the unit
circle. Let us consider the seasonal differencing operator applied to a
quarterly time series $(1 - B^{4})$. Its characteristic equation is
${(z}^{4} - 1) = 0$ with solutions given by$\ z = \sqrt[4]{1}$, i.e.
$z_{1,2} = \pm 1$ and $z_{3,4} = \pm i1$. The first two solutions are
real and the last two are complex conjugates. They are represented by
the black points on the unit circle on the figure below.

![Text](All_images/UG_A_image4.png)

**Unit roots on the unit circle**

For the seasonal differencing operator $(1 - B^{12})$ applied to the
monthly time series the characteristic equation ${\ (z}^{12} - 1) = 0$
has twelve non-stationary solutions given by$\ z = \sqrt[12]{1}:$ two
real and ten complex conjugates, represented by the white circles in
unit roots figure above.

The complex conjugates roots generate the periodic movements of the
type:

$$z_{t} = A^{t}\cos\left( \omega t + W \right).$$ $$14$$
<!---\[7.38\]-->

where:

$A$ -- amplitude;

$\omega$ -- angular frequency (in radians);

$W$ -- phase (angle at $t = 0)$.

The frequency $f$, i.e. the number of cycles per unit time, is
$\frac{\omega}{2\pi}$. If it is multiplied by *s*, the number of
observations per year, the number of cycles completed in one year is
derived. The period of function $$14$$ <!---\[7.38\]-->, denoted by
$\tau$, is the number of units of time (months/quarters) it takes for a
full circle to be completed.

For quarterly series the seasonal movements are produced by complex
conjugates roots with angular frequencies at $\frac{\pi}{2}$ (one cycle
per year) and $\pi$ (two cycles per year). The corresponding number of
cycles per year and the length of the movements are presented in the
table below.

**Seasonal frequencies for a quarterly time series**

{: .table .table-style} \| **Angular frequency (**$\omega$) \|
**Frequency (cycles per unit time) (**$f$) \| **Cycles per year** \|
**Length of the movement measured in quarters (**$\tau$) \|
\|-----------------\|-----------------\|-----------------\|-----------------\|
\| $\frac{\pi}{2}$ \| 0.25 \| 1 \| 4 \| \| $\pi$ \| 0.5 \| 2 \| 2 \|

For monthly time series the seasonal movements are produced by complex
conjugates roots at the angular frequencies:
$\ \frac{\pi}{6},\frac{\pi}{3},\ \frac{\pi}{2},\ \frac{2\pi}{3},\ \frac{5\pi}{6}\$and
$\pi$. The corresponding number of cycles per year and the length of the
movements are presented in the table below: Seasonal frequencies for a
monthly time series.

**Seasonal frequencies for a monthly time series**

{: .table .table-style} \| **Angular frequency (**$\omega$) \|
**Frequency (cycles per unit time) (**$f$) \| **Cycles per year** \|
**Length of the movement measured in months (**$\tau$) \|
\|-----------------\|-----------------\|-----------------\|-----------------\|
\| $\frac{\pi}{6}$ \| 0.083 \| 1 \| 12 \| \| $\frac{\pi}{3}$ \| 0.167 \|
2 \| 6 \| \| $\frac{\pi}{2}$ \| 0.250 \| 3 \| 4 \| \| $\frac{2\pi}{3}$
\| 0.333 \| 4 \| 3 \| \| $\frac{5\pi}{6}$ \| 0.417 \| 5 \| 2.4 \| \|
$$\pi$$ \| 0.500 \| 6 \| 2 \|

In JDemetra+ SEATS assigns the roots of the AR full polynomial to the
components according to their associated modulus and frequency,
i.e.:[^seasonal-adjustment-24]

[^seasonal-adjustment-24]: For details see MARAVALL, A., CAPORELLO, G.,
    PÉREZ, D., and LÓPEZ, R. (2014).

-   Roots of $\left( 1 - B \right)^{d}$ are assigned to trend component.

-   Roots of
    $\ \left( 1 - B^{s} \right)^{d_{s}} = {((1 - B)(1 + B + \ldots + B^{s - 1}))}^{d_{s}}\$are
    assigned to the trend component (root
    of${\ \left( 1 - B \right)}^{d_{s}}$) and to the seasonal component
    (roots of${\ (1 + B + \ldots + B^{s - 1})}^{d_{s}}$).

-   When the modulus of the inverse of a real positive root of
    $\varphi(B)$ is greater than $k$ or equal to $k$, where $k$ is the
    threshold value controlled by the *Trend boundary* parameter(in the
    original SEATS it is controlled by *rmod*)[^seasonal-adjustment-25],
    then the root is assigned to the trend component. Otherwise it is
    assigned to the transitory component.

-   Real negative inverse roots of
    $\text{ ϕ}_{p}\left( B \right)\$associated with the seasonal
    two-period cycle are assigned to the seasonal component if their
    modulus is greater than *k*, where $k$ is the threshold value
    controlled by the *Seasonal boundary* and the *Seas. boundary
    (unique)* parameters. Otherwise they are assigned to the transitory
    component.

-   Complex roots, for which the argument (angular frequency) is close
    enough to the seasonal frequency are assigned to the seasonal
    component. Closeness is controlled by the *Seasonal tolerance* and
    *Seasonal tolerance (unique)* parameters (in the original SEATS it
    is controlled by *epsphi*). Otherwise they are assigned to the
    transitory component.

-   If $d_{s}\$(seasonal differencing order) is present$\$and
    $\text{Bphi} < 0$ ($\text{Bphi}$ is the estimate of the seasonal
    autoregressive parameter), the real positive inverse root is
    assigned to the trend component and the other ($s - 1$) inverse
    roots are assigned to the seasonal component. When $d_{s} = 0$, the
    root is assigned to the seasonal when $\text{Bphi} < - 0.2$ and/or
    the overall test for seasonality indicates presence of seasonality.
    Otherwise it goes to the transitory component. Also, when
    $\text{Bphi} > 0$, roots are assigned to the transitory component.

[^seasonal-adjustment-25]: In JDemetra+ this argument is called *Trend
    boundary*.

For further details about JDemetra+ parameters see section
[TramoSeats](../reference-manual/sa-spec-tramo.html).

It should be highlighted that when$\ Q > P$, where $Q$ and $P$ denote
the orders of the polynomials $\varphi\left( B \right)$ and $\theta(B)$,
the SEATS decomposition yields a pure MA $(Q - P)$ component (hence
transitory). In this case the transitory component will appear even when
there is no AR factor allocated to it.

Once these rules are applied, the factorization of the AR polynomial
presented by $$2$$ <!---\[7.26\]--> yields to the identification of the
AR polynomials for the components which contain, respectively, the AR
roots associated with the trend component, the seasonal component and
the transitory component.[^seasonal-adjustment-26]

[^seasonal-adjustment-26]: The AR roots close to or at the trading day
    frequency generates a stochastic trading day component. A stochastic
    trading day component is always modelled as a stationary ARMA(2,2),
    where the AR part contains the roots close to the TD frequency, and
    the MA(2) is obtained from the model decomposition (MARAVALL, A.,
    and PÉREZ, D. (2011)). This component, estimated by SEATS, is not
    implemented by the current version of JDemetra+.

Then with the partial fraction expansion the spectrum of the final
components are obtained.

For example, the Airline model for a monthly time series:

$$(1 - B)(1 - B^{12})x_{t} = (1 + \theta_{1}B)(1 + \Theta_{1}B^{12})\ a_{t}$$,
$$15$$ <!---\[7.39\]-->

is decomposed by SEATS into the model for the trend component:

$$(1 - B)(1 - B)c_{t} = (1 + \theta_{c,1}B + \theta_{c,2}B^{2})a_{c,t}$$,
$$16$$ <!---\[7.40\]-->

and the model for the seasonal component:

$$\left( 1 + B + \ldots + B^{11} \right)s_{t} = \left( 1 + \theta_{s,1}B + \ldots + {\theta_{s,11}B}^{11} \right)a_{s,t},$$
$$17$$ <!---\[7.41\]-->

As a result, the Airline model is decomposed as follows:

$$\frac{(1 + \theta_{1}B)(1 + \Theta_{1}B^{12})}{(1 - B)(1 - B)}a_{t} = \frac{\left( 1 + \theta_{s,1}B + \ldots + {\theta_{s,11}B}^{11} \right)}{\left( 1 + B + \ldots + B^{11} \right)}a_{s,t} + \frac{(1 + \theta_{c,1}B + \theta_{c,2}B^{2})}{(1 - B)(1 - B)}a_{c,t} + u_{t}$$.
$$18$$ <!---\[7.42\]-->

The transitory component is not present in this case and the irregular
component is the white noise.

The partial fractions decomposition is performed in a frequency domain.
In essence, it consists in portioning of the
pseudo-spectrum[^seasonal-adjustment-27] of $x_{t}$ into additive
spectra of the components. When the AMB decomposition of the ARIMA model
results in the non-negative spectra for all components, the
decomposition is called admissible[^seasonal-adjustment-28]. In such
case an infinite number of admissible decompositions exists, i.e.
decompositions that yield the non-negative spectra of all components.
Therefore, the MA polynomials and the innovation variances cannot be yet
identified from the model of $x_{t}$. As sketched above, to solve this
underidentification problem and identify a unique decomposition, it is
assumed that for each component the order of the MA polynomial is no
greater than the order of the AR polynomial and the canonical solution
of S.C. Hillmer and G.C. Tiao is applied[^seasonal-adjustment-29], i.e.
all additive white noise is added to the irregular component As a
consequence all components derived from the canonical decomposition,
except from the irregular, have a spectral minimum of zero and are thus
noninvertible[^seasonal-adjustment-30]. Given the stochastic features of
the series, it can be shown by that the canonical decomposition produces
as stable as possible trend and seasonal components since it maximizes
the variance of the irregular and minimizes the variance of the other
components[^seasonal-adjustment-31]. However, there is a price to be
paid as canonical components can produce larger revisions in the
preliminary estimators of the component[^seasonal-adjustment-32] than
any other admissible decomposition.

[^seasonal-adjustment-27]: The term pseudo-spectrum is used for a
    non-stationary time series, while the term spectrum is used for a
    stationary time series.

[^seasonal-adjustment-28]: If the ARIMA model estimated in TRAMO does
    not accept an admissible decomposition, SEATS replaces it with a
    decomposable approximation. The modified model is therefore used to
    decompose the series. There are also other rare situations when the
    ARIMA model chosen by TRAMO is changed by SEATS. It happens when,
    for example, the ARIMA models generate unstable seasonality or
    produce a senseless decomposition. Such examples are discussed by
    MARAVALL, A. (2009).

[^seasonal-adjustment-29]: HILLMER, S.C., and TIAO, G.C. (1982).

[^seasonal-adjustment-30]: GÓMEZ, V., and MARAVALL, A. (2001a).

[^seasonal-adjustment-31]: HILLMER, S.C., and TIAO, G.C. (1982).

[^seasonal-adjustment-32]: MARAVALL, A. (1986).

The figure below represents the pseudo-spectrum for the canonical trend
and an admissible trend.

![Text](All_images/UG_A_image5.png)

**A comparison of canonical trend and admissible trend**

A pseudo-spectrum is denoted by$\ g_{i}(\omega)$, where $\omega$
represents the angular frequency. The pseudo-spectrum of $x_{\text{it}}$
is defined as the Fourier transform of ACGF of$\ x_{t}$ which is
expressed as:

$$\frac{\psi_{i}\left( B \right)\psi_{i}\left( F \right)}{\delta_{i}\left( B \right)\delta_{i}\left( F \right)}V(a_{i})$$,
$$19$$ <!---\[7.43\]-->

where:

$\psi_{i}\left( F \right) = \frac{\theta_{i}\left( F \right)}{\phi_{i}\left( F \right)}$

$\psi_{i}\left( B \right) = \frac{\theta_{i}\left( B \right)}{\phi_{i}\left( B \right)}$

$B$ is the backward operator,

$F$ is the forward operator.

A pseudo-spectrum for a monthly time series $x_{t}\$is presented in the
figure below: The pseudo-spectrum for a monthly series. The frequency
$\omega = 0$ is associated with the trend, frequencies in the range
$$$0 + \epsilon_{1},\ \frac{\pi}{6} - \epsilon_{2}$$$ with
$\left[0 + \epsilon_{1},\ \frac{\pi}{6} - \epsilon_{2}\right]$
$\epsilon_{1},\ \epsilon_{2} > 0$ and
$\epsilon_{1} < \ \frac{\pi}{6} - \epsilon_{2}\$ are usually associated
with the business-cycle and correspond to a period longer than a year
and bounded[^seasonal-adjustment-33]. The frequencies in the range
$$$\frac{\pi}{6},\ \pi$$$ are associated with the short term movements,
whose cycle is completed in less than a year. If a series contains an
important periodic component, its spectrum reveals a peak around the
corresponding frequency and in the ARIMA model it is captured by an AR
root. In the example below spectral peaks occur at the frequency
$\omega = 0$ and at the seasonal frequencies ( $\frac{\pi}{6}$,
$\frac{2\pi}{6},\ \frac{3\pi}{6},\ \frac{4\pi}{6},\frac{5\pi}{6},\pi$).
[^seasonal-adjustment-34]

[^seasonal-adjustment-33]: Ibid.

[^seasonal-adjustment-34]: KAISER, R., and MARAVALL, A. (2000).

![Text](All_images/UG_A_image6.png)

**The pseudo-spectrum for a monthly series**

In the decomposition procedure, the pseudo-spectrum of the time series
$x_{t}$ is divided into the spectra of its components (in the example
figure below, four components were obtained).

![Text](All_images/UG_A_image7.png)

**The pseudo-spectra for the components**

#### Estimation of the components with the Wiener-Kolmogorow filter {#estimation-of-the-components-with-the-wiener-kolmogorow-filter}

The various components are estimated using Wiener-Kolmogorow (WK)
filters. JDemetra+ includes three options to estimate the WK filter,
namely *Burman*, *KalmanSmoother* and
*MCElroyMatrix*[^seasonal-adjustment-35]. Here the first of
abovementioned options, proposed by BURMAN, J.P. (1980) will be
explained.

[^seasonal-adjustment-35]: The choice of the estimation method is
    controlled by the *Method* parameter, explained in the [SEATS
    specification](../reference-manual/sa-spec-tramo.html) section.

The estimation procedure and the properties of the WK filter are easier
to explain with a two-component model. Let the seasonally adjusted
series ($s_{t}$) be the signal of interest and the seasonal component
($n_{t}$) be the remainder, "the noise". The series is given by the
model $$2$$ <!---\[7.26\]--> and from $$4$$ <!---\[7.28\]--> the models
for theoretical components are:

$$\varphi_{s}(B)s_{t} = \theta_{s}(B)a_{\text{st}}$$ $$20$$
<!---\[7.44\]-->

and

$$\varphi_{n}(B)n_{t} = \theta_{n}(B)a_{\text{nt}}$$. $$21$$
<!---\[7.45\]-->

From $$6$$ <!---\[7.30\]--> and $$7$$ <!---\[7.31\]--> it is clear that
$\varphi\left( B \right) = \varphi_{s}(B)\varphi_{n}(B)$ and
$\theta\left( B \right)a_{t} = \theta_{s}(B)a_{\text{st}}+\theta_{n}(B)a_{\text{nt}}$.

As the time series components are never observed, their estimators have
to be used. Let us note $X_{T}$ an infinite realization of the time
series $x_{t}$. SEATS computes the Minimum Mean Square Error (MMSE)
estimator of $s_{t}$, e.g. the estimator $$\widehat{s}_{t}$$ that
minimizes
$$E\lbrack\left({s_{t}-{\widehat{s}}_{t})}^{2}|X_{T} \right)\rbrack$$.
Under the normality assumption $${\widehat{s}}_{t|T}$$ is also equal to
the conditional expectation $$E\left(s_{t}|X_{T}\right)$$, so it can be
presented as a linear function of the elements in
$$X_{T}$$.[^seasonal-adjustment-36] WHITTLE (1963) shows that the MMSE
estimator of $${\widehat{s}}_{t}$$ is:

[^seasonal-adjustment-36]: MARAVALL, A. (2008c).

$${\widehat{s}}_{t} = k_{s}\frac{\psi_{s}(B)\psi_{s}(F)}{\psi(B)\psi(F)}x_{t}$$,
$$22$$ <!---\[7.46\]-->

where $$\psi(B)= \frac{\theta(B)}{\phi(B)}$$,

$$F = B^{- 1}$$ and $$k_{s}=\frac{V(a_{s})}{V(a)}$$,

$$V(a_{s})$$ is the variance of $$a_{st}$$ and $$V(a)$$ is the variance
of $$a_{t}$$.

Expressing the $$\psi\left(B\right)$$ polynomials as functions of the AR
and MA polynomials, after cancelation of roots, the estimator of
$$s_{t}$$ can be expressed as:

$${\widehat{s}}_{t} = k_{s}\frac{\theta_{s}\left(B\right)\theta_{s}\left(F\right)\varphi_{n}\left(B \right)\delta_{n}\left(B\right)\varphi_{n}\left(F\right)\delta_{n}\left(F\right)}{\theta\left(B\right)\theta\left(F \right)}x_{t}$$,
$$23$$ <!---\[7.47\]-->

where:

$$\nu_{s}\left( B,F \right) = k_{s}\frac{\theta_{s}\left( B \right)\theta_{s}\left( F \right)\varphi_{n}\left( B \right)\delta_{n}\left( B \right)\varphi_{n}\left( F \right)\delta_{n}\left( F \right)}{\theta\left( B \right)\theta\left( F \right)}$$
$$24$$ <!---\[7.48\]-->

is a WK filter.

Equation $$24$$ <!---\[7.48\]--> shows that the WK filter is two-sided
(uses observations both from the past and from the future), centered
(the number of points in the past is the same as in the future) and
symmetric (for any $k$ the weight applied to $x_{t - k}$ and $x_{t + k}$
is the same), which allows the phase effect to be avoided. Due to
invertibility of $\theta\left( B \right)$ (and $\theta\left( F \right)$)
the filter is convergent in the past and in the future.

The estimator can be presented as

$${\widehat{s}}_{t} = \nu_{i}\left(B,F\right)x_{t}$$, $$25$$
<!---\[7.49\]-->

where

$$\nu_{i}\left(B,F\right)=\nu_{0}+ \sum_{j = 1}^{\infty}\nu_{ij}(B^{j}+F^{j})$$

is the WK filter.

The example of the WK filters obtained for the pseudo-spectra of the
series illustrated above is shown on the figure below: WK filters for
components.

![Text](All_images/UG_A_image8.png)

**WK filters for components**

The WK filter from $$24$$ <!---\[7.48\]--> can also be expressed as a
ratio of two pseudo-autocovariance generating functions (p-ACGF). The
p-ACGF function summarizes the sequence of absolutely summable
autocovariances of a stationary process $x_{t}$ (see section section
[Spectral Analysis](..\theory\spectral.html)).

The ACGF function of an ARIMA process is expressed as:

$$acgf(B) = \frac{\theta\left( B \right)\theta\left( F \right)}{\phi\left( B \right)\delta\left( B \right)\phi\left( F \right)\delta\left( F \right)}V(a)$$
$$26$$ <!---\[7.50\]-->

And, the WK filter can be rewritten as:

$$\nu_{s}\left( B,F \right) = \frac{\gamma_{s}(B,F)}{\gamma(B,F)}$$,
$$27$$ <!---\[7.51\]-->

where:

$$\gamma_{s}\left( B,F \right) = \frac{\theta_{s}\left( B \right)\theta_{s}\left( F \right)}{\phi_{s}\left( B \right)\delta_{s}\left( B \right)\phi_{s}\left( F \right)\delta_{s}\left( F \right)}V(a_{s})$$
is the p-ACGF of $$s_{t}$$;

$\gamma\left( B,F \right) = \frac{\theta\left( B \right)\theta\left( F \right)}{\phi\left( B \right)\delta\left( B \right)\phi\left( F \right)\delta\left( F \right)}V(a)$
is the p-ACGF of $x_{t}$.

From $$24$$ <!---\[7.48\]--> it can be seen that the WK filter depends
on both the component and the series models. Consequently, the estimator
of the component and the WK filter reflect the characteristic of data
and by construction, the WK filter adapts itself to the series under
consideration. Therefore, the ARIMA model is of particular importance
for the SEATS method. Its misspecification results in an incorrect
decomposition.

This adaptability, if the model has been correctly determined, avoids
the dangers of under and overestimation with an ad-hoc filtering. For
example, for the series with a highly stochastic seasonal component the
filter adapts to the width of the seasonal peaks and the seasonally
adjusted series does not display any spurious
seasonality[^seasonal-adjustment-37]. Examples of WK filters for
stochastic and stable seasonal components are presented on the figure
below.

[^seasonal-adjustment-37]: MARAVALL, A. (1995).

![Text](All_images/UG_A_image9.png)

**WK filters for stable and stochastic seasonal components**

The derivation of the components requires an infinite realization of
$x_{t}$ in the direction of the past and of the future. However, the
convergence of the WK filter guarantees that, in practice, it could be
approximated by a truncated (finite) filter and, in most applications,
for large $$k$$ the estimator for the central periods of the series can
be safely seen as generated by the WK filter[^seasonal-adjustment-38]:

[^seasonal-adjustment-38]: MARAVALL, A., and PLANAS, C. (1999).

$${\widehat{s}}_{t}=\nu_{k}x_{t-k} + \ldots + \nu_{0}x_{t} + \ldots + \nu_{k}x_{t+k}$$.
$$28$$ <!---\[7.52\]-->

When $T > 2L + 1$, where $T$ is the last observed period, and $L$ is an
a priori number that typically expands between 3 and 5 years, the
estimator expressed by $$23$$ <!---\[7.47\]--> can be assumed as the
final (historical) estimator for the central observations of the
series[^seasonal-adjustment-39]. In practice, the Wiener-Kolmogorov
filter is applied to $x_{t}$ extended with forecasts and backcasts from
the ARIMA model. The final or historical estimator of
$${\widehat{s}}_{t}$$, is obtained with a doubly infinite filter, and
therefore contains an error $$e_{st}$$ called final estimation error,
which is equal $$e_{st}=s_{t}-{\widehat{s}}_{t}$$.

[^seasonal-adjustment-39]: MARAVALL, A. (1998).

In the frequency domain, the Wiener-Kolmogorov filter$\ \nu(B,F)$ that
provides the final estimator of $s_{t}\$is expressed as the ratio of the
$s_{t}\$and $x_{t}$ pseudo-spectra:

$$\widetilde{\nu}\left( \omega \right) = \frac{g_{s}(\omega)}{g_{x}(\omega)}$$.
$$29$$ <!---\[7.53\]-->

The function $\widetilde{\nu}\left( \omega \right)\$is also referred as
the gain of the filter.[^seasonal-adjustment-40] GÓMEZ, V., and
MARAVALL, A. (2001a) show that when for some frequency the signal (the
seasonally adjusted series) dominates the noise (seasonal fluctuations)
the gain $\widetilde{\nu}\left( \omega \right)$ approaches 1. On the
contrary, when for some frequency the noise dominates the gain
$\widetilde{\nu}\left( \omega \right)\$approaches 0.

[^seasonal-adjustment-40]: GÓMEZ, V., and MARAVALL, A. (2001a).

The spectrum of the estimator of the seasonal component is expressed as:

$$g_{\widehat{s}}\left( \omega \right) = \left\lbrack \frac{g_{s}(\omega)}{g_{x}(\omega)} \right\rbrack^{2}g_{x}(\omega)$$,
$$30$$ <!---\[7.54\]-->

where$\ \left\lbrack \widetilde{\nu}\left( \omega \right) \right\rbrack^{2} = \left\lbrack \frac{g_{s}(\omega)}{g_{x}(\omega)} \right\rbrack^{2} = \left\lbrack \frac{g_{s}(\omega)}{g_{s}(\omega) + g_{n}(\omega)} \right\rbrack^{2} = \left\lbrack \frac{1}{1 + \frac{1}{r(\omega)}} \right\rbrack^{2}$
is the squared gain of the filter and
$r\left( \omega \right) = \frac{g_{s}(\omega)}{g_{n}(\omega)}$
represents the signal-to-noise ratio.

For each $\omega$, the MMSE estimation gives the signal-to-noise ratio.
If this ratio is high, then the contribution of that frequency to the
estimation of the signal will be also high. Assume that the trend is a
signal that needs to be extracted from a seasonal time series. Then
$R\left( 0 \right) = 1\$and the frequency $\omega = 0$ will only be used
for trend estimations. For seasonal frequencies
$R\left( \omega \right) = 0,$ so that these frequencies are ignored in
computing the trend resulting in spectral zeros in
$g_{\widehat{s}}\left( \omega \right)$. For this reason, unlike the
spectrum of the component, the component spectrum contains dips as it
can be seen on the figure below: Component spectrum and estimator
spectrum for trend.

![Text](All_images/UG_A_image10.png)

**Component spectrum and estimator spectrum for trend**

From the equation $$29$$ <!---\[7.53\]--> it is clear that the squared
gain of the filter determines how the variance of the series contributes
to the variance of the seasonal component for the different frequencies.
When $\widetilde{\nu}\left( \omega \right) = 1$, the full variation of
$x_{t}$ for that frequency is passed to $${\widehat{s}}_{t}$$, while if
$$\widetilde{\nu}\left(\omega\right) = 0 $$ the variation of $x_{t}$ for
that frequency is fully ignored in the computation of
$${\widehat{s}}_{t}$$. These two cases are well illustrated by the
figure below that shows the square gain of the WK filter for two series
already analysed in the figure above (Figure: WK filters for stable and
stochastic seasonal components).

![Text](All_images/UG_A_image11.png)

**The squared gain of the WK filter for stable and stochastic seasonal
components.**

Since $r\left( \omega \right) \geq 0$, then
$\widetilde{\nu}\left( \omega \right) \leq 1$ and from $$29$$
<!---\[7.53\]--> it can be derived that
$g_{\widehat{s}}\left( \omega \right) = \widetilde{\nu}\left( \omega \right)g_{s}(\omega)$.
As a result, the estimator will always underestimate the component, i.e.
it will be always more stable that the
component.[^seasonal-adjustment-41]

[^seasonal-adjustment-41]: Ibid.

Since
$g_{\widehat{n}}\left( \omega \right) < g_{n}\left( \omega \right)$
and$\ g_{\widehat{s}}\left( \omega \right) < g_{s}\left( \omega \right)$
the expression:
$g_{x}\left( \omega \right) - \left\lbrack g_{\widehat{n}}\left( \omega \right) + g_{\widehat{s}}\left( \omega \right) \right\rbrack \geq 0$
is the cross-spectrum. As it is positive, the MMSE yields correlated
estimators. This effect emerges since variance of estimator is smaller
than the variance of component. Nevertheless, if at least one
non-stationary component exists, cross-correlations estimated by
TRAMO-SEATS will tend to zero as cross-covariances between estimators of
the components are finite. In practice, the inconvenience caused by this
property will likely be of little relevance.

**Preliminary estimators for the components**

GÓMEZ, V., and MARAVALL, A. (2001a) point out that *the properties of
the estimators have been derived for the final (or historical)
estimators. For a finite (long enough) realization, they can be assumed
to characterize the estimators for the central observations of the
series, but for periods close to the beginning of the end the filter
cannot be completed and some preliminary estimator has to be used*.
Indeed, the historical estimator shown in $$28$$ <!---\[7.52\]--> is
obtained for the central periods of the series. However, when $t$
approaches $T$ (last observation), the WK filter requires observations,
which are not available yet. For this reason a preliminary estimator
needs to be used.

To introduce preliminary estimators let us consider a semi-finite
realization $\lbrack x_{- \infty}$,...$\ x_{T}$], where $T$ is the last
observed period. The preliminary estimator of $$x_{\text{it}}$$ obtained
at $T$ $$(T - t = k \geq 0)$$ can be expressed as

$$
{\widehat{x}}_{it|t + k}=\nu_{i}\left(B,F\right)x_{t|T}^{e}
$$, $$31$$ <!---\[7.55\]-->

where $$\nu_{i}\left(B,F \right)$$ is the WK filter and $$x_{t|T}^{e}$$
is the extended series, such that $x_{t|T}^{e} = x_{t}$ for $t \leq T$
and $$x_{t|T}^{e}={\widehat{x}}_{t|T}$$ for $$t>T$$, where
$${\widehat{x}}_{t|T}$$ denotes the forecast of $x_{t}$ obtained at
period $T$.

The future $k$ values necessary to apply the filter are not yet
available and are replaced by their optimal forecasts from the ARIMA
model on $$x_{t}$$. When $$k=0$$ the preliminary estimator becomes the
concurrent estimator. As the forecasts are linear functions of present
and past observations of $$x_{t}$$, the preliminary estimator
$${\widehat{x}}_{it}$$ will be a truncated asymmetric filter applied to
$$x_{t}$$ that generates a phase effect[^seasonal-adjustment-42].

[^seasonal-adjustment-42]: KAISER, R., and MARAVALL, A. (2000).

When a new observation $$x_{T + 1}$$ becomes available the forecast
$${\widehat{x}}_{T + 1|T}$$ is replaced by the observation and the
forecast $${\widehat{x}}_{iT + j|T}$$, $$j > 1$$ are updated to
$$x_{T + j|T + 1}$$ resulting in the revision
error[^seasonal-adjustment-43]. The total error in the preliminary
estimator $$d_{it|t + k}$$ is expressed as a sum of the final estimation
error ($$e_{it}$$) and the revision error ($$r_{it|t + k}$$), i.e.:

[^seasonal-adjustment-43]: MARAVALL, A. (1995).

$$
d_{it|t + k} = x_{it}-{\widehat{x}}_{it|t + k} = \left(x_{it} - {\widehat{x}}_{it}\right) + \left(          {\widehat{x}}_{it} - {\widehat{x}}_{it|t + k} \right) = e_{it} + r_{it|t + k}
$$, $$32$$ <!---\[7.56\]-->

where:

$$x_{it}-i^{th}$$ component;

$${\widehat{x}}_{it|t + k}$$- the estimator of $$x_{it}$$ when the last
observation is $$x_{t + k}$$.

Therefore the preliminary estimator is subject not only to the final
error but also to a revision error, which are orthogonal to each
other[^seasonal-adjustment-44]. The revision error decreases as $$k$$
increases, until it can be assumed equal to 0 for large enough $$k$$.

[^seasonal-adjustment-44]: MARAVALL, A. (2009).

It's worth remembering that SEATS estimates the unobservable components
of the time series so the "true" components are never observed.
Therefore, MARAVALL, A. (2009) stresses that *the error in the
historical estimator is more of academic rather than practical interest.
In practice, interest centres on revisions. (...) the revision standard
deviation will be an indicator of how far we can expect to be from the
optimal estimator that will be eventually attained, and the speed of
convergence of* ${\theta\left( B \right)\ }^{- 1}$ *will dictate the
speed of convergence of the preliminary estimator to the historical
one.* The analysis of an error is therefore useful for making decision
concerning the revision policy, including the policy for revisions and
horizon of revisions.

#### PsiE-weights

The estimator of the component is calculated as
$${\widehat{x}}_{it} = \nu_{s}\left(B,F\right)x_{t}$$. By replacing
$$x_{it}=\frac{\theta(B)}{\gamma(B)\delta(B)}a_{t}$$, the component
estimator can be expressed as[^seasonal-adjustment-45]:

[^seasonal-adjustment-45]: The section is based on KAISER, R., and
    MARAVALL, A. (2000).

$$
  {\widehat{x}}_{it} = \xi_{s}\left(B,F\right)a_{t}
  $$, $$33$$ <!---\[7.57\]-->

where
$\xi_{s}\left( B,F \right) = \ldots + \xi_{j}B^{j} + \ldots + \xi_{1}B + \xi_{0} + \xi_{- 1}F\ldots\xi_{- j}F^{j} + \ldots$.

This representation shows the estimator as a filter applied to the
innovation $$a_{t}$$, rather than on the series
$$x_{t}$$[^seasonal-adjustment-46]. Hence, the filter from $$32$$
<!---\[7.56\]--> can be divided into two components: the first one, i.e.
$$\ldots + \xi_{j}B^{j}+ \ldots+ \xi_{1}B + \xi_{0}$$, applies to prior
and concurrent innovations, the second one, i.e.
$$\xi_{- 1}F + \ldots + \xi_{- j}F^{j}$$ applies to future (i.e.
posterior to $$t$$) innovations. Consequently, $$\xi_{j}$$ determines
the contribution of $$a_{t - j}$$ to $${\widehat{s}}_{t}$$ while
$$\xi_{- j}$$ determines the contribution of $$a_{t + j}$$ to
$${\widehat{s}}_{t}$$. Finally, the estimator of the component can be
expressed as:

[^seasonal-adjustment-46]: See section PsiE-weights. For further details
    see MARAVALL, A. (2008).

$$
  {\widehat{x}}_{it} =\xi_{i}(B)^{-}a_{t} + \xi_{i}(F)^{+}a_{t + 1}
  $$, $$34$$ <!---\[7.58\]-->

where:

$\xi_{i}{(B)}^{-}a_{t}$ is an effect of starting conditions, present and
past innovations in series;

$\xi_{i}{(F)}^{+}a_{t + 1}$ is an effect of future innovations.

For the two cases already presented in figure *WK filters for stable and
stochastic seasonal components* and figure *The squared gain of the WK
filter for stable and stochastic seasonal components* above, the
psi-weights are shown in the figure below.

![Text](All_images/UG_A_image12.png)

It can be shown that $${\xi}_{- 1},\ldots,\xi_{- j}$$ are convergent and
$$\xi_{j},\ldots, {\xi}_{1},\xi_{0}$$ are divergent. From $$33$$
<!---\[7.57\]-->, the concurrent estimator is equal to $$
{\widehat{x}}_{it|t} = E_{t}x_{it}=E_{t}{\widehat{x}}_{it} = {\xi}_{i}(B)^{-}a_{t}
$$, $$35$$ <!---\[7.59\]-->

so that the revision $$
r_{it} = {\widehat{x}}_{it} - {\widehat{x}}_{it|t} = \xi_{i}(F)^{+}a_{t + 1}
$$ $$36$$ <!---\[7.60\]-->

is a zero-mean stationary MA process. As a result, historical and
preliminary estimators are cointegrated. From expression $$25$$
<!---\[7.49\]--> the relative size of the full revision and the speed of
convergence can be obtained.

### Quality assesment of the seasonal adjustement process

#### Diagnostics reading

#### Residual seasonality

We consider below tests on the seasonally adjusted series ($sa_t$) or on
the irregular component ($irr_t$). When the reasoning applies on both
components, we will use $y_t$. The functions $stdev$ stands for
"standard deviation" and $rms$ for "root mean squares"

The tests are computed on the log-transformed components in the case of
multiplicative decomposition.

### Non significant irregular

When $ir_t$ is not significant, we don't compute the tests on it, to
avoid irrelevant results. We consider that $ir_t$ is significant if
$stdev( ir_t)>0.01$ (multiplicative case) or if
$stdev(ir_t)/rms(sa_t) >0.01$ (additive case).

### QS test

The QS test is similar to the Ljung-Box test computed on the first two
seasonal lags, except that negative auto-correlations are set to 0. The
tests are computed on $\tilde \Delta sa_t$ and on $\tilde \Delta irr_t$.
The operator $\tilde \Delta$ applies as much differencing as needed on
the (log-transformed) series and corrects the result for mean effect.

The tests are not computed if the differences are not "significant".
$\tilde \Delta y_t$ is significant if
$stdev(\tilde \Delta y_t)/rms(y_t) >0.005$.

### F test

We compute by OLS the following model (SD = contrasts of seasonal
dummies, freq-1 variables):

$$y_t=\mu + \alpha y_{t-1} + \beta SD_t + \epsilon_t $$

The tests are the usual joint F-tests on $\beta \quad (H_0:\beta=0)$. By
default, we compute the tests on the 8 last years of the components, so
that they might highlight moving seasonal effects.

We could consider various modelling of the series. For instance, $y_t$
could follow an ARIMA model, pre-specified or automatically identified.
The solution used in JD+ 2.2 has the advantage of being fast and robust.
It is rather similar to what is used, for instance, in the Canova-Hansen
test.

#### Revision history

-   link to implementation in GUI
-   add implemenation in R ?
-   add illustrations ? (gui graphs ? estp training)

Revisions are calculated as differences between the first (earliest)
adjustment of an observation at time $t$, computed when this observation
is the last observation of the time series (*concurrent adjustment,*
denoted as $A_{t|t}$) and a later adjustment based on all future data
available at the time of the diagnostic analysis (*the* *most recent*
*adjustment,* denoted as $A_{t|N}$).

In the case of the multiplicative decomposition the revision history of
the seasonal adjustment from time $N_{0}\$to $N_{1}$ is a sequence of
$R_{t|N}^{A}$ calculated in the following way :

$$
  R_{t|N}^{A} = 100 \times \frac{A_{t|N} - A_{t|t}}{A_{t|t}}
  $$

The revision history of the trend is computed in the same manner.

With an additive decomposition $R_{t|N}^{A}$ is calculated in the same
way if all values $A_{t|t}$ have the same sign. Otherwise differences
are calculated as:

$$
  R_{t|N}^{A} = A_{t|N} - A_{t|t}
  $$

The analogous expression for the trend component is:

$$
  R_{t|N}^{T} = T_{t|N} - T_{t|t}
  $$

Revision in the period-to-period (month-on-month or quarter-to-quarter)
change in the seasonally adjusted series at time $t$ calculated from the
series $y_{1},y_{2},\ldots y_{n}$ is defined as:

$$
  R_{t}^{A} = C_{t|N} - C_{t|t}
  $$

where $$
\text{C}_{t|n}^{A} = \frac{A_{t|n} - A_{t - 1|n}}{A_{t - 1|n}}
$$.

Revisions for the period-to-period changes in the trend component are
computed in the same manner.

### Sliding spans

The sliding spans technique involves the comparison of the correlated
seasonal adjustments of a given period obtained by applying the
adjustment procedure to a sequence of two, three or four overlapping
spans of data, all of which contain this period (month or
quarter)[^seasonal-adjustment-47].

[^seasonal-adjustment-47]: FINDLEY, D., MONSELL, B.C., SHULMAN, H.B.,
    and PUGH, M.G. (1990).

Each period that belongs to more than one span is examined to see if its
seasonal adjustments vary more than a specified amount across the
spans[^seasonal-adjustment-48]. For the multiplicative decomposition a
seasonal factor is regarded to be unreliable if the following condition
is fulfilled:

[^seasonal-adjustment-48]: FINDLEY, D., MONSELL, B.C., BELL, W., OTTO,
    M., and CHEN, B.-C. (1990).

$$
  S_{t}^{\max} = \frac{\max_{k \in N_{t}}S_{t}\left( k \right) - \min_{k \in N_{t}}S_{t}(k)}{\min_{k \in N_{t}}S_{t}(k)} > 0.03
  $$

where:

$S_{t}(k)$ -- the seasonal factor estimated from span $k$ for month
(quarter) $t$;

$N_{t}$ -- {$\text{k}$: month (quarter) $\text{t}$ is in the $k$-th
span}.

For the additive decomposition JDemetra+ uses the rule in equation [2]
for checking for the reliability of the seasonal factor.

$$
  S_{t}^{\max} = \frac{\max_{k \in N_{t}}S_{t}\left( k \right) - \min_{k \in N_{t}}S_{t}(k)}{\sqrt{\frac{\sum_{i}^{n}y_{i}^{2}}{n}}} > 0.03
  $$

where:

$n$ -- number of observations of the orginal time series $y_{i}$.

The month-to-month percentage change in the seasonally adjusted value
from span $k$ for month $t$ is calculated as:

$$
 \text{MM}_{m}\left(k\right) = \frac{A_{m}\left(k\right) - A_{m - 1}\left(k\right)}{A_{m - 1}\left(k\right)}
 $$

where:

$A_{m}\left( k \right)$ -- the seasonally (and trading day) adjusted
value from span $k$ for month $t$;

$\text{MM}_{m}\left( k \right)$ is considered unreliable if the
statistics below is higher than 0.03.

$$
\text{MM}_{m}^{\max} = \max_{k \in {N1}_{m}}\text{MM}_{m}\left( k \right) - \min_{k \in {N1}_{m}}\text{MM}_{m}\left( k \right) > 0.03
$$

where:

${N1}_{t}$ -- {$\text{k}$: month $\text{t}$ and $t$-1 are in the $k$-th
span}.

The respective formula for the quarter-to-quarter percentage change in
the seasonally adjusted value from span $k$ for quarter $t$ is
calculated as:

$$
\text{QQ}_{q}\left( k \right) = \frac{A_{q}\left( k \right) - A_{q - 1}\left( k \right)}{A_{q - 1}\left( k \right)}
$$

where:

$A_{q}\left( k \right)$ -- the seasonally (and trading day) adjusted
value from span $k$ for quarter $q$.

$\text{QQ}_{q}\left( k \right)$ is considered unreliable if the
statistics below is higher than 0.03.

$$
\text{QQ}_{q}^{\max} = \max_{k \in {N1}_{q}}\text{QQ}_{q}\left( k \right) - \min_{k \in {N1}_{t}}\text{QQ}_{q}\left( k \right) > 0.03
$$

where:

${N1}_{q}$ -- {$\text{k}$: quarter $\text{t}$ and $t$-1 are in the
$k$-th span}.

The respective diagnostic can be also performed for the trading
days/working days component.

### Revision policies

-   harmonize voc (cf. cruncher vignette) and guidelines pb
-   add controlled current
-   add bbk plugin
-   replace descp by my pdf ? (how to allow modif in the project ?)
    (auxiliary file)

A description of the options is presented in the following table.

| **Option**                                                                          | **Description**                                                                                                                                                                                                                                                                                             |
|------------------|------------------------------------------------------|
| Partial concurrent adjustment → Fixed model                                         | The ARIMA model, outliers and other regression parameters are not re-identified and the values of all parameters are fixed. The transformation type remains unchanged.                                                                                                                                      |
| Partial concurrent adjustment → Estimate regression coefficients                    | The ARIMA model, outliers and other regression parameters are not re-identified. The coefficients of the ARIMA model are fixed, other coefficients are re-estimated. The transformation type remains unchanged.                                                                                             |
| Partial concurrent adjustment → Estimate regression coefficients + Arima parameters | The ARIMA model, outliers and other regression parameters are not re-identified. All parameters of the RegARIMA model are re-estimated. The transformation type remains unchanged.                                                                                                                          |
| Partial concurrent adjustment → Estimate regression coefficients + Last outliers    | The ARIMA model, outliers (except from the outliers in the last year of the sample) and other regression parameters are not re-identified. All parameters of the RegARIMA model are re-estimated. The outliers in the last year of the sample are re-identified. The transformation type remains unchanged. |
| Partial concurrent adjustment → Estimate regression coefficients + all outliers     | The ARIMA model and regression parameters, except from outliers) are not re-identified. All parameters of the RegARIMA model are re-estimated. All outliers are re-identified. The transformation type remains unchanged.                                                                                   |
| Partial concurrent adjustment → Estimate regression coefficients + Arima model      | Re-identification of the ARIMA model, outliers and regression variables, except from the calendar variables. The transformation type remains unchanged.                                                                                                                                                     |
| Concurrent                                                                          | Re-identification of the whole RegARIMA model.                                                                                                                                                                                                                                                              |

##### **Partial concurrent adjustment**

According to the *ESS Guidelines on Seasonal Adjustment* (2015), partial
concurrent adjustment is the strategy in which the model, filters,
outliers and calendar regressors are re-identified once a year and the
respective parameters and factors re-estimated every time new or revised
data become available. JDemetra+ offers several types of partial
concurrent adjustment.

### Practical steps

#### Graphical analysis recommendations

(edit) (link to GUI graphical capabilities, graphs with rjd) The Tools
menu includes, among other functionalities, tools that are helpful for
the graphical analysis of a time series. The [*'X-13ARIMA-SEATS
Reference Manual'
(2015)*](https://www.census.gov/ts/x13as/docX13ASHTML.pdf) strongly
recommends studying a high resolution plot of the time series as it is
helpful to gain insight on issues, such as, seasonal patterns, potential
outliers and stochastic non-stationarity. Also the *'ESS Guidelines on
Seasonal Adjustment' (2015)* recommends carrying out a graphical
analysis on both unadjusted data and the initial run of the seasonal
adjustment. Graphical analysis should consider: \* The length of the
series and the model span;\
\* The presence of zeros, outliers or problems in the data;\
\* The structure of the series: presence of; long term and cyclical
movements, seasonal components, volatility etc.;\
\* The presence of possible breaks in the seasonal behaviour;\
\* The decomposition scheme (additive, multiplicative).

The [*'ESS Guidelines on Seasonal Adjustment'
(2015)*](https://ec.europa.eu/eurostat/documents/3859598/6830795/KS-GQ-15-001-EN-N.pdf/d8f1e5f5-251b-4a69-93e3-079031b74bd3),
recommend that this exercise should be performed and documented for the
most important series to be adjusted at least once a year. The following
functionalities are available from the *Tools* menu:
