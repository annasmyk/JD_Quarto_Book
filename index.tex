% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}

\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\KOMAoption{captions}{tableheading}
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={JDemetra+ online documentation},
  pdfauthor={Stace documentation group},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{JDemetra+ online documentation}
\author{Stace documentation group}
\date{12/01/2022}

\begin{document}
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[interior hidden, borderline west={3pt}{0pt}{shadecolor}, boxrule=0pt, frame hidden, enhanced, breakable, sharp corners]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}
\addcontentsline{toc}{chapter}{Preface}

This is a Quarto book.

To learn more about Quarto books visit
\url{https://quarto.org/docs/books}.

\hypertarget{jdemetra-software}{%
\chapter{JDemetra+ Software}\label{jdemetra-software}}

\hypertarget{structure-of-this-book}{%
\section{Structure of this book}\label{structure-of-this-book}}

\hypertarget{available-algorithms}{%
\subsection{Available algorithms}\label{available-algorithms}}

\hypertarget{tools-to-access-this-algorithms}{%
\subsection{Tools to access this
algorithms}\label{tools-to-access-this-algorithms}}

\hypertarget{underlying-statistical-methods}{%
\subsection{Underlying Statistical
Methods}\label{underlying-statistical-methods}}

\hypertarget{audience}{%
\section{Audience}\label{audience}}

\hypertarget{how-jdemetra-came-to-be}{%
\section{How Jdemetra+ came to be}\label{how-jdemetra-came-to-be}}

history of the project .

\hypertarget{quick-start-with}{%
\chapter{Quick start with\ldots{}}\label{quick-start-with}}

objective: describe key steps + provide useful liks to relevant code

\hypertarget{seasonal-adjustment}{%
\section{Seasonal Adjustment}\label{seasonal-adjustment}}

\hypertarget{seasonal-adjustment-of-high-frequency-data}{%
\section{Seasonal Adjustment of High-Frequency
Data}\label{seasonal-adjustment-of-high-frequency-data}}

\hypertarget{use-of-jd-algorithms-in-r}{%
\section{Use of JD+ algorithms in R}\label{use-of-jd-algorithms-in-r}}

\hypertarget{use-of-jd-graphical-interface}{%
\section{Use of JD+ graphical
interface}\label{use-of-jd-graphical-interface}}

\hypertarget{main-functions-overview}{%
\chapter{Main functions overview}\label{main-functions-overview}}

link to key references * 2 handbooks * sets of guidelines

Objective: present JDemetra+ capabilities by category

\hypertarget{seasonal-adjustment-algorithms}{%
\section{Seasonal adjustment
algorithms}\label{seasonal-adjustment-algorithms}}

\emph{below: pieces of old pages to edit and update}

\hypertarget{data-frequencies}{%
\subsection{Data frequencies}\label{data-frequencies}}

The seasonal adjustment methods available in JDemetra+ aim to decompose
a time series into components and remove seasonal fluctuations from the
observed time series. The X-11 method considers monthly and quarterly
series while SEATS is able to decompose series with 2, 3, 4, 6 and 12
observations per year.

\hypertarget{x-13}{%
\subsection{X-13}\label{x-13}}

X-13ARIMA is a seasonal adjustment program developed and supported by
the U.S. Census Bureau. It is based on the U.S. Census Bureau's earlier
X-11 program, the X-11-ARIMA program developed at Statistics Canada, the
X-12-ARIMA program developed by the U.S. Census Bureau, and the SEATS
program developed at the Banco de España. The program is now used by the
U.S. Census Bureau for a seasonal adjustment of time series.

\hypertarget{tramo-seats}{%
\subsection{Tramo-Seats}\label{tramo-seats}}

\hypertarget{stl}{%
\subsection{STL}\label{stl}}

\hypertarget{basic-structural-models}{%
\subsection{Basic Structural Models}\label{basic-structural-models}}

\hypertarget{trend-cycle-estimation}{%
\section{Trend-cycle estimation}\label{trend-cycle-estimation}}

\hypertarget{nowacsting}{%
\section{Nowacsting}\label{nowacsting}}

\hypertarget{temporal-disaggregation}{%
\section{Temporal Disaggregation}\label{temporal-disaggregation}}

\hypertarget{seasonal-adjustment-1}{%
\chapter{Seasonal Adjustment}\label{seasonal-adjustment-1}}

\hypertarget{motivation}{%
\section{Motivation}\label{motivation}}

The primary aim of the seasonal adjustment process is to remove seasonal
fluctuations from the time series. To achieve this goal, seasonal
adjustment methods decompose the original time series into components
that capture specific movements. These components are: trend-cycle,
seasonality and irregularity. The trend-cycle component includes
long-term and medium-term movements in the data. For seasonal adjustment
purposes there is no need to divide this component into two parts.
JDemetra+ refers to the trend-cycle as trend and consequently this
convention is used here.

This section presents the options of the seasonal adjustment processes
performed by the methods implemented in JDemetra+
(X-12-ARIMA/X-13ARIMA-SEATS and TRAMO/SEATS) and discusses the output
displayed by JDemetra+. As these seasonal adjustment methods use
different approach to the decomposition, the output produced for both of
them has different structure and content. Therefore, the results for
both methods are discussed separately. However, in contrast to the
original programs, in JDemetra+ some quality indicators have been
implemented for both methods, allowing for an easier compaison of the
results.

\hypertarget{unobserved-components-uc}{%
\section{Unobserved Components (UC)}\label{unobserved-components-uc}}

The main components, each representing the impact of certain types of
phenomena on the time series (\(X_{t}\)), are:

\begin{itemize}
\item
  The trend (\(T_{t}\)) that captures long-term and medium-term
  behaviour;
\item
  The seasonal component (\(S_{t}\)) representing intra-year
  fluctuations, monthly or quarterly, that are repeated more or less
  regularly year after year;
\item
  The irregular component (\(I_{t}\)) combining all the other more or
  less erratic fluctuations not covered by the previous components.
\end{itemize}

In general, the trend consists of 2 sub-components:

\begin{itemize}
\item
  The long-term evolution of the series;
\item
  The cycle, that represents the smooth, almost periodic movement around
  the long-term evolution of the series. It reveals a succession of
  phases of growth and recession.
\end{itemize}

For seasonal adjustment purposes both TRAMO-SEATS and X-13ARIMA-SEATS do
not separate the long-term trend from the cycle as these two components
are usually too short to perform their reliable estimation.
Consequently, hereafter TRAMO-SEATS and X-13ARIMA-SEATS estimate the
trend component. However, the original TRAMO-SEATS may separate the
long-term trend from the cycle through the Hodrick-Precsott filter using
the output of the standard decomposition. It should be remembered that
JDemetra+ refers to the trend-cycle as trend (\(T_{t}\)), and
consequently this convention is used in this document.

TRAMO-SEATS considers two decomposition models:

\begin{itemize}
\item
  The additive model: \(X_{t} = T_{t} + S_{t} + I_{t}\);
\item
  The log additive model:
  \(log(X_{t}) = log(T_{t}) + log(S_{t}) + log(I_{t})\).
\end{itemize}

Apart from these two decomposition types X-13ARIMA-SEATS allows the user
to apply also the multiplicative model:
\(X_{t} = T_{t} \times S_{t} \times I_{t}\).

A time series \(x_{t}\), which is a subject to a decomposition, is
assumed to be a realisation of a discrete-time stochastic,
covariance-stationary linear process, which is a collection of random
variables \(x_{t}\), where \(t\) denotes time. It can be shown that any
stochastic, covariance-stationary process can be presented in the form:

\(x_{t} = \mu_{t} + {\widetilde{x}}_{t}\), \[1\]

where \(\mu_{t}\) is a linearly deterministic component and
\({\widetilde{x}}_{t}\) is a linearly interderministic component, such
as:

\$\$ \{\widetilde{x}\}\emph{\{t\} = \{\sum}\{j =
0\}\^{}\{\infty\}\psi\emph{\{j\}a\}}\{t - j\}

\[, \]2\$\$

where \(\sum_{j = 0}^{\infty}\psi_{i}^{2} < \infty\) (coefficients
\(\psi_{j}\) are absolutely summable), \(\psi_{0} = 1\) and \(a_{t}\) is
the white noise error with zero mean and constant variance \(V_{a}\).
The error term \(a_{t}\) represents the one-period ahead forecast error
of \(x_{t}\), that is:

\$\$

a\_\{t\} = \{\widetilde{x}\}\emph{\{t\} - \{\widehat{x}\}}\{t\textbar t
- 1\}

\[, \]3\$\$

where \[{\widehat{x}}_{t|t - 1}\] is the forecast of

\[{\widetilde{x}}_{t}\] made at period \(t - 1\). As \(a_{t}\)
represents what is new in \[{\widetilde{x}}_{t}\] in point \(t\), i.e.,
not contained in the past values of \[{\widetilde{x}}_{t}\], it is also
called innovation of the process. From \[3\] \[{\widetilde{x}}_{t}\] can
be viewed as a linear filter applied to the innovations.

The equation 7.1 is called a Wold representation. It presents a process
as a sum of linearly deterministic component \(\mu_{t}\) and linearly
interderministic component \(\sum_{j = 0}^{\infty}\psi_{j}a_{t - j}\),
the first one is perfectly predictable once the history of the process
\(x_{t - 1}\) is known and the second one is impossible to predict
perfectly. This explains why the stochastic process cannot be perfectly
predicted.

Under suitable conditions \[{\widetilde{x}}_{t}\] can be presented as a
weighted sum of its past values and \(a_{t}\), i.e.:

\$\$

\{ \{\widetilde{x}\}\emph{\{t\} = \sum}\{j =
0\}\^{}\{\infty\}\pi\emph{\{j\}\{\widetilde{x}\}}\{t - j\} + a\}\_\{t\}

\[, \]4\$\$

In general, for the observed time series, the assumptions concerning the
nature of the process \[1\] do not hold for various reasons. Firstly,
most observed time series display a mean that cannot be assumed to be
constant due to the presence of a trend and the seasonal movements.
Secondly, the variance of the time series may vary in time. Finally, the
observed time series usually contain outliers, calendar effects and
regression effects, which are treated as deterministic. Therefore, in
practice a prior transformation and an adjustment need to be applied to
the time series. The constant variance is usually achieved through
taking a logarithmic transformation and the correction for the
deterministic effects, while stationarity of the mean is achieved by
applying regular and seasonal differencing. These processes, jointly
referred to as preadjustment or linearization, can be performed with the
TRAMO or RegARIMA models. Besides the linearisation, forecasts and
backcasts of stochastic time series are estimated with the ARIMA model,
allowing for later application of linear filters at both ends of time
series. The estimation performed with these models delivers the
stochastic part of the time series, called the linearised series, which
is assumed to be an output of a linear stochastic process.\footnote{When
  the series are non-stationary differentiation is performed before the
  seasonality tests.} The deterministic effects are removed from the
time series and used to form the final components.

In the next step the linearised series is decomposed into its
components. There is a fundamental difference in how this process is
performed in TRAMO-SEATS and X-13ARIMA-SEATS. In TRAMO-SEATS the
decomposition is performed by the SEATS procedure, which follows a so
called ARIMA model based approach. In principle, it aims to derive the
components with statistical models. More information is given in the
\href{../theory/SA_SEATS.html}{SEATS} section. X-13ARIMA-SEATS offers
two algorithms for decomposition: SEATS and X-11. The X-11 algorithm,
which is described in the \href{../theory/SA_X11.html}{X-11 section}
section, decomposes a series by means of linear filters. Finally, in
both methods the final components are derived by the assignment of the
deterministic effects to the stochastic components. Consequently, the
role of the ARIMA models is different in each method. TRAMO-SEATS
applies the ARIMA models both in the preadjustment step and in the
decomposition procedure. On the contrary, when the X-11 algorithm is
used for decomposition, X-13ARIMA-SEATS uses the ARIMA model only in the
preadjustment step. In summary, the decomposition procedure that results
in an estimation of the seasonal component requires prior identification
of the deterministic effects and their removal from the time series.
This is achieved through the linearisation process performed by the
TRAMO and the RegARIMA models, shortly discussed in the
\href{../theory/SA_lin.html}{Linearisation with the TRAMO and RegARIMA
models} section.The linearised series is then decomposed into the
stochastic components with \href{../theory/SA_SEATS.html}{SEATS} or
\href{../theory/SA_X11.html}{X-11} algorithms.

\hypertarget{seasonality-tests}{%
\section{Seasonality tests}\label{seasonality-tests}}

\hypertarget{overview}{%
\subsection{Overview}\label{overview}}

\hypertarget{f-test-on-seasonal-dummies}{%
\subsection{F-test on seasonal
dummies}\label{f-test-on-seasonal-dummies}}

The F-test on seasonal dummies checks for the presence of deterministic
seasonality. The model used here uses seasonal dummies (mean effect and
11 seasonal dummies for monthly data, mean effect and 3 for quarterly
data) to describe the (possibly transformed) time series behaviour. The
test statistic checks if the seasonal dummies are jointly statistically
not significant. When this hypothesis is rejected, it is assumed that
the deterministic seasonality is present and the test results are
displayed in green.

This test refers to Model-Based \$\chi\^{}\{2\}\$and F-tests for Fixed
Seasonal Effects proposed by LYTRAS, D.P., FELDPAUSCH, R.M., and BELL,
W.R. (2007) that is based on the estimates of the regression dummy
variables and the corresponding t-statistics of the RegARIMA model, in
which the ARIMA part of the model has a form (0,1,1)(0,0,0). The
consequences of a misspecification of a model are discussed in LYTRAS,
D.P., FELDPAUSCH, R.M., and BELL, W.R. (2007).

For a monthly time series the RegARIMA model structure is as follows:

\[\left( 1 - B \right)\left( y_{t} - \beta_{1}M_{1,t} - \ldots - \beta_{11}M_{11,t} - \gamma X_{t} \right) = \mu + (1 - B)a_{t}
\]

where:

\[
M_{j,t} =
\begin{cases}
1 & \text{ in month } j = 1, \ldots, 11 \\
- 1 & \text{ in December}\\
0 & \text{ otherwise}
\end{cases} \text{ - dummy variables;}
\]

\(y_{t}\) -- the original time series;

\(B\) -- a backshift operator;

\(X_{t}\) -- other regression variables used in the model
(e.g.~outliers, calendar effects, user-defined regression variables,
intervention variables);

\(\mu\) -- a mean effect;

\(a_{t}\) -- a white-noise variable with mean zero and a constant
variance.

In the case of a quarterly series the estimated model has a form:

\[\left( 1 - B \right)\left( y_{t} - \beta_{1}M_{1,t} - \ldots - \beta_{3}M_{3,t} - \gamma X_{t} \right) = \mu + (1 - B)a_{t}\],
\[2\]

where:

\[
M_{j,t} =
\begin{cases}
1 & \text{ in quarter} j = 1, \ldots, 3 \\
- 1 & \text{ in the fourth quarter}\\
0 & \text{ otherwise}
\end{cases} \text{ - dummy variables;}
\]

One can use the individual t-statistics to assess whether seasonality
for a given month is significant, or a chi-squared test statistic if the
null hypothesis is that the parameters are collectively all zero. The
chi-squared test statistic is
\({\widehat{\chi}}^{2} = {\widehat{\beta}}^{'}{\lbrack Var(\widehat{\beta})}^{\ })^{- 1}\rbrack{\widehat{\beta}}^{\ }\)
in this case compared to critical values from a
\(\chi^{2}\left( \text{df} \right)\)-distribution, with degrees of
freedom \$df = 11\$(monthly series) or \(df = 3\) (quarterly series).
Since the \({Var(\widehat{\beta})}^{\ }\) computed using the estimated
variance of \(\alpha_{t}\) may be very different from the actual
variance in small samples, this test is corrected using the proposed
\(\text{F}\) statistic:

\[
  F = \frac{ {\widehat{\chi}}^{2}}{s - 1} \times \frac{n - d - k}{n - d}
  \]\emph{,} \[3\]

where \(n\) is the sample size, \(d\) is the degree of differencing, s
is time series frequency (12 for a monthly series, 4 for a quarterly
series) and \(k\) is the total number of regressors in the RegARIMA
model (including the seasonal dummies \(\text{M}_{j,t}\) and the
intercept).

This statistic follows a \[F_{s - 1,n - d - k}\] distribution under the
null hypothesis.

\hypertarget{qs-test-on-autocorrelation-at-seasonal-lags}{%
\subsection{QS Test on autocorrelation at seasonal
lags}\label{qs-test-on-autocorrelation-at-seasonal-lags}}

The QS test is a variant of the
\href{../theory/Tests_LB.html}{Ljung-Box} test computed on seasonal
lags, where we only consider positive auto-correlations

More exactly,

\[ QS=n \left(n+2\right)\sum_{i=1}^k\frac{\left[ \max  \left(0, \hat\gamma_{i \cdot l}\right)\right]^2}{n-i \cdot l}\]

where \[k=2\], so only the first and second seasonal lags are
considered. Thus, the test would checks the correlation between the
actual observation and the observations lagged by one and two years.
Note that \[l=12\] when dealing with monthly observations, so we
consider the autocovariances \[\hat\gamma_{12}\] and \[\hat\gamma_{24}\]
alone. In turn, \[k=4\] in the case of quarterly data.

Under H0, which states that the data are independently distributed, the
statistics follows a \[\chi \left(k\right)\] distribution. However, the
elimination of negative correlations makes it a bad approximation. The
p-values would be given by \(P(\chi^{2}\left( k \right) > Q)\) for
\(k = 2\). As \({P(\chi}^{2}(2)) > 0.05 = 5.99146\) and
\({P(\chi}^{2}(2)) > 0.01 = 9.21034\), \(QS > 5.99146\) and
\(QS > 9.21034\) would suggest rejecting the null hypothesis at \(95\%\)
and \(99\%\) significance levels, respecively.

\hypertarget{modification}{%
\paragraph{Modification}\label{modification}}

Maravall (2012) proposes approximate the correct distribution (p-values)
of the QS statistic using simulation techniques. Using 1000K
replications of sample size 240, the correct critical values would be
3.83 and 7.09 with confidence levels of \(95\%\) and \(99\%\),
respectively (lower than the 5.99146 and 9.21034 shown above). For each
of the simulated series, he obtains the distribution by assuming
\(QS=0\) when \[\hat\gamma_{12}\], so in practice this test will detect
seasonality

only when any of these conditions hold: - Statistically significant
positive autocorrelation at lag 12 - Nonnegative sample autocorrelation
at lag 12 and statistically significant positive autocorrelation at lag
24

\hypertarget{implementation}{%
\subsubsection{Implementation}\label{implementation}}

\hypertarget{in-the-graphical-user-interface-gui}{%
\paragraph{In the graphical user interface
(GUI)}\label{in-the-graphical-user-interface-gui}}

The test can be applied directly to any series by selecting the option
\emph{Statistical Methods \textgreater\textgreater{} Seasonal Adjustment
\textgreater\textgreater{} Tools \textgreater\textgreater{} Seasonality
Tests}. This is an example of how results are displayed for the case of
a monthly series:

\begin{figure}

{\centering \includegraphics{./All_images/qs.png}

}

\caption{qs}

\end{figure}

The test can be applied to the input series before any seasonal
adjustment method has been applied. It can also be applied to the
seasonally adjusted series or to the irreguar component.

\hypertarget{via-r-package-rjd3toolkit-blank}{%
\paragraph{Via R package: RJD3toolkit
(blank)}\label{via-r-package-rjd3toolkit-blank}}

\hypertarget{java-library}{%
\paragraph{Java Library}\label{java-library}}

This test is implemented in the class
\texttt{ec.satoolkit.diagnostics.QsTest}

\hypertarget{references}{%
\paragraph{References}\label{references}}

\begin{itemize}
\tightlist
\item
  LJUNG G. M. and G. E. P. BOX (1978). ``On a Measure of a Lack of Fit
  in Time Series Models''. Biometrika 65 (2): 297--303.
  \url{doi:10.1093/biomet/65.2.297}
\item
  MARAVALL, A. (2011). ``Seasonality Tests and Automatic Model
  Identification in Tramo-Seats''. Manuscript
\item
  MARAVALL, A. (2012). ``Update of Seasonality Tests and Automatic Model
  Identification in TRAMO-SEATS''. Bank of Spain (November 2012)
\end{itemize}

\hypertarget{qs-test-for-seasonality-bis-solve-this}{%
\subsection{QS Test for seasonality (BIS : solve
this)}\label{qs-test-for-seasonality-bis-solve-this}}

More exactly,

\[ qs=n \left(n+2\right)\sum_{i=1}^k\frac{\left[ \max  \left(0, \hat\gamma_{i \cdot l}\right)\right]^2}{n-i \cdot l}\]

The current implementation still considers that the statistics is
distributed as a \[\chi \left(k\right)\] even if it is obvioulsly
incorrect.

\hypertarget{kurskall-wallis}{%
\subsection{Kurskall-Wallis}\label{kurskall-wallis}}

The Kruskal-Wallis test is a non-parametric test used for testing
whether samples originate from the same distribution. The parametric
equivalent of the Kruskal-Wallis test is the one-way analysis of
variance (ANOVA). When rejecting the null hypothesis of the
Kruskal-Wallis test, then at least one sample stochastically dominates
at least one other sample. The test does not identify where this
stochastic dominance occurs or for how many pairs of groups stochastic
dominance obtains. The null hypothesis states that all months (or
quarters, respectively) have the same mean. Under this hypothesis the
test statistic follows a \[ \chi^2 \] distribution. When this hypothesis
is rejected, it is assumed that time series values differ significantly
between periods and the test results are displayed in green

The test is typically applied to \[ k  \] groups of data

\[ \left\{x_{i}\right\}_{j} \]. Each group \[ j=1,…,k \] is composed of
\[ n_j \] observations, which are indexed by \[ i=1,…,n_j \]. Each month

(or quarter) groups all the observations available for a certain number
of years.

As opposed to the notation used in the
\href{../theory/Tests_Friedman.html}{Friedman test}, number of
observations here is not necessarily equal for each group. The ranking
of each data point, represented by variable \[ r_{ij} \]., is now
defined different than in Friedman test, since it considers all
observables

\[ N=n_1+ \dots + n_g \], thereby ignoring group membership.

The test statistic is given by

\$\$

Q=\frac{SS_t}{SS_e}

\$\$

where \[ SS_t=(N-1)\sum_{j=1}^{g}n_i(\bar{r}_{.j}-\bar{r})^2 \] and

\[ SS_e=\sum_{j=1}^{g}\sum_{i=1}^{n_j}(r_{ij}-\bar{r})^2 \] - \[ n_j \]

is the number of observations in group \[ j  \] - \[ \bar{r}_{.j} \] is
the average of the absolute ranks of the data in group \[ j  \] - The
average rank is \[ \bar{r} =\frac{1}{2}(N+1) \]

Under the null hypothesis that all groups are generated from the same
distribution, the test statistic Q is approximated by a chi-squared
distribution. Thus, the p-value is given by \[ P( \chi^2_{g-1}>Q) \].
This approximation can be misleading if some of the groups are very
small (i.e.~less than five elements). If the statistic is not
significant, then there is no evidence of stochastic dominance between
the samples. However, if the test is significant then at least one
sample
\href{http://en.wikipedia.org/wiki/Stochastic_dominance}{stochastically
dominates} another sample.

\hypertarget{use}{%
\subsubsection{Use}\label{use}}

The test can be applied directly to any series by selecting the option
\emph{Statistical Methods \textgreater\textgreater{} Seasonal Adjustment
\textgreater\textgreater{} Tools \textgreater\textgreater{} Seasonality
Tests}. This is an example of how results are displayed for the case of
a monthly series:

\begin{figure}

{\centering \includegraphics{./All_images/kw.png}

}

\caption{kwResults}

\end{figure}

The test can be applied to the input series before any seasonal
adjustment method has been applied. It can also be applied to the
seasonally adjusted series or to the irreguar component.

\hypertarget{implementation-1}{%
\subsubsection{Implementation}\label{implementation-1}}

This test is implemented in the class
\texttt{ec.satoolkit.diagnostics.KruskallWallisTest}

\hypertarget{references-1}{%
\subsubsection{References}\label{references-1}}

\begin{itemize}
\tightlist
\item
  Kruskal; Wallis (1952). ``Use of ranks in one-criterion variance
  analysis''. Journal of the American Statistical Association 47 (260):
  583--621. \url{doi:10.1080/01621459.1952.10483441}.
\end{itemize}

\hypertarget{friedman-test-stable-seasonality-test}{%
\subsection{Friedman test (stable seasonality
test)}\label{friedman-test-stable-seasonality-test}}

The Friedman test is a non-parametric method for testing that samples
are drawn from the same population or from populations with equal
medians. The significance of the month (or quarter) effect is tested.
The Friedman test requires no distributional assumptions. It uses the
rankings of the observations. If the null hypothesis of no stable
seasonality is rejected at the 0.10\% significance level then the series
is considered to be seasonal and the test's outcome is displayed in
green.

The test statistic is constructed as follows. Consider first the matrix
of data \[ \left\{x_{ij}\right\}_{n \times k} \] with \[ n \] rows (the
blocks, i.e.~number of years in the sample), \[ k \] columns (the
treatments, i.e.~either 12 months or 4 quarters, depending on the
frequency of the data).\\
The data matrix needs to be replaced by a new matrix

\[ \left\{r_{ij}\right\}_{n \times k} \], where the entry \[ r_{ij} \]

is the rank of \[ x_{ij} \] within block \[ i \] .

The test statistic is given by

\$\$

Q=\frac{SS_t}{SS_e}

\$\$

where \[ SS_t=n \sum_{j=1}^{k}(\bar{r}_{.j}-\bar{r})^2 \] and

\[ SS_e=\frac{1}{n(k-1)} \sum_{i=1}^{n}\sum_{j=1}^{k}(r_{ij}-\bar{r})^2 \]

It represents the variance of the average ranking across treatments j
relative to the total.

Under the hypothesis of no seasonality, all months can be equally
treated. For the sake of completeness: - \[ \bar{r}_{.j} \] is the
average ranks of each treatment (month) j within each block (year) - The
average rank is given by

\[ \bar{r}= \frac{1}{nk}\sum_{i=1}^{n}\sum_{j=1}^{k}(r_{ij})\]

For large \[ n \] or \[ k \] , i.e.~n \textgreater{} 15 or k
\textgreater{} 4, the probability distribution of \[ Q \] can be
approximated by that of a chi-squared distribution. Thus, the p-value is
given by \[ P( \chi^2_{k-1}>Q) \] .

\hypertarget{use-1}{%
\subsubsection{Use}\label{use-1}}

The test can be applied directly to any series by selecting the option
\emph{Statistical Methods \textgreater\textgreater{} Seasonal Adjustment
\textgreater\textgreater{} Tools \textgreater\textgreater{} Seasonality
Tests}. This is an example of how results are displayed for the case of
a monthly series:

\begin{figure}

{\centering \includegraphics{./All_images/friedman.png}

}

\caption{friedman}

\end{figure}

If the null hypothesis of no stable seasonality is rejected at the 1\%
significance level, then the series is considered to be seasonal and the
outcome of the test is displayed in green.

The test can be applied to the input series before any seasonal
adjustment method has been applied. It can also be applied to the
seasonally adjusted series or to the irreguar component. In the case of
X-13ARIMA-SEATS, the test is applied to the preliminary estimate of the
unmodified Seasonal-Irregular component\footnote{The unmodified
  Seasonal-Irregular component corresponds to the Seasonal-Irregular
  factors with the extreme values.} (time series shown in Table B3). In
this estimate, the number of observations is lower than in the final
estimate of the unmodified Seasonal-Irregular component. Thus, the
number of degrees of freedom in the stable seasonality test is lower
than the number of degrees of freedom in the test for the
\href{../theory/Tests_presence_stability.html}{presence of seasonality
assuming stability}. For example, X-13ARIMA-SEATS uses a centred moving
average of order 12 to calculate the preliminary estimation of trend.
Consequently, the first six and last six points in the series are not
computed at this stage of calculation. The preliminary estimation of the
trend is then used for the calculation of the preliminary estimation of
the unmodified Seasonal-Irregular.

\hypertarget{related-tests}{%
\subsubsection{Related tests}\label{related-tests}}

\begin{itemize}
\tightlist
\item
  When using this kind of design for a binary response, one instead uses
  the Cochran's Q test.
\item
  Kendall's W is a normalization of the Friedman statistic between 0 and
  1.
\item
  The Wilcoxon signed-rank test is a nonparametric test of
  non-independent data from only two groups.
\end{itemize}

\hypertarget{implementation-2}{%
\subsubsection{Implementation}\label{implementation-2}}

This test is implemented in the class
\texttt{ec.satoolkit.diagnostics.FriedmanTest}

\hypertarget{references-2}{%
\subsubsection{References}\label{references-2}}

\begin{itemize}
\item
  Friedman, Milton (December 1937). ``The use of ranks to avoid the
  assumption of normality implicit in the analysis of variance''.
  Journal of the American Statistical Association (American Statistical
  Association) 32 (200): 675--701. \url{doi:10.2307/2279372}. JSTOR
  2279372.
\item
  Friedman, Milton (March 1939). ``A correction: The use of ranks to
  avoid the assumption of normality implicit in the analysis of
  variance''. Journal of the American Statistical Association (American
  Statistical Association) 34 (205): 109. \url{doi:10.2307/2279169}.
  JSTOR

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \setcounter{enumi}{2279168}
  \tightlist
  \item
  \end{enumerate}
\item
  Friedman, Milton (March 1940). ``A comparison of alternative tests of
  significance for the problem of m rankings''. The Annals of
  Mathematical Statistics 11 (1): 86--92.
  \url{doi:10.1214/aoms/1177731944}. JSTOR 2235971.
\end{itemize}

\hypertarget{stable-seasonality-test-missing}{%
\subsection{Stable seasonality test
(missing)}\label{stable-seasonality-test-missing}}

\hypertarget{moving-seasonality-test}{%
\subsection{Moving seasonality test}\label{moving-seasonality-test}}

The evolutive seasonality test is based on a two-way analysis of
variance model. The model uses the values from complete years only.
Depending on the decomposition type for the Seasonal -- Irregular
component it uses \[1\] (in the case of a multiplicative model) or \[2\]
(in the case of an additive model):

\$\$

\left\textbar{}\text{SI}\emph{\{\text{ij}\} - 1 \right\textbar{} =
X}\{\text{ij}\} = b\_\{i\} + m\_\{j\} + e\_\{\text{ij}\} \[, \]1\$\$

\[
  \left| \text{SI}_{\text{ij}} \right| = X_{\text{ij}} = b_{i} + m_{j} + e_{\text{ij}}
  \], \[2\]

where:

\(m_{j}\) -- the monthly or quarterly effect for \(j\)-th period,
\(j = (1,\ldots,k)\), where \(k = 12\) for a monthly series and
\(k = 4\) for a quarterly series;

\(b_{j}\) -- the annual effect \(i\), \((i = 1,\ldots,N)\) where \(N\)
is the number of complete years;

\(e_{\text{ij}}\) -- the residual effect.

The test is based on the following decomposition:

\[S^{2} = S_{A}^{2} + S_{B}^{2} + S_{R}^{2},\] \[3\]

where:

\$\$

S\^{}\{2\} = \sum\emph{\{j = 1\}\^{}\{k\}\{\sum}\{i = 1\}\^{}\{N\}\left(
\{\overline{X}\}\emph{\{\text{ij}\} -
\{\overline{X}\}}\{\bullet \bullet\} \right)\^{}\{2\}\}~

\$\$ --the total sum of squares;

\$\$

S\_\{A\}\^{}\{2\} = N\sum\emph{\{j = 1\}\^{}\{k\}\left(
\{\overline{X}\}}\{\bullet j\} - \{\overline{X}\}\_\{\bullet \bullet\}
\right)\^{}\{2\}

\$\$ -- the inter-month (inter-quarter, respectively) sum of squares,
which mainly measures the magnitude of the seasonality;

\$\$

S\_\{B\}\^{}\{2\} = k\sum\emph{\{i = 1\}\^{}\{N\}\left(
\{\overline{X}\}}\{i \bullet\} - \{\overline{X}\}\_\{\bullet \bullet\}
\right)\^{}\{2\}

\$\$ -- the inter-year sum of squares, which mainly measures the
year-to-year movement of seasonality;

\$\$

S\_\{R\}\^{}\{2\} = \sum\emph{\{i = 1\}\^{}\{N\}\{\sum}\{j =
1\}\^{}\{k\}\left( \{\overline{X}\}\emph{\{\text{ij}\} -
\{\overline{X}\}}\{i \bullet\} - \{\overline{X}\}\emph{\{\bullet j\} -
\{\overline{X}\}}\{\bullet \bullet\} \right)\^{}\{2\}\}

\$\$ -- the residual sum of squares.

The null hypothesis \$H\_\{0\}\$is that \(b_{1} = b_{2} = ... = b_{N}\)
which means that there is no change in seasonality over the years. This
hypothesis is verified by the following test statistic:

\$\$

F\_\{M\} =
\frac{\frac{S_{B}^{2}}{(n - 1)}}{\frac{S_{R}^{2}}{(n - 1)(k - 1)}}
\[, \]4\$\$

which follows an \(F\)-distribution with \(k - 1\) and \(n - k\) degrees
of freedom.

\hypertarget{identifiable-seasonality}{%
\subsection{Identifiable seasonality}\label{identifiable-seasonality}}

This test combines the values of the \(F\)-statistic of the parametric
test for stable seasonality and the values of the moving seasonality
test, which was described above.

The test statistic is:

\[
  T = \left( \frac{\frac{7}{F_{S}} + \frac{3F_{M}}{F_{S}}}{2} \right)^{\frac{1}{2}}
  \], \[1\]

where \(F_{S}\) is a stable seasonality test statistic and \(F_{M}\) is
moving seasonality test statistic. The test checks if the stable
seasonality is not dominated by moving seasonality. In such a case the
seasonality is regarded as identifiable. This test statistic is used in
the combined seasonality tests (see section
\href{../theory/Tests_combined.html}{Combined seasonality test}. The
detailed description of the test is available in LOTHIAN, J., and MORRY,
M. (1978).

\hypertarget{combined-seasonality-test}{%
\subsection{Combined seasonality test}\label{combined-seasonality-test}}

This test combines the Kruskal-Wallis test along with test for the
presence of seasonality assuming stability (\(F_{S}\)), and evaluative
seasonality test for detecting the presence of identifiable seasonality
(\(F_{M}\)). Those three tests are calculated using the final unmodified
SI component. The main purpose of the combined seasonality test is to
check whether the seasonality of the series is identifiable. For
example, the identification of the seasonal pattern is problematic if
the process is dominated by highly moving seasonality\footnote{DAGUM,
  E.B. (1987).}. The testing procedure is shown in the figure below.

\begin{figure}

{\centering \includegraphics{./All_images/UG_A_image18.png}

}

\caption{Text}

\end{figure}

\textbf{Combined seasonality test, source: LADIRAY, D., QUENNEVILLE, B.
(2001)}

\hypertarget{spectral-analysis}{%
\subsection{Spectral analysis}\label{spectral-analysis}}

In order to decide whether a series has a seasonal component that is
predictable (stable) enough, these tests use visual criteria and formal
tests for the periodogram. The periodogram is calculated using complete
years, so that the set of Fourier frequencies contains exactly all
seasonal frequencies\footnote{For definition of the periodogram and
  Fourier frequencies see section
  \href{../theory/spectral.html}{Spectral Analysis}}.

The tests rely on two basic principles:

\begin{itemize}
\item
  The peaks associated with seasonal frequencies should be larger than
  \textgreater{} the median spectrum for all frequencies and;
\item
  The peaks should exceed the spectrum of the two adjacent values by
  \textgreater{} more than a critical value.
\end{itemize}

\begin{quote}
JDemetra+ performs this test on the original series. If these two
requirements are met, the test results are displayed in green. The
statistical significance of each of the seasonal peaks (i.e. frequencies
\$\frac{\pi}{6},~\frac{\pi}{3},~\frac{\pi}{2},~\frac{2\pi}{3}\text{ and }
\frac{5\pi}{6}\$corresponding to 1, 2, 3, 4 and 5 cycles per year) is
also displayed. The seasonal and trading days frequencies depends on the
frequency of time series. They are shown in the table below. The symbol
\(d\) denotes a default frequency and is described below the table.
\end{quote}

\textbf{The seasonal and trading day frequencies by time series
frequency}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2466}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5068}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2466}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Number of months per full period}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Seasonal frequency}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Trading day frequency (radians)}
\end{minipage} \\
\midrule()
\endhead
12 &
\(\frac{\pi}{6},\frac{\pi}{3},\ \frac{\pi}{2},\frac{2\pi}{3},\ \frac{5\pi}{6},\ \pi\)
& \(d\), 2.714 \\
6 & \(\frac{\pi}{3},\frac{2\pi}{3}\), \(\pi\) & \(d\) \\
4 & \(\frac{\pi}{2}\), \(\pi\) & \(d\), 1.292, 1.850, 2.128 \\
3 & \(\pi\) & \(d\) \\
2 & \(\pi\) & \(d\) \\
\bottomrule()
\end{longtable}

The calendar (trading day or working day) effects, related to the
variation in the number of different days of the week per period, can
induce periodic patterns in the data that can be similar to those
resulting from pure seasonal effects. From the theoretical point of
view, trading day variability is mainly due to the fact that the average
number of days in the months or quarters is not equal to a multiple of
\(7\) (the average number of days of a month in the year of
\$365.25\$days is equal to \(\frac{365.25}{12} = 30.4375\) days). This
effect occurs \(\frac{365.25}{12} \times \frac{1}{7} = 4.3482\) times
per month: one time for each one of the four complete weeks of each
month, and a residual of \(0.3482\) cycles per month, i.e.
\(0.3482 \times 2\pi = 2.1878\ radians\). This turns out to be a
fundamental frequency for the effects associated with monthly data. In
JDemetra+ the fundamental frequency corresponding to \(0.3482\) cycles
per month is used in place of the closest
frequency\(\ \frac{\text{πk}}{60}\). Thus, the quantity
\(\frac{\pi \times 42}{60}\) is replaced by
\[\omega_{42} = 0.3482 \times 2\pi = 2.1878\]. The frequencies

neighbouring \(\omega_{42}\), i.e.~\[\omega_{41}\] and \[\omega_{43}\]
are set to, respectively, \[2.1865 - \frac{1}{60}\] and

\[2.1865 + \frac{1}{60}\].

The default frequencies (\$d)\$for calendar effect are: 2.188 (monthly
series) and 0.280 (quarterly series). They are computed as:

\(\omega_{\text{ce}} = \frac{2\pi}{7}\left( n - 7 \times \left\lbrack \frac{n}{7} \right\rbrack \right)\),
\[1\] where:

\(n = \frac{365.25}{s}\), \(s = 4\) for quarterly series and \(s = 12\)
for monthly series.

Other frequencies that correspond to trading day frequencies are: 2.714
(monthly series) and 1.292, 1.850, 2.128 (quarterly series).

In particular, the calendar frequency in monthly data (marked in red on
the figure below) is very close to the seasonal frequency corresponding
to 4 cycles per year \(\text{ω}_{40} = \frac{2}{3}\pi = 2.0944\).

\begin{figure}

{\centering \includegraphics{./All_images/UG_A_image19.png}

}

\caption{Text}

\end{figure}

\textbf{Periodogram with seasonal (grey) and calendar (red) frequencies
highlighted}

This implies that it may be hard to disentangle both effects using the
frequency domain techniques.

\hypertarget{defining-a-f-test}{%
\subsubsection{Defining a F-test}\label{defining-a-f-test}}

link to the definition of the periodogram in the methods part

Brockwell and Davis (1991, section 10.2) exploit the fact that the
periodogram can be expressed as the projection on the orthonormal basis
defined above to derive a test. Thus, under the null hypothesis:

\begin{itemize}
\tightlist
\item
  \[ 2I(\omega_{k})= \| P_{\bar{sp}_{\left\{ c_{k},s_{k} \right\}}} \mathbf{X} \|^{2}  \sim \sigma^{2} \chi^{2}(2) \],
  for Fourier frequencies \[ 0 < \omega_{k}=2\pi k/n < \pi \]
\item
  \[ I(\pi)= \| P_{\bar{sp}_{\left\{ e_{n/2} \right\}}} \mathbf{X} \|^{2}  \sim \sigma^{2} \chi^{2}(1) \],
  for \[ \pi \]
\end{itemize}

Because \[ I(\omega_{k}) \] is independent from the projection error sum
of squares, we can define our F-test statistic as follows:

\begin{itemize}
\tightlist
\item
  \[ \frac{ 2I(\omega_{k})}{\|\mathbf{X}-P_{\bar{sp}_{\left\{ e_0,c_{k},s_{k} \right\}}} \mathbf{X}\|^2} \frac{n-3}{2} \sim F(2,n-3) \],
  for Fourier frequencies \[ 0 < \omega_{k}=2\pi k/n < \pi \]
\item
  \[ \frac{ I(\pi)}{\|\mathbf{X}-P_{\bar{sp}_{\left\{ e_0,e_{n/2} \right\}}} \mathbf{X}\|^2} \frac{n-2}{1} \sim F(1,n-2)\],
  for \[ \pi \]
\end{itemize}

where -

\[ \|\mathbf{X}-P_{\bar{sp}_{\left\{ e_0,c_{k},s_{k} \right\}}} \mathbf{X}\|^2  = \sum_{i=1}^{n}\mathbf{X^2_i}-I(0)-2I(\omega_{k}) \sim \sigma^{2} \chi^{2}(n-3)\]

for Fourier frequencies \[ 0 < \omega_{k}=2\pi k/n < \pi \] -

\[ \|\mathbf{X}-P_{\bar{sp}_{\left\{ e_0,e_{n/2} \right\}}} \mathbf{X}\|^2 = \sum_{i=1}^{n}\mathbf{X^2_i}-I(0)-I(\pi) \sim \sigma^{2} \chi^{2}(n-2)  \]

for \[ \pi \]

Thus, we reject the null if our F-test statistic computed at a given
seasonal frequency (different from \[ \pi \]) is larger than

\[ F_{1-α}(2,n-3)\]. If we consider \[ \pi  \], our test statistic
follows a \[ F_{1-α}(1,n-2)\] distribution.

\hypertarget{implementation-of-f-test}{%
\subsubsection{Implementation of
F-test}\label{implementation-of-f-test}}

The implementation of JDemetra+ considers simultaneously the whole set
of seasonal frequencies (1, 2, 3, 4, 5 and 6 cycles per year). Thus, the
resulting test-statistic is:

\$\$

\frac{ 2I(\pi/6)+ 2I(\pi/3)+ 2I(2\pi/3)+ 2I(5\pi/6)+ \delta I(\pi)}{\left\|\mathbf{X}-P_{\bar{sp}_{\left\{ e_0,c_{1},s_{1},c_{2},s_{2},c_{3},s_{3},c_{4},s_{4},c_{5},s_{5}, \delta e_{n/2} \right\}}} \mathbf{X} \right\|^2}
\frac{n-12}{11} \sim F(11-\delta,n-12+\delta)

\[ where \] \delta=1 \[ if \] n \$\$ is even and 0 otherwise.

In small samples, the test performs better when the periodogram is
evaluated as the exact seasonal frequencies. JDemetra+ modifies the
sample size to ensure the seasonal frequencies belong to the set of
Fourier frequencies. This strategy provides a very simple and effective
way to eliminate the leakage problem.

Example of how results are displayed:

\includegraphics{./All_images/periodogram.png} \#\#\#\# Identification
of seasonal peaks in a Tukey periodogram and in an autoregressive
spectrum In order to decide whether a series has a seasonal component
that is predictable (stable) enough, these tests use visual criteria and
formal tests for the periodogram. The periodogram is calculated using
complete years, so that the set of Fourier frequencies contains exactly
all seasonal frequencies\footnote{For definition of the periodogram and
  Fourier frequencies see section
  \href{../theory/spectral.html}{Spectral Analysis}}.

The tests rely on two basic principles:

\begin{itemize}
\item
  The peaks associated with seasonal frequencies should be larger than
  the median spectrum for all frequencies and;
\item
  The peaks should exceed the spectrum of the two adjacent values by
  more than a critical value.
\end{itemize}

\begin{quote}
JDemetra+ performs this test on the original series. If these two
requirements are met, the test results are displayed in green. The
statistical significance of each of the seasonal peaks (i.e. frequencies
\(\frac{\pi}{6},\ \frac{\pi}{3},\ \frac{\pi}{2},\ \frac{2\pi}{3}\) and
\$\frac{5\pi}{6}\$ corresponding to 1, 2, 3, 4 and 5 cycles per year) is
also displayed. The seasonal and trading days frequencies depends on the
frequency of time series. They are shown in the table below. The symbol
\(d\) denotes a default frequency and is described below the table.
\end{quote}

\textbf{The seasonal and trading day frequencies by time series
frequency}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2466}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5068}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2466}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Number of months per full period}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Seasonal frequency}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Trading day frequency (radians)}
\end{minipage} \\
\midrule()
\endhead
12 &
\(\frac{\pi}{6},\frac{\pi}{3},\ \frac{\pi}{2},\frac{2\pi}{3},\ \frac{5\pi}{6},\ \pi\)
& \(d\), 2.714 \\
6 & \(\frac{\pi}{3},\frac{2\pi}{3}\), \(\pi\) & \(d\) \\
4 & \(\frac{\pi}{2}\), \(\pi\) & \(d\), 1.292, 1.850, 2.128 \\
3 & \(\pi\) & \(d\) \\
2 & \(\pi\) & \(d\) \\
\bottomrule()
\end{longtable}

The calendar (trading day or working day) effects, related to the
variation in the number of different days of the week per period, can
induce periodic patterns in the data that can be similar to those
resulting from pure seasonal effects. From the theoretical point of
view, trading day variability is mainly due to the fact that the average
number of days in the months or quarters is not equal to a multiple of 7
(the average number of days of a month in the year of 365.25 days is
equal to \(\frac{365.25}{12} =\) 30.4375 days). This effect occurs
\(\frac{365.25}{12} \times \frac{1}{7} =\) 4.3482 times per month: one
time for each one of the four complete weeks of each month, and a
residual of 0.3482 cycles per month,
i.e.~\(0.3482 \times 2\pi = 2.1878\) radians. This turns out to be a
fundamental frequency for the effects associated with monthly data. In
JDemetra+ the fundamental frequency corresponding to 0.3482 cycles per
month is used in place of the closest frequency
\(\frac{\text{πk}}{60}\). Thus, the quantity
\(\frac{\pi \times 42}{60}\) is replaced by
\(\omega_{42} = 0.3482 \times 2\pi = 2.1878\). The frequencies
neighbouring \(\omega_{42}\), i.e.~\(\omega_{41}\) and \(\omega_{43}\)
are set to, respectively, \(2.1865 - \frac{1}{60}\) and
\(2.1865 + \frac{1}{60}\).

The default frequencies (\$d)\$for calendar effect are: 2.188 (monthly
series) and 0.280 (quarterly series). They are computed as:

\$\$

\omega\_\{\text{ce}\} = \frac{2\pi}{7}\left( n - 7
\times \left\lbrack \frac{n}{7} \right\rbrack \right) \[, \]1\$\$

where:

\(n = \frac{365.25}{s}\), \(s = 4\) for quarterly series and \(s = 12\)
for monthly series.

Other frequencies that correspond to trading day frequencies are: 2.714
(monthly series) and 1.292, 1.850, 2.128 (quarterly series).

In particular, the calendar frequency in monthly data (marked in red on
the figure below) is very close to the seasonal frequency corresponding
to 4 cycles per year \(\text{ω}_{40} = \frac{2}{3}\pi = 2.0944\).

\begin{figure}

{\centering \includegraphics{./All_images/UG_A_image19.png}

}

\caption{Text}

\end{figure}

\textbf{Periodogram with seasonal (grey) and calendar (red) frequencies
highlighted}

This implies that it may be hard to disentangle both effects using the
frequency domain techniques.

\hypertarget{graphical-test-based-on-ar-spectrum}{%
\subsubsection{Graphical Test based on AR
spectrum}\label{graphical-test-based-on-ar-spectrum}}

for AR spectrum definition link to methods part

\begin{figure}

{\centering \includegraphics{./All_images/UG_A_image19.png}

}

\caption{Text}

\end{figure}

\textbf{Periodogram with seasonal (grey) and calendar (red) frequencies
highlighted}

The statistical significance of the peaks associated to a given
frequency can be informally tested using a visual criterion, which has
proved to perform well in simulation experiments. Visually significant
peaks for a frequency \[\lambda_{j}\] satisfy both conditions:

\begin{itemize}
\item
  \[ \frac{f_{x}(\lambda_{j})- \max \left\{f_{x}(\lambda_{j+1}),f_{x}(\lambda_{j-1}) \right\}}{\left[ \max_{k}f_{x}(\lambda_{k})-\min_{i}f_{x}(\lambda_{i}) \right]}\ge CV(\lambda_{j}) \],
  where \[ CV(\lambda_{j})\] can be set equal to \[6/52 \] for all

  \[j\]
\item
  \[ f_{x}(\lambda_{j})> median_{j} \left\{ f_{x}(\lambda_{j}) \right\}\],
  which guarantees \[ f_{x}(\lambda_{j}) \] it is not a local peak.
\end{itemize}

The first condition implies that if we divide the range

\[\max_{k}f_{x}(\lambda_{k})-\min_{i}f_{x}(\lambda_{i})\] in 52 parts
(traditionally represented by stars) the height of each pick should be
at least 6 stars.

\hypertarget{graphical-test-based-on-tukey-spectrum}{%
\subsubsection{Graphical Test based on Tukey
spectrum}\label{graphical-test-based-on-tukey-spectrum}}

link to methods/spectral analysis section for Tukeys definition

The current JDemetra+ implementation of the seasonality test is based on
a \[F(d_{1},d_{2})\] approximation that has been originally proposed by
Maravall (2012) for TRAMO-SEATS. This test is has been designed for a
Blackman-Tukey window based on a particular choices of the truncation
lag \[r\] and sample size. Following this approach, we determine
visually significant peaks for a frequency \[\omega_{j}\] when

\$\$

\frac{2 f_{x}(\omega_{j})}{\left[ f_{x}(\omega_{j+1})+ f_{x}(\omega_{j-1}) \right]}
\ge CV(\omega\_\{j\})

\$\$

where \[ CV(\omega_{j})\] is the critical value of a \[F(d_{1},d_{2})\]
distribution, where the degrees of freedom are determined using
simulations. For \[\omega_{j}= \pi\], we have a significant peak when

\[\frac{f_{x}(\omega_{[n/2]})}{\left[ f_{x}(\omega_{[(n-1)/2]})\right]} \ge CV(\omega_{j}) \]

Two significant levels for this test are considered: \[\alpha=0.05\]
(code ``t'') and \[\alpha=0.01\] (code ``T'').

As opposed to the
\href{\%7B\%7B\%20site.baseurl\%20\%7D\%7D/pages/theory/Tests_ARspectrum.html}{AR
spectrum}, which is computed on the basis of the last \[120\] data
points, we will use here all available observations. Those critical
values have been calculated given the recommended truncation lag
\[r=79\] for a sample size within the interval \[n \in [80,119]\] and
\[r=112\] for \[n \in [120,300]\] . The \[F\] approximation is less
accurate for

sample sizes larger than \[300\]. For quarterly data, \[r=44 \], but
there are no recommendations regarding the required sample size

JDemetra+ considers critical values for \[ \alpha=1\%\] (code ``T'') and

\[ \alpha=5\%\] (code ``t'') at each one of the seasonal frequencies
represented in the table below, e.g.~frequencies
\$\frac{\pi}{6},~\frac{\pi}{3},~\frac{\pi}{2},~\frac{2\pi}{3}\text{ and }
\frac{5\pi}{6}\$ corresponding to 1, 2, 3, 4, 5 and 6 cycles per year in
this example, since we are dealing with monthly data. The codes ``a''
and ``A'' correpond to the so-called
\href{\%7B\%7B\%20site.baseurl\%20\%7D\%7D/pages/theory/Tests_ARspectrum.html}{AR
spectrum}, so ignore them for the moment.

\textbf{The seasonal and trading day frequencies by time series
frequency}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2466}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5068}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2466}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Number of months per full period}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Seasonal frequency}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Trading day frequency (radians)}
\end{minipage} \\
\midrule()
\endhead
12 &
\(\frac{\pi}{6},\frac{\pi}{3},\ \frac{\pi}{2},\frac{2\pi}{3},\ \frac{5\pi}{6},\ \pi\)
& \(d\), 2.714 \\
6 & \(\frac{\pi}{3},\frac{2\pi}{3}\), \(\pi\) & \(d\) \\
4 & \(\frac{\pi}{2}\), \(\pi\) & \(d\), 1.292, 1.850, 2.128 \\
3 & \(\pi\) & \(d\) \\
2 & \(\pi\) & \(d\) \\
\bottomrule()
\end{longtable}

Currently, only seasonal frequencies are tested, but the program allows
you to manually plot the Tukey spectrum and focus your attention on both
seasonal and trading day frequencies.

\hypertarget{references-3}{%
\subsubsection{References}\label{references-3}}

Tukey, J. (1949). The sampling theory of power spectrum estimates.,
Proceedings Symposium on Applications of Autocorrelation Analysis to
Physical Problems, NAVEXOS-P-735, Office of Naval Research, Washington,
47-69

Brockwell, P.J., and R.A. Davis (1991). Times Series: Theory and
Methods. Springer Series in Statistics.

\hypertarget{example-of-non-seasonal-series}{%
\subsection{Example of non seasonal
series}\label{example-of-non-seasonal-series}}

The
\href{https://ec.europa.eu/eurostat/documents/3859598/6830795/KS-GQ-15-001-EN-N.pdf/d8f1e5f5-251b-4a69-93e3-079031b74bd3}{ESS
Guidelines on Seasonal Adjustment (2015)} recommend to apply seasonal
adjustment only to those time series for which the seasonal and/or
calendar effects can be properly explained, identified and estimated.
Therefore, seasonal adjustment of non-seasonal time series is an
inappropriate treatment. This case study explains how to recognize a
non-seasonal time series using the tools and functionalities implemented
in JDemetra+.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The picture below shows the results from the seasonal adjustment of a
  stock market turnover series from Greece using the RSA4c
  specification. The test diagnostics do not indicate any problems in
  the modelling phase (residual seasonality statistics and out-of-sample
  tests are displayed in green). The seasonality seems to be removed
  from the time series, but the overall assessment is uncertain, due to
  the failure of the m-statistics and the visual spectral analysis.

  \begin{figure}

  {\centering \includegraphics{./All_images/UG_SA_image5.jpg}

  }

  \caption{Text}

  \end{figure}

  \textbf{The diagnostic results for stock market turnover in Greece}
\item
  The inspection of a graph hints at the source of the problem. The
  original time series does not manifest any seasonal movements (left
  panel). It should be noted that when the X-13ARIMA-SEATS method is
  used for seasonal adjustment, the seasonal component is estimated
  regardless of the properties of the original time series (right
  panel). It means that the seasonal component is estimated even if
  there are no signs of the presence of seasonal fluctuations in the
  time series. In the picture below the seasonal component (blue line)
  is moving rather than being stable and the averages for the specific
  months (red lines) are not at the same level, suggesting some
  intra-year differences between seasons. Nevertheless, the SI ratios
  (dots) are rather far from the seasonal component, indicating that the
  irregular movements dominate over the seasonal ones.

  \begin{figure}

  {\centering \includegraphics{./All_images/UG_SA_image6.jpg}

  }

  \caption{Text}

  \end{figure}

  \textbf{Original and seasonally adjusted time series and the
  trend-cycle component (left) and SI ratios (right)}
\item
  The seasonality tests performed for the original time
  series\footnote{When the series are non-stationary differentiation is
    performed before the seasonality tests.} are ambiguous. Some suggest
  that seasonality is not present (the outcomes of three tests: the
  auto-correlation at seasonal lags, the spectral peaks test and the
  seasonal dummies test all indicate no seasonality in the original time
  series). These tests are available in the \emph{Diagnostic} section of
  the output tree. The seasonality tests can be executed independently
  from the seasonal adjustment proces. The descriptions of these tests
  are given in the
  \href{../case-studies/seasonalitytests.html}{\emph{Seasonality tests}}
  scenario.

  \begin{figure}

  {\centering \includegraphics{./All_images/UG_SA_image7.jpg}

  }

  \caption{Text}

  \end{figure}

  \textbf{Seasonality test for the original (transformed) series}
\item
  Another sign indicating that the presence of seasonality is uncertain
  should be addressed : the non-seasonal ARIMA model chosen by the
  automatic model identification procedure. The details of the RegARIMA
  model are available in the \emph{Pre-processing} node.

  \begin{figure}

  {\centering \includegraphics{./All_images/UG_SA_image8.jpg}

  }

  \caption{Text}

  \end{figure}

  \textbf{Estimation results for the RegARIMA model}
\item
  For X-13ARIMA-SEATS the most relevant tool to assess the presence of
  seasonal movement in the time series is a combined seasonality test.
  For the series presented in this case study the result of the combined
  seasonality test confirms that the movements observed in the time
  series are not stable and regular enough to be recognized as seasonal
  ones.

  \begin{figure}

  {\centering \includegraphics{./All_images/UG_SA_image9.jpg}

  }

  \caption{Text}

  \end{figure}

  \textbf{Combined seasonality test result}
\item
  Regardless of the presence and/or significance of seasonal movements
  in the original time series the seasonal component is always estimated
  by X-13ARIMA-SEATS, as shown in the picture below (from the panel on
  the left choose \emph{Main results} → \emph{Table}). Therefore
  X-13ARIMA-SEATS users should always check the outcome of the combined
  seasonality test.

  \begin{figure}

  {\centering \includegraphics{./All_images/UG_SA_image10.jpg}

  }

  \caption{Text}

  \end{figure}

  \textbf{Decomposition's results}
\item
  In general, in the case of a non-seasonal time series the TRAMO-SEATS
  method produces more coherent results than X-13ARIMA-SEATS. When no
  seasonal movements are detected the non-seasonal ARIMA model is used
  and the seasonal component is not estimated.

  \begin{figure}

  {\centering \includegraphics{./All_images/UG_SA_image11.jpg}

  }

  \caption{Text}

  \end{figure}

  \textbf{Decomposition result for a non-seasonal time series -
  TRAMO-SEATS}
\item
  Consequently, the \emph{SI ratios} (dots) estimated by TRAMO-SEATS are
  equal to the irregular component and for each month the seasonal
  component is equal to the mean (red, horizontal line), which is one.

  \begin{figure}

  {\centering \includegraphics{./All_images/UG_SA_image12.jpg}

  }

  \caption{Text}

  \end{figure}

  \textbf{SI ratios for a non-seasonal time series - TRAMO-SEATS}
\end{enumerate}

\hypertarget{calendar-correction}{%
\section{Calendar correction}\label{calendar-correction}}

here definitions, test in the pre adj part ?

\hypertarget{overview-of-calendar-effects-in-jdemetra}{%
\subsection{Overview of Calendar effects in
JDemetra+}\label{overview-of-calendar-effects-in-jdemetra}}

The following description of the calendar effects in JDemetra+ is
strictly based on PALATE, J. (2014).

A natural way for modelling calendar effects consists of distributing
the days of each period into different groups. The regression variable
corresponding to a type of day (a group) is simply defined by the number
of days it contains for each period. Usual classifications are:

\begin{itemize}
\item
  Trading days (7 groups): each day of the week defines a group
  (Mondays,...,Sundays);
\item
  Working days (2 groups): week days and weekends.
\end{itemize}

The definition of a group could involve partial days. For instance, we
could consider that one half of Saturdays belong to week days and the
second half to weekends.

Usually, specific holidays are handled as Sundays and they are included
in the group corresponding to "non-working days". This approach assumes
that the economic activity on national holidays is the same (or very
close to) the level of activity that is typical for Sundays.
Alternatively, specific holidays can be considered separately, e.g.~by
the specification that divided days into three groups:

\begin{itemize}
\item
  Working days (Mondays to Fridays, except for specific holidays),
\item
  Non-working days (Saturdays and Sundays, except for specific
  holidays),
\item
  Specific holidays.
\end{itemize}

\hypertarget{summary-of-the-method-used-in-jdemetra-to-compute-trading-day-and-working-day-effects}{%
\subsection{Summary of the method used in JDemetra+ to compute trading
day and working day
effects}\label{summary-of-the-method-used-in-jdemetra-to-compute-trading-day-and-working-day-effects}}

The computation of trading day and working days effects is performed in
four steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Computation of the number of each weekday performed for all periods.
\item
  Calculation of the usual contrast variables for trading day and
  working day.
\item
  Correction of the contrast variables with specific holidays (for each
  holiday add +1 to the number of Sundays and subtract 1 from the number
  of days of the holiday). The correction is not performed if the
  holiday falls on a Sunday, taking into account the validity period of
  the holiday.
\item
  Correction of the constant variables for long term mean effects,
  \textgreater{} taking into account the validity period of the holiday;
  see below \textgreater{} for the different cases.
\end{enumerate}

The corrections of the constant variables may receive a weight
corresponding to the part of the holiday considered as a Sunday.

An example below illustrates the application of the above algorithm for
the hypothetical country in which three holidays are celebrated:

\begin{itemize}
\item
  New Year (a fixed holiday, celebrated on 01 January);
\item
  Shrove Tuesday (a moving holiday, which falls 47 days before Easter
  Sunday, celebrated until the end of 2012);
\item
  Freedom day (a fixed holiday, celebrated on 25 April).
\end{itemize}

The consecutive steps in calculation of the calendar for 2012 and 2013
years are explained below.

First, the number of each day of the week in the given month is
calculated as it is shown in table below.

\textbf{Number of each weekday in different months}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1486}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1216}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1216}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1216}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1216}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1216}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1216}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1216}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Month}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Mon}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Tue}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Wed}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Thu}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Fri}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Sat}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Sun}
\end{minipage} \\
\midrule()
\endhead
Jan-12 & 5 & 5 & 4 & 4 & 4 & 4 & 5 \\
Feb-12 & 4 & 4 & 5 & 4 & 4 & 4 & 4 \\
Mar-12 & 4 & 4 & 4 & 5 & 5 & 5 & 4 \\
Apr-12 & 5 & 4 & 4 & 4 & 4 & 4 & 5 \\
May-12 & 4 & 5 & 5 & 5 & 4 & 4 & 4 \\
Jun-12 & 4 & 4 & 4 & 4 & 5 & 5 & 4 \\
Jul-12 & 5 & 5 & 4 & 4 & 4 & 4 & 5 \\
Aug-12 & 4 & 4 & 5 & 5 & 5 & 4 & 4 \\
Sep-12 & 4 & 4 & 4 & 4 & 4 & 5 & 5 \\
Oct-12 & 5 & 5 & 5 & 4 & 4 & 4 & 4 \\
Nov-12 & 4 & 4 & 4 & 5 & 5 & 4 & 4 \\
Dec-12 & 5 & 4 & 4 & 4 & 4 & 5 & 5 \\
Jan-13 & 4 & 5 & 5 & 5 & 4 & 4 & 4 \\
Feb-13 & 4 & 4 & 4 & 4 & 4 & 4 & 4 \\
Mar-13 & 4 & 4 & 4 & 4 & 5 & 5 & 5 \\
Apr-13 & 5 & 5 & 4 & 4 & 4 & 4 & 4 \\
May-13 & 4 & 4 & 5 & 5 & 5 & 4 & 4 \\
Jun-13 & 4 & 4 & 4 & 4 & 4 & 5 & 5 \\
Jul-13 & 5 & 5 & 5 & 4 & 4 & 4 & 4 \\
Aug-13 & 4 & 4 & 4 & 5 & 5 & 5 & 4 \\
Sep-13 & 5 & 4 & 4 & 4 & 4 & 4 & 5 \\
Oct-13 & 4 & 5 & 5 & 5 & 4 & 4 & 4 \\
Nov-13 & 4 & 4 & 4 & 4 & 5 & 5 & 4 \\
Dec-13 & 5 & 5 & 4 & 4 & 4 & 4 & 5 \\
\bottomrule()
\end{longtable}

Next, the contrast variables are calculated (table below) as a result of
the linear transformation applied to the variables presented in table
below.

\textbf{Contrast variables (series corrected for leap year effects)}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1169}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1169}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1169}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1169}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1169}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1169}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1558}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Month}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Mon}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Tue}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Wed}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Thu}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Fri}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Sat}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Length}
\end{minipage} \\
\midrule()
\endhead
Jan-12 & 0 & 0 & -1 & -1 & -1 & -1 & 0 \\
Feb-12 & 0 & 0 & 1 & 0 & 0 & 0 & 0.75 \\
Mar-12 & 0 & 0 & 0 & 1 & 1 & 1 & 0 \\
Apr-12 & 0 & -1 & -1 & -1 & -1 & -1 & 0 \\
May-12 & 0 & 1 & 1 & 1 & 0 & 0 & 0 \\
Jun-12 & 0 & 0 & 0 & 0 & 1 & 1 & 0 \\
Jul-12 & 0 & 0 & -1 & -1 & -1 & -1 & 0 \\
Aug-12 & 0 & 0 & 1 & 1 & 1 & 0 & 0 \\
Sep-12 & -1 & -1 & -1 & -1 & -1 & 0 & 0 \\
Oct-12 & 1 & 1 & 1 & 0 & 0 & 0 & 0 \\
Nov-12 & 0 & 0 & 0 & 1 & 1 & 0 & 0 \\
Dec-12 & 0 & -1 & -1 & -1 & -1 & 0 & 0 \\
Jan-13 & 0 & 1 & 1 & 1 & 0 & 0 & 0 \\
Feb-13 & 0 & 0 & 0 & 0 & 0 & 0 & -0.25 \\
Mar-13 & -1 & -1 & -1 & -1 & 0 & 0 & 0 \\
Apr-13 & 1 & 1 & 0 & 0 & 0 & 0 & 0 \\
May-13 & 0 & 0 & 1 & 1 & 1 & 0 & 0 \\
Jun-13 & -1 & -1 & -1 & -1 & -1 & 0 & 0 \\
Jul-13 & 1 & 1 & 1 & 0 & 0 & 0 & 0 \\
Aug-13 & 0 & 0 & 0 & 1 & 1 & 1 & 0 \\
Sep-13 & 0 & -1 & -1 & -1 & -1 & -1 & 0 \\
Oct-13 & 0 & 1 & 1 & 1 & 0 & 0 & 0 \\
Nov-13 & 0 & 0 & 0 & 0 & 1 & 1 & 0 \\
Dec-13 & 5 & 5 & 4 & 4 & 4 & 4 & 0 \\
\bottomrule()
\end{longtable}

In the next step the corrections for holidays is done in the following
way:

\begin{itemize}
\item
  New Year: In 2012 it falls on a Sunday. Therefore no correction is
  applied. In 2013 it falls on a Tuesday. Consequently, the following
  corrections are applied to the number of each weekday in January:
  Tuesday -1, Sunday +1, so the following corrections are applied to the
  contrast variables: -2 for Tuesday and -1 for the other contrast
  variables.
\item
  Shrove Tuesday: It is a fixed day of the week holiday that always
  falls on Tuesday. For this reason in 2012 the following corrections
  are applied to the number of each weekday in February: Tuesday -1,
  Sunday +1, so the following corrections are applied to the contrast
  variables: -2 for the contrast variable associated with Tuesday, and
  -1 for the other contrast variables. The holiday expires at the end of
  2012. Therefore no corrections are made for 2013.
\item
  Freedom Day: In 2012 it falls on a Wednesday. Consequently, the
  following corrections are applied to the number of each weekday in
  April: Wednesday -1, Sunday +1, so the following corrections are
  applied to the contrast variables: -2 for Wednesday and -1 for the
  other contrast variables. In 2013 it falls on Thursday. Therefore, the
  following corrections are applied to the number of each weekday in
  April: Thursday -1, Sunday +1, so the following corrections are
  applied to the contrast variables: -2 for Thursday, and -1 for the
  other contrast variables.
\end{itemize}

The result of these corrections is presented in table below.

\textbf{Contrast variables corrected for holidays}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1447}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1184}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1184}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1184}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1184}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1184}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1184}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1447}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Month}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Mon}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Tue}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Wed}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Thu}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Fri}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Sat}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Length}
\end{minipage} \\
\midrule()
\endhead
Jan-12 & 0 & 0 & -1 & -1 & -1 & -1 & 0 \\
Feb-12 & -1 & -2 & 0 & -1 & -1 & -1 & 0.75 \\
Mar-12 & 0 & 0 & 0 & 1 & 1 & 1 & 0 \\
Apr-12 & -1 & -2 & -3 & -2 & -2 & -2 & 0 \\
May-12 & 0 & 1 & 1 & 1 & 0 & 0 & 0 \\
Jun-12 & 0 & 0 & 0 & 0 & 1 & 1 & 0 \\
Jul-12 & 0 & 0 & -1 & -1 & -1 & -1 & 0 \\
Aug-12 & 0 & 0 & 1 & 1 & 1 & 0 & 0 \\
Sep-12 & -1 & -1 & -1 & -1 & -1 & 0 & 0 \\
Oct-12 & 1 & 1 & 1 & 0 & 0 & 0 & 0 \\
Nov-12 & 0 & 0 & 0 & 1 & 1 & 0 & 0 \\
Dec-12 & 0 & -1 & -1 & -1 & -1 & 0 & 0 \\
Jan-13 & -1 & -1 & 0 & 0 & -1 & -1 & 0 \\
Feb-13 & 0 & 0 & 0 & 0 & 0 & 0 & -0.25 \\
Mar-13 & -1 & -1 & -1 & -1 & 0 & 0 & 0 \\
Apr-13 & 0 & 0 & -1 & -2 & -1 & -1 & 0 \\
May-13 & 0 & 0 & 1 & 1 & 1 & 0 & 0 \\
Jun-13 & -1 & -1 & -1 & -1 & -1 & 0 & 0 \\
Jul-13 & 1 & 1 & 1 & 0 & 0 & 0 & 0 \\
Aug-13 & 0 & 0 & 0 & 1 & 1 & 1 & 0 \\
Sep-13 & 0 & -1 & -1 & -1 & -1 & -1 & 0 \\
Oct-13 & 0 & 1 & 1 & 1 & 0 & 0 & 0 \\
Nov-13 & 0 & 0 & 0 & 0 & 1 & 1 & 0 \\
Dec-13 & 0 & 0 & -1 & -1 & -1 & -1 & 0 \\
\bottomrule()
\end{longtable}

Finally, the long term corrections are applied on each year of the
validity period of the holiday.

\begin{itemize}
\item
  New Year: Correction on the contrasts: +1, to be applied to January of
  2012 and 2013.
\item
  Shrove Tuesday: It may fall either in February or in March. It will
  fall in March if Easter is on or after 17 April. Taking into account
  the theoretical distribution of Easter, it gives: prob(March) =
  +0.22147, prob(February) = +0.77853. The correction of the contrasts
  will be +1.55707 for Tuesday in February 2012 and +0.77853 for the
  other contrast variables. The correction of the contrasts will be
  +0.44293 for Tuesday in March 2012, +0.22147 for the other contrast
  variables.
\item
  Freedom Day: Correction on the contrasts: +1, to be applied to April
  of 2012 and 2013.
\end{itemize}

The modifications due to the corrections described above are presented
in table below.

\textbf{Trading day variables corrected for the long term effects}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1325}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1205}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1205}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1205}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1205}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1205}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1205}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1446}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Month}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Mon}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Tue}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Wed}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Thu}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Fri}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Sat}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Length}
\end{minipage} \\
\midrule()
\endhead
Jan-12 & 1 & 1 & 0 & 0 & 0 & 0 & 0 \\
Feb-12 & -0.22115 & -0.44229 & 0.778853 & -0.22115 & -0.22115 & -0.22115
& 0.75 \\
Mar-12 & 0.221147 & 0.442293 & 0.221147 & 1.221147 & 1.221147 & 1.221147
& 0 \\
Apr-12 & 0 & -1 & -2 & -1 & -1 & -1 & 0 \\
May-12 & 0 & 1 & 1 & 1 & 0 & 0 & 0 \\
Jun-12 & 0 & 0 & 0 & 0 & 1 & 1 & 0 \\
Jul-12 & 0 & 0 & -1 & -1 & -1 & -1 & 0 \\
Aug-12 & 0 & 0 & 1 & 1 & 1 & 0 & 0 \\
Sep-12 & -1 & -1 & -1 & -1 & -1 & 0 & 0 \\
Oct-12 & 1 & 1 & 1 & 0 & 0 & 0 & 0 \\
Nov-12 & 0 & 0 & 0 & 1 & 1 & 0 & 0 \\
Dec-12 & 0 & -1 & -1 & -1 & -1 & 0 & 0 \\
Jan-13 & 0 & 0 & 1 & 1 & 0 & 0 & 0 \\
Feb-13 & 0 & 0 & 0 & 0 & 0 & 0 & -0.25 \\
Mar-13 & -1 & -1 & -1 & -1 & 0 & 0 & 0 \\
Apr-13 & 1 & 1 & 0 & -1 & 0 & 0 & 0 \\
May-13 & 0 & 0 & 1 & 1 & 1 & 0 & 0 \\
Jun-13 & -1 & -1 & -1 & -1 & -1 & 0 & 0 \\
Jul-13 & 1 & 1 & 1 & 0 & 0 & 0 & 0 \\
Aug-13 & 0 & 0 & 0 & 1 & 1 & 1 & 0 \\
Sep-13 & 0 & -1 & -1 & -1 & -1 & -1 & 0 \\
Oct-13 & 0 & 1 & 1 & 1 & 0 & 0 & 0 \\
Nov-13 & 0 & 0 & 0 & 0 & 1 & 1 & 0 \\
Dec-13 & 0 & 0 & -1 & -1 & -1 & -1 & 0 \\
\bottomrule()
\end{longtable}

\hypertarget{mean-and-seasonal-effects-of-calendar-variables}{%
\subsection{Mean and seasonal effects of calendar
variables}\label{mean-and-seasonal-effects-of-calendar-variables}}

The calendar effects produced by the regression variables that fulfil
the definition presented above include a mean effect (i.e.~an effect
that is independent of the period) and a seasonal effect (i.e.~an effect
that is dependent of the period and on average it is equal to 0). Such
an outcome is inappropriate, as in the usual decomposition of a series
the mean effect should be allocated to the trend component and the fixed
seasonal effect should be affected to the corresponding component.
Therefore, the actual calendar effect should only contain effects that
don't belong to the other components.

In the context of JDemetra+ the mean effect and the seasonal effect are
long term theoretical effects rather than the effects computed on the
time span of the considered series (which should be continuously
revised).

The mean effect of a calendar variable is the average number of days in
its group. Taking into account that one year has on average 365.25 days,
the monthly mean effects for a working days are, as shown in the table
below, 21.7411 for week days and 8.696 for weekends.

\textbf{Monthly mean effects for the Working day variable}

\begin{longtable}[]{@{}ll@{}}
\toprule()
\textbf{Groups of Working day effect} & \textbf{Mean effect} \\
\midrule()
\endhead
Week days & 365.25/12*5/7 = \textbf{21.7411} \\
Weekends & 365.25/12*2/7 = \textbf{8.696} \\
Total & 365.25/12 = \textbf{30.4375} \\
\bottomrule()
\end{longtable}

The number of days by period is highly seasonal, as apart from February,
the length of each month is the same every year. For this reason, any
set of calendar variables will contain, at least in some variables, a
significant seasonal effect, which is defined as the average number of
days by period (Januaries..., first quarters...) outside the mean
effect. Removing that fixed seasonal effects consists of removing for
each period the long term average of days that belong to it. The
calculation of a seasonal effect for the working days classification is
presented in the table below.

\textbf{The mean effect and the seasonal effect for the calendar
periods}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1091}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2545}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.3000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1545}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1818}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Period}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Average number of days}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Average number of week days}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Mean effect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Seasonal effect}
\end{minipage} \\
\midrule()
\endhead
January & 31 & 31*5/7=22.1429 & 21.7411 & 0.4018 \\
February & 28.25 & 28.25*5/7=20.1786 & 21.7411 & -1.5625 \\
March & 31 & 31*5/7=22.1429 & 21.7411 & 0.4018 \\
April & 30 & 30*5/7=21.4286 & 21.7411 & -0.3125 \\
May & 31 & 31*5/7=22.1429 & 21.7411 & 0.4018 \\
June & 30 & 30*5/7=21.4286 & 21.7411 & -0.3125 \\
July & 31 & 31*5/7=22.1429 & 21.7411 & 0.4018 \\
August & 31 & 31*5/7=22.1429 & 21.7411 & 0.4018 \\
September & 30 & 30*5/7=21.4286 & 21.7411 & -0.3125 \\
October & 31 & 31*5/7=22.1429 & 21.7411 & 0.4018 \\
November & 30 & 30*5/7=21.4286 & 21.7411 & -0.3125 \\
December & 31 & 31*5/7=22.1429 & 21.7411 & 0.4018 \\
Total & 365.25 & 260.8929 & 260.8929 & 0 \\
\bottomrule()
\end{longtable}

For a given time span, the actual calendar effect for week days can be
easily calculated as the difference between the number of week days in a
specific period and the sum of the mean effect and the seasonal effect
assigned to this period, as it is shown in the table below for the
period 01.2013 -- 06.2013.

\textbf{The calendar effect for the period 01.2013 - 06.2013}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2211}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1579}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1789}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2211}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2211}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Time period (t)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Week days}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Mean effect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Seasonal effect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Calendar effect}
\end{minipage} \\
\midrule()
\endhead
Jan-2013 & 23 & 21.7411 & 0.4018 & 0.8571 \\
Feb-2013 & 20 & 21.7411 & -1.5625 & -0.1786 \\
Mar-2013 & 21 & 21.7411 & 0.4018 & -1.1429 \\
Apr-2013 & 22 & 21.7411 & -0.3125 & 0.5714 \\
May-2013 & 23 & 21.7411 & 0.4018 & 0.8571 \\
Jun-2013 & 20 & 21.7411 & -0.3125 & -1.4286 \\
Jul-2013 & 23 & 21.7411 & 0.4018 & 0.8571 \\
\bottomrule()
\end{longtable}

The distinction between the mean effect and the seasonal effect is
usually unnecessary. Those effects can be considered together (simply
called mean effects) and be computed by removing from each calendar
variable its average number of days by period. These global means effect
are considered in the next section.

\hypertarget{impact-of-the-mean-effects-on-the-decomposition}{%
\subsection{Impact of the mean effects on the
decomposition}\label{impact-of-the-mean-effects-on-the-decomposition}}

When the ARIMA model contains a seasonal difference -- something that
should always happen with calendar variables -- the mean effects
contained in the calendar variables are automatically eliminated, so
that they don't modify the estimation. The model is indeed estimated on
the series/regression variables after differencing. However, they lead
to a different linearised series (\(y_{\text{lin}})\). The impact of
other corrections (mean and/or fixed seasonal) on the decomposition is
presented in the next paragraph. Such corrections could be obtained, for
instance, by applying other solutions for the long term corrections or
by computing them on the time span of the series.

Now the model with "correct" calendar effects (denoted as \(C\)), i.e.
effects without mean and fixed seasonal effects, can be considered. To
simplify the problem, the model has no other regression effects.

For such a model the following relations hold:

\[y_{\text{lin}} = \ y - C\]

\[T = \ F_{T}\left( y_{\text{lin}} \right)\]

\[S = \ F_{S}\left( y_{\text{lin}} \right) + C\]

\[I = \ F_{I}\left( y_{\text{lin}} \right)\]

where:

T - the trend;

S - the seasonal component;

I - the irregular component;

\(F_{X}\) - the linear filter for the component X.

Consider next other calendar effects (\(\widetilde{C}\)) that contain
some mean (\(\text{cm}\), integrated to the final trend) and fixed
seasonal effects (\(\text{cs}\), integrated to the final seasonal). The
modified equations are now:

\[\widetilde{C} = C + cm + cs\]

\[{\widetilde{y}}_{\text{lin}} = \ y - \widetilde{C} = \ y_{\text{lin}} - cm - cs\]

\[\widetilde{T} = \ F_{T}\left( {\widetilde{y}}_{\text{lin}} \right) + cm\]

\[\widetilde{S} = \ F_{S}\left( {\widetilde{y}}_{\text{lin}} \right) + C + cs\]

\[\widetilde{I} = \ F_{I}\left( {\widetilde{y}}_{\text{lin}} \right)\]

Taking into account that \(F_{X}\) is a linear transformation and
that\footnote{In case of SEATS the properties can be trivially derived
  from the matrix formulation of signal extraction. They are also valid
  for X-11 (additive).}

\[F_{T}\left( \text{cm} \right) = cm\]

\[F_{T}\left( \text{cs} \right) = 0\]

\[F_{S}\left( \text{cm} \right) = 0\ \]

\[F_{S}\left( \text{cs} \right) = cs\]

\[F_{I}\left( \text{cm} \right) = 0\]

\[F_{I}\left( \text{cs} \right) = 0\]

The following relationships hold:

\[\widetilde{T} = \ F_{T}\left( {\widetilde{y}}_{\text{lin}} \right) + cm = F_{T}\left( y_{\text{lin}} \right) - cm + cm = T\]

\[\widetilde{S} = \ F_{S}\left( {\widetilde{y}}_{\text{lin}} \right) + C + cs = F_{S}\left( y_{\text{lin}} \right) - cs + C + cs = S\]

\[\widetilde{I} = \ I\]

If we don't take into account the effects and apply the same approach as
in the ``correct'' calendar effects, we will get:

\[\breve{T} = \ F_{T}\left( {\widetilde{y}}_{\text{lin}} \right) = T - cm\]

\[\breve{S} = \ F_{S}\left( {\widetilde{y}}_{\text{lin}} \right) + \widetilde{C} = S + cm\]

\[\breve{I} = \ F_{I}\left( {\widetilde{y}}_{\text{lin}} \right) = I\]

The trend, seasonal and seasonally adjusted series will only differ by a
(usually small) constant.

In summary, the decomposition does not depend on the mean and fixed
seasonal effects used for the calendar effects, provided that those
effects are integrated in the corresponding final components. If these
corrections are not taken into account, the main series of the
decomposition will only differ by a constant.

\hypertarget{linear-transformations-of-the-calendar-variables}{%
\subsection{Linear transformations of the calendar
variables}\label{linear-transformations-of-the-calendar-variables}}

As far as the RegARIMA and the TRAMO models are considered, any
non-degenerated linear transformation of the calendar variables can be
used. It will produce the same results (likelihood, residuals,
parameters, joint effect of the calendar variables, joint F-test on the
coefficients of the calendar variables\ldots). The linearised series
that will be further decomposed is invariant to any linear
transformation of the calendar variables.

However, it should be mentioned that choices of calendar corrections
based on the tests on the individual t statistics are dependent on the
transformation, which is rather arbitrary. This is the case in old
versions of TRAMO-SEATS. That is why the joint F-test (as in the version
of TRAMO-SEATS implemented in TSW+) should be preferred.

An example of a linear transformation is the calculation of the contrast
variables. In the case of the usual trading day variables, they are
defined by the following transformation: the 6 contrast variables
(\(\text{No.}\left( \text{Mondays} \right) - No.\left( \text{Sundays} \right),\ldots No.\left( \text{Saturdays} \right) - No.(Sundays)\))
used with the length of period.

\[\begin{bmatrix}                 
  1 & 0 & 0 & 0 & 0 & 0 & - 1 \\    
  0 & 1 & 0 & 0 & 0 & 0 & - 1 \\    
  0 & 0 & 1 & 0 & 0 & 0 & - 1 \\    
  0 & 0 & 0 & 1 & 0 & 0 & - 1 \\    
  0 & 0 & 0 & 0 & 1 & 0 & - 1 \\    
  0 & 0 & 0 & 0 & 0 & 1 & - 1 \\    
  1 & 1 & 1 & 1 & 1 & 1 & 1 \\      
  \end{bmatrix}\begin{bmatrix}      
  \text{Mon} \\                     
  \text{Tue} \\                     
  \text{Wed} \\                     
  \text{Thu} \\                     
  \text{Fri} \\                     
  \text{Sat} \\                     
  \text{Sun} \\                     
  \end{bmatrix} = \begin{bmatrix}   
  Mon - Sun \\                      
  Tue - Sun \\                      
  Wed - Sun \\                      
  Thu - Sun \\                      
  Fri - Sun \\                      
  Sat - Sun \\                      
  \text{Length of period} \\      
  \end{bmatrix}\]

For the usual working day variables, two variables are used: one
contrast variable and the length of period

\[\begin{bmatrix}                  
  1 & - \frac{5}{2} \\              
  1 & 1 \\                          
  \end{bmatrix}\begin{bmatrix}      
  \text{Week} \\                    
  \text{Weekend} \\                 
  \end{bmatrix} = \begin{bmatrix}   
  \text{Contrast week} \\          
  \text{Length of period} \\      
  \end{bmatrix}\]

The \(\text{Length of period}\) variable is defined as a deviation from
the length of the month (in days) and the average month length, which is
equal to \(30.4375.\) Instead, the leap-year variable can be used here
(see Regression sections in \protect\hyperlink{regression}{RegARIMA} or
\protect\hyperlink{regression}{Tramo})\footnote{GÓMEZ, V., and MARAVALL,
  A (2001b).}.

Such transformations have several advantages. They suppress from the
contrast variables the mean and the seasonal effects, which are
concentrated in the last variable. So, they lead to fewer correlated
variables, which are more appropriate to be included in the regression
model. The sum of the effects of each day of the week estimated with the
trading (working) day contrast variables cancel out.

\hypertarget{handling-of-specific-holidays}{%
\subsection{Handling of specific
holidays}\label{handling-of-specific-holidays}}

check vs GUI (v3) and rjd3 modelling

Three types of holidays are implemented in JDemetra+:

\begin{itemize}
\item
  Fixed days, corresponding to the fixed dates in the year (e.g.~New
  Year, Christmas).
\item
  Easter related days, corresponding to the days that are defined in
  relation to Easter (e.g.~Easter +/- n days; example: Ascension,
  Pentecost).
\item
  Fixed week days, corresponding to the fixed days in a given week of a
  given month (e.g.~Labor Day celebrated in the USA on the first Monday
  of September).
\end{itemize}

From a conceptual point of view, specific holidays are handled in
exactly the same way as the other days. It should be decided, however,
to which group of days they belong. Usually they are handled as Sundays.
This convention is also used in JDemetra+. Therefore, except if the
holiday falls on a Sunday, the appearance of a holiday leads to
correction in two groups, i.e.~in the group that contains the weekday,
in which holiday falls, and the group that contains the Sundays.

Country specific holidays have an impact on the mean and the seasonal
effects of calendar effects. Therefore, the appropriate corrections to
the number of particular days (which are usually the basis for the
definition of other calendar variables) should be applied, following the
kind of holidays. These corrections are applied to the period(s) that
may contain the holiday. The long term corrections in JDemetra+ don't
take into account the fact that some moving holidays could fall on the
same day (for instance the May Day and the Ascension). However, those
events are exceptional, and their impact on the final result is usually
not significant.

\hypertarget{fixed-day}{%
\subsubsection{Fixed day}\label{fixed-day}}

The probability that the holiday falls on a given day of the week is
1/7. Therefore, the probability to have 1 day more that is treated like
Sunday is 6/7. The effect on the means for the period that contains the
fixed day is presented in the table below (the correction on the
calendar effect has the opposite sign).

\textbf{The effect of the fixed holiday on the period, in which it
occurred}

\begin{longtable}[]{@{}lll@{}}
\toprule()
\textbf{Sundays} & \textbf{Others days} & \textbf{Contrast variables} \\
\midrule()
\endhead
+ 6/7 & - 1/7 & 1/7 - (+ 6/7)= -1 \\
\bottomrule()
\end{longtable}

\hypertarget{easter-related-days}{%
\subsubsection{Easter related days}\label{easter-related-days}}

Easter related days always fall the same week day (denoted as Y in the
table below: The effects of the Easter Sunday on the seasonal means).
However, they can fall during different periods (months or quarters).
Suppose that, taking into account the distribution of the dates for
Easter and the fact that this holiday falls in one of two periods, the
probability that Easter falls during the period \(m\) is \(p\), which
implies that the probability that it falls in the period \(m + 1\) is
\(1 - p\). The effects of Easter on the seasonal means are presented in
the table below.

\textbf{The effects of the Easter Sunday on the seasonal means}

\textbar{}\textbf{Period} \textbar{} \textbf{Sundays} \textbf{Days X}
\textbf{Others days} \textbf{Contrast Y} \textbf{Other contrasts}
\textbar------------\textbar{} ------------- ------------
----------------- ------------------- --------------------- \textbar m
\textbar+ p - p 0 - 2p - p \textbar m+1 \textbar{} + (1-p) - (1-p) 0 -
2\(\times\)(1-p) - (1-p)

The distribution of the dates for Easter may be approximated in
different ways. One of the solutions consists of using some well-known
algorithms for computing Easter on a very long period. JDemetra+
provides the Meeus/Jones/Butcher's and the Ron Mallen's algorithms (they
are identical till year 4100, but they slightly differ after that date).
Another approach consists in deriving a raw theoretical distribution
based on the definition of Easter. It is the solution used for Easter
related days. It is shortly explained below.

The date of Easter in the given year is the first Sunday after the full
moon (the Paschal Full Moon) following the northern hemisphere's vernal
equinox. The definition is influenced by the Christian tradition,
according to which the equinox is reckoned to be on 21 March\footnote{In
  fact, astronomical observations show that the equinox occurs on 20
  March in most years.} and the full moon is not necessarily the
astronomically correct date. However, when the full moon falls on
Sunday, then Easter is delayed by one week. With this definition, the
date of Easter Sunday varies between 22 March and 25 April. Taking into
account that an average lunar month is \(29.530595\) days the
approximated distribution of Easter can be derived. These calculations
do not take into account the actual ecclesiastical moon calendar.

For example, the probability that Easter Sunday falls on 25 March is
0.004838 and results from the facts that the probability that 25 March
falls on a Sunday is \(1/7\) and the probability that the full moon is
on 21 March, 22 March, 23 March or 24 March is \(5/29.53059\). The
probability that Easter falls on 24 April is 0.01708 and results from
the fact that the probability that 24 April is Sunday is \(1/7\) and
takes into account that 18 April is the last acceptable date for the
full moon. Therefore the probability that the full moon is on 16 April
or 17 April is \(1/29.53059\) and the probability that the full moon is
on 18 April is \(1.53059/29.53059\).

\textbf{The approximated distribution of Easter dates}

\begin{longtable}[]{@{}ll@{}}
\toprule()
\textbf{Day} & \textbf{Probability} \\
\midrule()
\endhead
22 March & 1/7 * 1/29.53059 \\
23 March & 1/7 * 2/29.53059 \\
24 March & 1/7 * 3/29.53059 \\
25 March & 1/7 * 4/29.53059 \\
26 March & 1/7 * 5/29.53059 \\
27 March & 1/7 * 6/29.53059 \\
28 March & 1/29.53059 \\
29 March & 1/29.53059 \\
\ldots{} & \ldots{} \\
18 April & 1/29.53059 \\
19 April & 1/7 * (6 + 1.53059)/29.53059 \\
20 April & 1/7 * (5 + 1.53059)/29.53059 \\
21 April & 1/7 * (4 + 1.53059)/29.53059 \\
22 April & 1/7 * (3 + 1.53059)/29.53059 \\
23 April & 1/7 * (2 + 1.53059)/29.53059 \\
24 April & 1/7 * (1 + 1.53059)/29.53059 \\
25 April & 1/7 * 1.53059/29.53059 \\
\bottomrule()
\end{longtable}

\hypertarget{fixed-week-days}{%
\subsubsection{Fixed week days}\label{fixed-week-days}}

Fixed week days always fall on the same week day (denoted as Y in the
table below) and in the same period. Their effect on the seasonal means
is presented in the table below.

\textbf{The effect of the fixed week holiday on the period, in which it
occurred}

\begin{longtable}[]{@{}lll@{}}
\toprule()
\textbf{Sundays} & \textbf{Day Y} & \textbf{Others days} \\
\midrule()
\endhead
+ 1 & - 1 & 0 \\
\bottomrule()
\end{longtable}

The impact of fixed week days on the regression variables is zero
because the effect itself is compensated by the correction for the mean
effect.

\hypertarget{holidays-with-a-validity-period}{%
\subsection{Holidays with a validity
period}\label{holidays-with-a-validity-period}}

When a holiday is valid only for a given time span, JDemetra+ applies
the long term mean corrections only on the corresponding period.
However, those corrections are computed in the same way as in the
general case.

It is important to note that using or not using mean corrections will
impact in the estimation of the RegARIMA and TRAMO models. Indeed, the
mean corrections do not disappear after differencing. The differences
between the SA series computed with or without mean corrections will no
longer be constant.

\hypertarget{different-kinds-of-calendars}{%
\subsection{Different Kinds of
calendars}\label{different-kinds-of-calendars}}

see link with GUI

This scenario presents how to define different kinds of calendars. These
calendars can be applied to the specifications that take into account
country-specific holidays and can be used for detecting and estimating
the calendar effects.

The calendar effects are those parts of the movements in the time series
that are caused by different number of weekdays in calendar months (or
quarters). They arise as the number of occurrences of each day of the
week in a month (or a quarter) differs from year to year. These
differences cause regular effects in some series. In particular, such
variation is caused by a leap year effect because of an extra day
inserted into February every four years. As with seasonal effects, it is
desirable to estimate and remove calendar effects from the time series.

The calendar effects can be divided into a mean effect, a seasonal part
and a structural part. The mean effect is independent from the period
and therefore should be allocated to the trend-cycle. The seasonal part
arises from the properties of the calendar that recur each year. For one
thing, the number of working days of months with 31 calendar days is on
average larger than that of months with 30 calendar days. This effect is
part of the seasonal pattern captured by the seasonal component (with
the exception of leap year effects). The structural part of the calendar
effect remains to be determined by the calendar adjustment. For example,
the number of working days of the same month in different years varies
from year to year.

Both X-12-ARIMA/X-13ARIMA-SEATS and TRAMO/SEATS estimate calendar
effects by adding some regressors to the equation estimated in the
pre-processing part (RegARIMA or TRAMO, respectively). Regressors
mentioned above are generated from the default calendar or the user
defined calendar.

The calendars of JDemetra+ simply correspond to the usual trading days
contrast variables based on the Gregorian calendar, modified to take
into account some specific holidays. Those holidays are handled as
"Sundays" and the variables are properly adjusted to take into account
the long term mean effects.

\hypertarget{tests-for-residual-trading-days}{%
\subsection{Tests for residual trading
days}\label{tests-for-residual-trading-days}}

We consider below tests on the seasonally adjusted series (\(sa_t\)) or
on the irregular component (\(irr_t\)). When the reasoning applies on
both components, we will use \(y_t\). The functions \(stdev\) stands for
``standard deviation'' and \(rms\) for ``root mean squares''

The tests are computed on the log-transformed components in the case of
multiplicative decomposition.

TD are the usual contrasts of trading days, 6 variables (no specific
calendar).

\hypertarget{non-significant-irregular}{%
\subsubsection{Non significant
irregular}\label{non-significant-irregular}}

When \(irr_t\) is not significant, we don't compute the test on it, to
avoid irrelevant results. We consider that \(irr_t\) is significant if
\(stdev( irr_t)>0.01\) (multiplicative case) or if
\(stdev(irr_t)/rms(sa_t) >0.01\) (additive case).

\hypertarget{f-test}{%
\subsubsection{F test}\label{f-test}}

The test is the usual joint F-test on the TD coefficients, computed on
the following models:

\hypertarget{autoregressive-model-ar-modelling-option}{%
\paragraph{Autoregressive model (AR modelling
option)}\label{autoregressive-model-ar-modelling-option}}

We compute by OLS:

\[y_t=\mu + \alpha y_{t-1} + \beta TD_t + \epsilon_t \]

\hypertarget{difference-model}{%
\paragraph{Difference model}\label{difference-model}}

We compute by OLS:

\[\Delta y_t - \overline{\Delta y_t}=\beta TD_t + \epsilon_t \]

So, the latter model is a restriction of the first one
(\(\alpha =1, \mu =μ=\overline{\Delta y_t}\))

The tests are the usual joint F-tests on \(\beta \quad (H_0:\beta=0)\).

By default, we compute the tests on the 8 last years of the components,
so that they might highlight moving calendar effects.

Remark:

In Tramo, a similar test is computed on the residuals of the Arima
model. More exactly, the F-test is computed on
\(e_t=\beta TD_t + \epsilon_t\), where \(e_t\) are the one-step-ahead
forecast errors.

\hypertarget{outliers-and-intervention-variables}{%
\section{Outliers and intervention
variables}\label{outliers-and-intervention-variables}}

here just definition and purpose use in the regression in the reg-arima
regression part

Outliers{[}\^{}2{]} are abnormal values of a time series. In general,
they cannot be properly explained by the ARIMA model and its underlying
normality assumption. They tend to be associated with irregular special
events that produce a distortion in the series. The presence of such
values disturbs the modelling of time series with methods like
X-13ARIMA-SEATS and TRAMO-SEATS because of the linear procedures (e.g.
moving averages and regression analysis) implemented by them. The
presence of outliers has an adverse effect on the quality of seasonal
adjustment because outliers can lead to model misspecification, biased
parameter estimation, poor forecasts and inappropriate decomposition of
a series. Therefore, it is vital to identify and include them in the
modelling step of seasonal adjustment. The aim is to remove the effect
of outliers from a time series before its decomposition into its
components. Both X-13ARIMA-SEATS and TRAMO-SEATS include automatic
procedure for the treatment of outliers (detection and correction).
However, a priori information about an event that may have caused the
abnormal observations (the date of its occurrence and type of an effect)
can be included in the model by the user. This case study explains how
to do it.

In the automatic outlier detection and correction procedures, three
outlier types are considered by default:

\begin{itemize}
\item
  additive outlier (AO) -- an abnormal value in an isolated point of the
  series;
\item
  transitory change (TC) -- a series of outliers with a temporarily
  decreasing effect on the level of the series;
\item
  level shift (LS) -- series of innovation outliers with a constant
  long-term effect on the level of the series, where for an innovation
  outlier is meant an anomalous value in the innovation series.
\end{itemize}

Seasonal outliers, which are defined as an abrupt increase or decrease
of the seasonal component for a specific month or quarter and are of
permanent nature can be automatically detected once the user has chosen
the appropriate option. The relevant instructions are given in this case
study.

The user may also introduce into the model a ramp effect, which is
described as a smooth, linear transition between two time points unlike
the abrupt change associated with level shifts. This case study explains
how to add ramp effects into a specification.

The formulas that describe outliers are given
\href{../theory/SA_lin.html}{here}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The picture below presents the number of registered unemployed persons
  in Poland. It is clear that in the beginning of 1999 a sudden,
  permanent shift in the trend level took place as a result of the poor
  state of the economy. At the end of 2008 a single peak can be
  observed, which can be interpreted as a reaction by entrepreneurs to
  the beginning of the economic crisis.

  \begin{figure}

  {\centering \includegraphics{./All_images/UG_SA_image13.jpg}

  }

  \caption{Text}

  \end{figure}

  \textbf{Registered unemployed persons in Poland}
\end{enumerate}

(feed from estp ? + info on SO from \ldots)

\hypertarget{pre-adjustment}{%
\section{Pre-adjustment}\label{pre-adjustment}}

\hypertarget{overview-1}{%
\subsection{Overview}\label{overview-1}}

(edit) \#\#\#\# Modeliing part to adapt\\
The algorithms implemented in JDemetra+ enable a modelling of the
original time series with the RegARIMA model, including estimation of
the regression effects such as outliers and calendar effects. These
procedures can be used just for modelling and forecasting of the
original time series but also as a pre-treatment before performing a
seasonal adjustement of the series. Hence, this pre-treatement will
allow for a more reliable estimation of the time series components
performed by the seasonal adjustment procedures.

This section is divided into two parts: *
\href{../reference-manual/modelling-specifications.html}{Specifications},
which presents parameters of the modelling procedure. *
\href{../reference-manual/output-modelling.html}{Output}, which details
a typical output produced by the modelling procedure.

The specifications and output of the modelling procedure are displayed
in the \href{../reference-manual/workspace.html}{\emph{Workspace}
window}.

\begin{figure}

{\centering \includegraphics{./All_images/A_Ref_d1.jpg}

}

\caption{Text}

\end{figure}

\textbf{The \emph{Workspace} window with the nodes for the modelling
procedure marked}

\hypertarget{transformation-choices}{%
\subsection{Transformation choices}\label{transformation-choices}}

The log transformation of the original data is an option that is often
applied to achieve a stationary autocovariance function. The decision
concerning logging (or not) of a time series has a great impact on
seasonal adjustment outcomes\footnote{\emph{ESS Guidelines on Seasonal
  Adjustment} (2015).}. JDemetra+ offers two options: logging (which
means that the multiplicative decomposition is used) or no
transformation (the additive decomposition is used). The selection of
the transformation type can be done automatically, on the basis of the
outcome of a log-level test.

The test used by TRAMO-SEATS is based on the maximum likelihood
estimation of the parameter \(\lambda\) in the Box-Cox transformations
(which is a power transformations such that the transformed values of
time series \$\text{y }\$are a monotonic function of the observations,
i.e.:

\$\$y\_\{i\}\^{}\{\alpha\} = \left\{
\frac{\left( y_{i}^{\alpha} - 1 \right)}{\begin{matrix}



\lambda \\
\log{y_{i}^{\alpha},\lambda = 0\ } \\
\end{matrix}} \right.~,~\lambda \neq 0\$\$

The automatic procedure first fits two Airline models (i.e.~ARIMA
(0,1,1)(0,1,1)) on the time series: one in logs (\(\lambda = 0\)), the
other without logs (\(\lambda = 1\)). The test compares the sum of
squares of the model without logs with the sum of squares multiplied by
the square of the geometric mean from the model in logs. Logs are taken
in case the last function is the maximum\footnote{Gómez, V., and
  Maravall, A. (1998).}. The parameter \emph{fct} controls the bias in
the log/level pre-test (the function is active when \textbf{Function} is
set to \emph{Auto}); \emph{fct} \textgreater{} 1 favours levels,
\emph{fct} \textless{} 1 favours logs.
\protect\hyperlink{transformation}{This test is used for modelling with
the TRAMO model}.

\begin{figure}

{\centering \includegraphics{./All_images/UG_SA_image37.jpg}

}

\caption{Text}

\end{figure}

\textbf{The \emph{Transformation} options for the TRAMO-SEATS method}

The test used by X-13ARIMA-SEATS is based on the AICC information
criteria\footnote{Formula and further information available in
  Grudkowska, S. (2015).}. To choose the transformation type,
X-13ARIMA-SEATS fits the RegARIMA model to the untransformed and the
transformed series. X-13ARIMA-SEATS will choose the log transformation
except when\footnote{Description from \emph{Guide to seasonal adjustment
  with X-12-ARIMA} (2007).}:

\[\text{AICC}_{\log} - \text{AICC}_{\text{no\ log}} < \Delta_{\text{AICC}}\]

where:

\(\text{AICC}_{\text{no\ log}}\) is the value of AICC from fitting the
RegARIMA model to the untransformed series;

\(\text{AICC}_{\log}\) is the value of AICC from fitting the RegARIMA
model to the transformed series;

\(\Delta_{\text{AICC}}\) is the threshold value;
\(\Delta_{\text{AICC}}\)\textgreater{} 0 favours levels and
\(\Delta_{\text{AICC}}\) \textless{} 0 favours logs.

The RegARIMA model used in the test is the one specified in the ARIMA
part of the specification. If model is specified then the (0,1,1)(0,1,1)
model is used. \protect\hyperlink{transformation}{This test is used for
modelling with the RegARIMA model}.

\begin{figure}

{\centering \includegraphics{./All_images/UG_SA_image38.jpg}

}

\caption{Text}

\end{figure}

\textbf{The \emph{Transformation} options for the X-13ARIMA-SEATS
method}

According to the
\href{https://ec.europa.eu/eurostat/documents/3859598/6830795/KS-GQ-15-001-EN-N.pdf/d8f1e5f5-251b-4a69-93e3-079031b74bd3}{ESS
Guidelines on Seasonal Adjustment (2015)}, the automatic procedures
should be applied for the transformation choice, however in case of the
most problematic series the manual selection is recommended. The manual
selection of the transformation is usually made in the specifications
used for a regular data production.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  To determine the transformation choice first
  \protect\hyperlink{pre-defined-specifications}{create and open a new
  specification}.
\item
  For \emph{tramo} and \emph{tramoseats} specifications from the
  \emph{Transformation} section choose the \emph{function} option and
  input the \emph{fct} parameter's value. Click \emph{OK} to confirm
  your choice.

  \begin{figure}

  {\centering \includegraphics{./All_images/UG_SA_image39.jpg}

  }

  \caption{Text}

  \end{figure}

  \textbf{Transformation's option for the \emph{tramoseats}
  specification}
\item
  For the \emph{regarima} and \emph{x13} specifications from the
  \emph{Transformation} section choose the \emph{function} option and
  \emph{Aic difference} parameter's value. Click \emph{OK} to confirm
  your choice.

  \begin{figure}

  {\centering \includegraphics{./All_images/UG_SA_image40.jpg}

  }

  \caption{Text}

  \end{figure}

  \textbf{Transformation's option for \emph{X13} specification}

  It is also possible to change the transformation's options in the
  currently used specification (see
  \href{../case-studies/detailedsa-spec.html}{\emph{Defining and
  modifying a specification}} scenario, step 4).
\end{enumerate}

\hypertarget{reg-arima-linarization}{%
\subsection{Reg-Arima linarization}\label{reg-arima-linarization}}

Old page mixing * general info, parameters * gui display * pasted in GUI
chapter * here + trim the gui part + add link to R (rjdemetra, go
version 3 first but reference help pages)

The \emph{Model} node includes basic information about the outcome of
the model identification procedure and checking the goodness of fit. The
summary information about the final model is available directly from the
main \emph{Model} node. The content of this panel depends on the
\href{../reference-manual/modelling-specifications.html}{settings}
applied to the modelling procedure.

\begin{figure}

{\centering \includegraphics{./All_images/image22_RMSB.jpg}

}

\caption{Text}

\end{figure}

\textbf{The \emph{Model} node in the navigation tree}

The first part contains fundamental information about the model.

\begin{figure}

{\centering \includegraphics{./All_images/image23_RMSB.jpg}

}

\caption{Text}

\end{figure}

\textbf{The \emph{Summary} section of the \emph{Model} node}

\emph{Estimation span} informs about the first and the last observation
used for modelling. The notation of the estimation span varies according
to the frequencies (for example, the span \[2-1993 : 10-2006\]
represents a monthly time series and the span \[II-1994 : I-2011\]
represents a quarterly time series). The message \emph{Series has been
log-transformed} is only displayed if a logarithmic transformation has
been applied.

In the case of
\href{../reference-manual/modelling-specifications.html}{the pre-defined
specifications}: TR0, TR1, TR3, RG0, RG1 and RG3 no trading day effect
is estimated. For TR2, RG2c, TR4 and RG4c pre-defined specifications,
working day effects and the leap year effect are pre-tested and
estimated if present. If the working day effect is significant, the
pre-processing part includes the message \emph{Working days effect (1
regressor)}. The message \emph{Working days effect (2 regressors)} means
that the leap year effect has also been estimated. For TR5 and RG5c the
trading day effect and the leap year effect are pre-tested. If the
trading day effect has been detected, either of the messages
\emph{Trading days effect (6 regressors)} or \emph{Trading days effect
(7 regressors)} are displayed, depending whether the leap year effect
has been detected or not. If the Easter effect is statistically
significant, \emph{Easter effect detected} is displayed.

In this section the total number of detected outliers is displayed. The
additional information on detected outliers, i.e.~type, location and
coefficients' values, can be found in the \emph{Arima model} subsection
of the \emph{Model} node.

The \emph{Final model} section informs about the outcome of the
estimation process. \emph{Number of effective observations} is the
number of observations used to estimate the model, i.e.~the number of
observations of the transformed series (regularly and/or seasonally
differenced) reduced by the \emph{Number of estimated parameters}, which
is the sum of regular and seasonal parameters for both autoregressive
and moving average processes, mean effect, trading/working day effect,
outliers, regressors and one.

\emph{Likelihood} is a maximized value of a Likelihood\footnote{The
  likelihood function is the joint probability (density) function of
  observable random variables. It is viewed as the function of the
  parameters given the realized random variables. Therefore, this
  function measures the probability of observing the particular set of
  dependent variable values that occur in the sample.} function after
the iterations processed in Exact Maximum Likelihood Estimation, which
is a method used to estimate the model. This value is used by the model
selection criteria: \emph{AIC}, \emph{AICC}, \emph{BIC (corrected by
length)} and \emph{Hannan-Quinn}\footnote{\protect\hyperlink{model-selection-criteria}{AIC,
  AICC, BIC and Hannan-Quinn criteria are used by X-13ARIMA-SEATS while
  BIC (TRAMO definition)} by TRAMO/SEATS. These criteria are used in
  seasonal adjustment procedures for the selection of the optimum ARIMA
  model. The model with the smaller value of the model selection
  criteria is preferred.}. \emph{Standard error of the regression (ML
estimate)} is the standard error of the regression from Maximum
Likelihood Estimation\footnote{Maximum Likelihood Estimation is a
  statistical method for estimating the coefficients of a model. This
  method determines the parameters that maximize the probability
  (likelihood) of the sample data.}.

The \emph{scores at the solution} section presents the gradient of the
loglikelihood. The different items of the scores are related to the
different parameters of the ARIMA model. The output indicates to which
extent the optimization procedure reached the maximum. At the maximum of
the likelihood, it should be 0. However, it is never exactly the case,
due to numerical approximations. Usually, the scores can be improved by
using a higher precision (smaller tolerance). This precision is
controlled by the \textbf{Tolerance} parameter in the \textbf{Estimate}
section of the \emph{Specifications} window (see how to use this
parameter \protect\hyperlink{estimate}{for Tramo} and
\protect\hyperlink{estimate}{for Arima}).

An example of the output is presented in the chart below.

\begin{figure}

{\centering \includegraphics{./All_images/image24_RMSB.jpg}

}

\caption{Text}

\end{figure}

\textbf{The content of the \emph{Final model} section}

Next, the estimated values of model parameters (\emph{Coefficients}),
t-statistics (\emph{T-Stat}) and corresponding p-values
(\emph{P}\[\|T\|\>t\]) are displayed. JDemetra+ uses the following
notation:

\begin{itemize}
\item
  \emph{Phi(p)} -- the \textbackslash(p\^{}\{\text{th}\}\textbackslash)
  term in the non-seasonal autoregressive polynomial;
\item
  \emph{Theta(q)} -- the \(q^{\text{th}}\) term in the non-seasonal
  moving average polynomial;
\item
  \emph{BPhi(P)} -- the \(P^{\text{th}}\) term in the seasonal
  autoregressive polynomial;
\item
  \emph{BTheta(Q)} -- the \(Q^{\text{th}}\) term in the seasonal moving
  average polynomial.
\end{itemize}

In the example below, the ARIMA model (0,1,1)(0,1,1) was chosen, which
means that one regular and one seasonal moving average parameter were
identified and estimated. The p-values indicate that \emph{BTheta(1)}
parameter is significant in contrast to the \emph{Theta(1)}, which is
not significant \footnote{The variable is called statistically
  significant if it is so extreme that such a result would be expected
  to arise simply by chance only in rare circumstances (with the
  probability equal to p-value). Generally, the regressor is thought to
  be significant if p-value is lower than 5\%.}.

\begin{figure}

{\centering \includegraphics{./All_images/image25_RMSB.jpg}

}

\caption{Text}

\end{figure}

\textbf{The estimation's results of the ARIMA model}

For the
\protect\hyperlink{tramo-specification--options-for-manual-identification-of-the-arima-model}{fixed
ARIMA parameters}, JDemetra+ shows only the values of the parameters.
Figure below presents the output from the manually chosen ARIMA model
(2,0,0)(0,1,1) with a fixed parameter \emph{BTheta(1).} For the fixed
parameter the \emph{T-Stat} and
\emph{(P\textbar T\textbar\textgreater t)} are not displayed as no
estimation is done for this parameter.

\begin{figure}

{\centering \includegraphics{./All_images/image26_RMSB.jpg}

}

\caption{Text}

\end{figure}

\textbf{The results of the estimation of the ARIMA model with a fixed
coefficient}

If the ARIMA model contains a constant term (detected automatically or
introduced by the user), the estimated value and related statistics are
reported.

\begin{figure}

{\centering \includegraphics{./All_images/image27_RMSB.jpg}

}

\caption{Text}

\end{figure}

\textbf{The results of the estimation of a mean effect}

JDemetra+ presents estimated values of the coefficients of one or six
regressors depending on the type of a calendar effect specification. For
a working days effect one regressor is estimated.

\begin{figure}

{\centering \includegraphics{./All_images/image28_RMSB.jpg}

}

\caption{Text}

\end{figure}

\textbf{The results of the estimation of a working day effect}

When a trading days effect is estimated the \emph{Joint F-test} value is
reported under the table that presents estimated values. When the result
of \emph{Joint F-test} indicates that the trading day variables are
jointly not significant the test result is displayed in red.

\begin{figure}

{\centering \includegraphics{./All_images/image29_RMSB.jpg}

}

\caption{Text}

\end{figure}

\textbf{The results of the estimation of a trading day effect: the case
of jointly not significant variables}

In the example below the RSA5c specification has been used and a trading
day effect has been detected. In spite of the fact that some trading day
regressors are not significant at the 5\% significance level, the
outcome of the joint F-test indicates that the trading day regressors
are jointly significant (the F-test statistic is lower than 5\%).

\begin{figure}

{\centering \includegraphics{./All_images/image30_RMSB.jpg}

}

\caption{Text}

\end{figure}

\textbf{The results of the estimation of a trading day effect: the case
of jointly significant variables}

If a leap year regressor has been used in the model specification, the
value of the estimated leap year coefficient is also reported with the
corresponding t-statistics and p-value. As the p-value presented on the
picture below is greater than 0.05, it indicates that the leap year
effect is not significant.

\begin{figure}

{\centering \includegraphics{./All_images/image31_RMSB.jpg}

}

\caption{Text}

\end{figure}

\textbf{The results of the estimation of a leap year effect}

When the option \protect\hyperlink{regression}{\emph{UserDefined}} is
used, JDemetra+ displays the \emph{User-defined calendar variables}
section with variables and corresponding estimation results (the values
of the parameters, corresponding t-statistics and p-values). The outcome
of the joint F-test is displayed when more than one user-defined
calendar variable is used.

\begin{figure}

{\centering \includegraphics{./All_images/image32_RMSB.jpg}

}

\caption{Text}

\end{figure}

\textbf{The results of the estimation of user-defined calendar
variables}

When the Easter effect is estimated, the following table is displayed in
the output. In the case presented below Easter has a negative,
significant effect on the time series.

\begin{figure}

{\centering \includegraphics{./All_images/image33_RMSB.jpg}

}

\caption{Text}

\end{figure}

\textbf{The results of the estimation of the Easter effect}

JDemetra+ also presents the results of an outlier detection procedure.
The table includes the type of outlier, its date, the value of the
coefficient and corresponding t-statistics and p-values.

\begin{figure}

{\centering \includegraphics{./All_images/image34_RMSB.jpg}

}

\caption{Text}

\end{figure}

\textbf{The results of the outlier identification procedure}

In all pre-defined specifications, except for TR0 and RG0, only additive
outliers, temporary changes and level shifts are considered in the
automatic outlier identification procedure. When seasonal outliers are
also enabled, they appear in the same table as other outliers.

\begin{figure}

{\centering \includegraphics{./All_images/image35_RMSB.jpg}

}

\caption{Text}

\end{figure}

\textbf{The results of the outlier identification procedure that enables
seasonal outliers}

Results for \protect\hyperlink{regression}{pre-specified outliers} are
displayed in a separate table.

\begin{figure}

{\centering \includegraphics{./All_images/image36_RMSB.jpg}

}

\caption{Text}

\end{figure}

\textbf{The results of the estimation of the pre-specified outliers}

Regression variables, like ramps and intervention variables, are not
identified automatically. They need to be defined by the user. The
results of an estimation of ramps that are pre-defined types of
regression variables are displayed in a separate table. All information
concerning ramps, including spans, estimated coefficients and related
statistics, is shown in a separate table.

\begin{figure}

{\centering \includegraphics{./All_images/image37_RMSB.jpg}

}

\caption{Text}

\end{figure}

\textbf{The results of the estimation of the ramp effect}

All other intervention variables with corresponding statistics are shown
under the \emph{Intervention variable(s)} table.

\begin{figure}

{\centering \includegraphics{./All_images/image38_RMSB.jpg}

}

\caption{Text}

\end{figure}

\textbf{The results of the estimation of the intervention variable}

User-defined variables are marked as \emph{Vars-1.x\_1, Vars-1.x\_2},
\ldots, \emph{Vars-1.x\_n} and displayed in the separate tables.

\begin{figure}

{\centering \includegraphics{./All_images/image39_RMSB.jpg}

}

\caption{Text}

\end{figure}

\textbf{The results of the estimation of the user-defined variables}

JDemetra+ also reports a list of missing observations, if any. JDemetra+
applies the AO approach to the estimation of the missing
observations\footnote{Missing observations are treated as additive
  outliers and interpolated accordingly.}.

\begin{figure}

{\centering \includegraphics{./All_images/image40_RMSB.jpg}

}

\caption{Text}

\end{figure}

\textbf{The results of the estimation of the missing observations}

Detailed results are divided into several sections and are investigated
in the following sections: -
\href{../reference-manual/forecasts.html}{Forecasts} -
\href{../reference-manual/regressors.html}{Regressors} -
\href{../reference-manual/arima.html}{ARIMA} -
\href{../reference-manual/pre-adjustment-series.html}{Pre-adjustment
series} - \href{../reference-manual/residuals.html}{Residuals} -
\href{../reference-manual/likelihood.html}{Likelihood}

\hypertarget{foreasts-out-of-sample}{%
\subsection{Foreasts Out-of sample}\label{foreasts-out-of-sample}}

One of the important steps in of the a validation of the model is an
analysis of the out-of-sample forecasts\footnote{PEÑA, D. (2001).}.
Using the identified TRAMO model JDemetra+ produces the point forecasts,
the forecast standard errors, and a prediction interval. The prediction
interval on the transformed scale is denoted
as\(\ point\ forecast \pm K \times forecast\ standard\ error\), where
\(K\) is the standard error multiplier taken from a table of the normal
distribution, corresponding to the specified coverage probability.
JDemetra+ displays 95\% prediction interval, which corresponds to
\(K = 1.96.\) The forecasting procedure assumes that no outliers appear
in the forecasting period.

The graph presenting time series together with the values forecasted for
the next year and corresponding prediction interval can be found
directly in the \emph{Forecast} node.

\begin{figure}

{\centering \includegraphics{./All_images/image41_RMSB.png}

}

\caption{Text}

\end{figure}

\textbf{The \emph{Forecast} panel with visual presentation of the
estimated forecasts}

The local menu offers the copy and export options, including sending the
graph to the printer and save the graph as clipboard or a file in the
PNG format. The \emph{Show precision gradient} option highlights the
precision of the estimation using different shades of orange. As a rule,
the precision decreases in time, which is depicted by gradually more
intense orange. The \emph{Copy all series} option enables the user to
export time series together with the forecasts and the prediction
intervals to the another application.

\begin{figure}

{\centering \includegraphics{./All_images/image42_RMSB.jpg}

}

\caption{Text}

\end{figure}

\textbf{The \emph{Forecast} panel with visual presentation of the
precision of the estimated forecasts}

The \emph{Forecast} node is divided into two subsections: \emph{Table}
and \emph{Out-of-sample test}. The \emph{Table} section presents the
forecasts as a table.

\begin{figure}

{\centering \includegraphics{./All_images/image43_RMSB.png}

}

\caption{Text}

\end{figure}

\textbf{A local menu for the content of the \emph{Table} panel}

A standard local menu, which is available for this table, includes:

\begin{itemize}
\item
  \textbf{Select all} -- copies series and allows it to be pasted to the
  another application e.g.~into Excel.
\item
  \textbf{Transpose} -- changes the orientation of the table from
  horizontal to vertical.
\item
  \textbf{Reverse chronology} -- displays the series from last to first
  observation.
\item
  \textbf{Single time series} -- when it is marked observations are
  divided by calendar's periods. Otherwise, data are presented as a
  standard time series.
\item
  \textbf{Edit format} -- allows changing the format used for displaying
  dates and values.
\item
  \textbf{Use color scheme} -- allows series values to be displayed in
  colour.
\item
  \textbf{Color scheme} -- allows for a choice of colour scheme from a
  pre-specified list.
\item
  \textbf{Show bars} -- presents values in a table as horizontal bars.
\item
  \textbf{Show crosshair} -- highlights an active cell.
\item
  \textbf{Zoom} -- an option for modifying the chart size.
\end{itemize}

\textbf{Paste} and \textbf{Clear} are disabled as they are not relevant
for this view.

When a time series is marked, a local menu offers the following options:

\begin{itemize}
\item
  \textbf{Open} -- opens selected time series in the new window that
  contain \emph{Chart} and \emph{Grid} panels.
\item
  \textbf{Open with} -- opens the time series in a separate window
  according to the user choice (\emph{Chart} \emph{\& grid} or
  \emph{Simple chart}). The \emph{All ts views} option is not currently
  available.
\item
  \textbf{Save} -- saves the marked series in a spreadsheet file or in a
  text file.
\item
  \textbf{Rename} -- enables the user to change the time series name.
\item
  \textbf{Copy} -- copies series and allows it to be pasted to another
  application e.g.~into Excel.
\item
  \textbf{Paste} -- pastes the time series previously marked.
\item
  \textbf{Select all} -- selects all time series presented in the graph.
\item
  \textbf{Transpose} -- changes the orientation of the table from
  horizontal to vertical.
\item
  \textbf{Reverse chronology} -- displays the series from the last to
  the first observation.
\item
  \textbf{Single time series} -- removes from the table all time series
  apart from the selected one.
\item
  \textbf{Clear} -- removes all time series from the chart.
\item
  \textbf{Edit format} -- enables the user to change a data format.
\item
  \textbf{Use color scheme} -- allows the series to be displayed in
  colour.
\item
  \textbf{Color scheme} -- allows the colour scheme used in the graph to
  be changed.
\item
  \textbf{Show bars} -- presents values in a table as horizontal bars.
\item
  \textbf{Show crosshair} -- highlights an active cell.
\item
  \textbf{Zoom} -- option for modifying the chart size.
\end{itemize}

\begin{figure}

{\centering \includegraphics{./All_images/image44_RMSB.png}

}

\caption{Text}

\end{figure}

\textbf{A local menu for a selected series in the \emph{Table} panel}

The \emph{Out-of-sample test} section presents two tests. These tests
check the performance of the out-of-sample forecasts, by comparison of
forecast limits and the data. To perform this exercise, first the
historical data sample is divided into a fit period (whole estimation
span excluding last 18 observations) and a test period (last 18
observations). For the fit period the model is automatically selected
and its parameters are fixed. The model is used to produce a
one-period-ahead forecast (i.e.~the forecast for \(\mathbf{n + 1}\),
where \(\mathbf{n}\) is the length of the time series). This estimation
is performed 18 times for the gradually extended time series.

The first test compares forecast errors with the standard error of the
in-sample residuals. The goodness of fit is assessed by checking if the
mean of the forecast errors can be assumed zero.

The second test compares mean squared of forecast error with mean
squared of in-sample residuals. The result of the test is accepted when
these two indicators are close to each other. The lack of consistency is
a clear evidence of a model inadequacy.

\begin{figure}

{\centering \includegraphics{./All_images/image45_RMSB.jpg}

}

\caption{Text}

\end{figure}

\textbf{An example of out-of-sample test results}

\hypertarget{decomposition}{%
\section{Decomposition}\label{decomposition}}

\hypertarget{x-11-moving-average-based-decomposition}{%
\subsection{X-11 moving average based
decomposition}\label{x-11-moving-average-based-decomposition}}

A complete documentation of the X-11 method is available in LADIRAY, D.,
and QUENNEVILLE, B. (2001). The X-11 program is the result of a long
tradition of non-parametric smoothing based on moving averages, which
are weighted averages of a moving span of a time series (see hereafter).
Moving averages have two important drawbacks:

\begin{itemize}
\item
  They are not resistant and might be deeply impacted by outliers;
\item
  The smoothing of the ends of the series cannot be done except with
  asymmetric moving averages which introduce phase-shifts and delays in
  the detection of turning points.
\end{itemize}

These drawbacks adversely affect the X-11 output and stimulate the
development of this method. To overcome these flaws first the series are
modelled with a RegARIMA model that calculates forecasts and estimates
the regression effects. Therefore, the seasonal adjustment process is
divided into two parts.

\begin{itemize}
\item
  In a first step, the RegARIMA model is used to clean the series from
  \textgreater{} non-linearities, mainly outliers and calendar effects.
  A global \textgreater{} ARIMA model is adjusted to the series in order
  to compute the \textgreater{} forecasts.
\item
  In a second step, an enhanced version of the X-11 algorithm is used
  \textgreater{} to compute the trend, the seasonal component and the
  irregular \textgreater{} component.
\end{itemize}

\begin{figure}

{\centering \includegraphics{./All_images/UG_A_image13.png}

}

\caption{Text}

\end{figure}

\textbf{The flow diagram for seasonal adjustment with X-13ARIMA-SEATS
using the X-11 algorithm.}

\hypertarget{moving-averages}{%
\subsubsection{Moving averages}\label{moving-averages}}

The moving average of coefficient \(\theta_{i}\) is defined as:

\[M\left( X_{t} \right) = \sum_{k = - p}^{+ f}\theta_{k}X_{t + k}\]

\[1\]

The value at time \(t\) of the series is therefore replaced by a
weighted average of \(p\) ``past'' values of the series, the current
value, and \(f\) ``future'' values of the series. The quantity \$p + f +
1\$is called the moving average order. When \(p\) is equal to \(f\),
that is, when the number of points in the past is the same as the number
of points in the future, the moving average is said to be centred. If,
in addition, \(\theta_{- k} = \theta_{k}\) for any \(k\), the moving
average \(M\) is said to be symmetric. One of the simplest moving
averages is the symmetric moving average of order \(P = 2p + 1\) where
all the weights are equal to\(\ \frac{1}{P}\).

This moving average formula works well for all time series observations,
except for the first \(p\) values and last \(f\) values. Generally, with
a moving average of order \$p + f + 1\$calculated for instant \(t\)nwith
points \(p\) in the past and points \(f\) in the future, it will be
impossible to smooth out the first \(p\) values and the last \(f\)
values of the series because of lack of input to the moving average
formula.

In the X-11 method, symmetric moving averages play an important role as
they do not introduce any phase-shift in the smoothed series. But, to
avoid losing information at the series ends, they are either
supplemented by \emph{ad hoc} asymmetric moving averages or applied on
the series extended by forecasts.

For the estimation of the seasonal component, X-13ARIMA-SEATS uses
\(P \times Q\) composite moving averages, obtained by composing a simple
moving average of order \(P\), which coefficients are all equal to
\(\frac{1}{P}\), and a simple moving average of order \(Q\), which
coefficients are all equal to \(\frac{1}{Q}\).

The composite moving averages are widely used by the X-11 method. For an
initial estimation of trend X-11 method uses a \(2 \times 4\) moving
average in case of a quarterly time series while for a monthly time
series a \$2 \times 12\$moving average is applied. The \(2 \times 4\)
moving average is an average of order 5 with coefficients
\[\frac{1}{8}\left\{1, 2, 2, 2, 1\right\}\]. It eliminates frequency

\(\frac{\pi}{2}\) corresponding to period 4 and therefore it is suitable
for seasonal adjustment of the quarterly series with a constant
seasonality. The \(2 \times 12\) moving average, with coefficients

\[\frac{1}{24}\left\{1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1\right\} \]that
retains linear trends, eliminates order-\(12\) constant seasonality and
minimises the variance of the irregular component. The \(2 \times 4\)
and \(2 \times 12\) moving averages are also used in the X-11 method to
normalise the seasonal factors. The composite moving averages are also
used to extract the seasonal component. These, which are used in the
purely automatic run of the X-11 method (without any intervention from
the user) are \(3 \times 3\), \(3 \times 5\) and \(3 \times 9\).

In the estimation of the trend also Henderson moving averages are used.
These filters have been chosen for their smoothing properties. The
coefficients of a Henderson moving average of order \(2p + 1\) may be
calculated using the formula:

\(\theta_{i} = \frac{315\left\lbrack \left( n - 1 \right)^{2} - i^{2} \right\rbrack\left\lbrack n^{2} - i^{2} \right\rbrack\left\lbrack \left( n + 1 \right)^{2} - i^{2} \right\rbrack\left\lbrack {3n}^{2} - 16 - 11i^{2} \right\rbrack}{8n\left( n^{2} - 1 \right)\left( {4n}^{2} - 1 \right)\left( {4n}^{2} - 9 \right)\left( 4n^{2} - 25 \right)}\),
\[2\]

where: \(n = p + 2\)\(n = p + 2\).

\hypertarget{the-basic-algorithm-of-the-x-11-method}{%
\subsubsection{The basic algorithm of the X-11
method}\label{the-basic-algorithm-of-the-x-11-method}}

The X-11 method is based on an iterative principle of estimation of the
different components using appropriate moving averages at each step of
the algorithm. The successive results are saved in tables. The list of
the X-11 tables displayed in JDemetra+ is included at the end of this
section.

The basic algorithm of the X-11 method will be presented for a monthly
time series \(X_{t}\) that is assumed to be decomposable into trend,
seasonality and irregular component according to an additive model
\(X_{t} = TC_{t} + S_{t} + I_{t}\).

A simple seasonal adjustment algorithm can be thought of in eight steps.
The steps presented below are designed for the monthly time series. In
the algorithm that is run for the quarterly time series the
\(2 \times 4\) moving average instead of the \(2 \times 12\) moving
average is used.

\textbf{\emph{Step 1: Estimation of Trend by}} \(\mathbf{2 \times 12}\)
\textbf{\emph{moving average:}}

\(TC_{t}^{(1)} = M_{2 \times 12}(X_{t})\) \[3\]

\textbf{\emph{Step 2: Estimation of the Seasonal-Irregular component:}}

\(\left( S_{t} + I_{t} \right)^{(1)} = X_{t} - \text{TC}_{t}^{(1)}\)

\[4\]

\textbf{\emph{Step 3: Estimation of the Seasonal component by}}
\(\mathbf{3 \times 3}\) \textbf{\emph{moving average over each month:}}

\(S_{t}^{(1)} - M_{3 \times 3}\left\lbrack \left( S_{t} + I_{t} \right)^{(1)} \right\rbrack\)
\[5\]

The moving average used here is a \(3 \times 3\) moving average over
\(5\) terms, with coefficients
\[\frac{1}{9} \left\{1, 2, 3, 2, 1 \right\}\]. The seasonal component is
then centred using a \(2 \times 12\) moving average.

\[
   \widetilde{S}_{t}^{(1)} = S_{t}^{(1)} - M_{2 \times 12}\left( S_{t}^{(1)} \right)
  \] \[6\]

\textbf{\emph{Step 4: Estimation of the seasonally adjusted series:}}

\$\$

SA\_\{t\}\^{}\{\left( 1 \right)\} = \left( \text{TC}\emph{\{t\} +
I}\{t\} \right)\^{}\{(1)\} = X\_\{t\} -
\{\widetilde{S}\}\_\{t\}\^{}\{(1)\} \[ \]7\$\$

This first estimation of the seasonally adjusted series must, by
construction, contain less seasonality. The X-11 method again executes
the algorithm presented above, changing the moving averages to take this
property into account.

\textbf{\emph{Step 5: Estimation of Trend by 13-term Henderson moving
average:}}

\[
  TC_{t}^{(2)} = H_{13}\left( \text{SA}_{t}^{\left( 1 \right)} \right)
  \] \[8\]

Henderson moving averages, while they do not have special properties in
terms of eliminating seasonality (limited or none at this stage), have a
very good smoothing power and retain a local polynomial trend of degree
\(2\) and preserve a local polynomial trend of degree \(3\).

\textbf{\emph{Step 6: Estimation of the Seasonal-Irregular component:}}

\$\$

\left( S\_\{t\} + I\_\{t\} \right)\^{}\{(2)\} = X\_\{t\} -
\text{TC}\_\{t\}\^{}\{(2)\} \[ \]9\$\$

\textbf{\emph{Step 7: Estimation of the Seasonal component by}}
\(\mathbf{3 \times 5}\) \textbf{\emph{moving average over each month:}}

\[S_{t}^{(2)} - M_{3 \times 3}\left\lbrack \left( S_{t} + I_{t} \right)^{(2)} \right\rbrack\]

\[10\]

The moving average used here is a \(3 \times 5\) moving average over
\(7\) terms, of coefficients
\[\frac{1}{15} \left\{ 1,\ 2,\ 3,\ 3,\ 3,\ 2,\ 1 \right\}\] and retains

linear trends. The coefficients are then normalised such that their sum
over the whole \(12\)-month period is approximately cancelled out:

\[{ \widetilde{S}}_{t}^{(2)} = S_{t}^{(2)} - M_{2 \times 12}\left( S_{t}^{(2)} \right)\]

\[11\]

\textbf{\emph{Step 8: Estimation of the seasonally adjusted series:}}

\[SA_{t}^{\left( 2 \right)} = \left(TC_{t} + I_{t} \right)^{(2)} = X_{t} - {\widetilde{S}}_{t}^{(2)}\]

\[12\]

The whole difficulty lies, then, in the choice of the moving averages
used for the estimation of the trend in steps \(1\) and \(5\) on the one
hand, and for the estimation of the seasonal component in steps \(3\)
and \(5\). The course of the algorithm in the form that is implemented
in JDemetra+ is presented in the figure below. The adjustment for
trading day effects, which is present in the original X-11 program, is
omitted here, as since calendar correction is performed by the RegARIMA
model, JDemetra+ does not perform further adjustment for these effects
in the decomposition step.

\textbf{A workflow diagram for the X-11 algorithm based upon training
material from the Deutsche Bundesbank}

\hypertarget{the-iterative-principle-of-x-11}{%
\paragraph{\texorpdfstring{\textbf{The iterative principle of
X-11}}{The iterative principle of X-11}}\label{the-iterative-principle-of-x-11}}

To evaluate the different components of a series, while taking into
account the possible presence of extreme observations, X-11 will proceed
iteratively: estimation of components, search for disruptive effects in
the irregular component, estimation of components over a corrected
series, search for disruptive effects in the irregular component, and so
on.

The Census X-11 program presents four processing stages (A, B, C, and
D), plus 3 stages, E, F, and G, that propose statistics and charts and
are not part of the decomposition per se. In stages B, C and D the basic
algorithm is used as is indicated in the figure below.

\textbf{A workflow diagram for the X-11 algorithm implemented in
JDemetra+. Source: Based upon training material from the Deutsche
Bundesbank}

\begin{itemize}
\tightlist
\item
  \textbf{Part A: Pre-adjustments}
\end{itemize}

This part, which is not obligatory, corresponds in X-13ARIMA-SEATS to
the first cleaning of the series done using the RegARIMA facilities:
detection and estimation of outliers and calendar effects (trading day
and Easter), forecasts and backcasts{[}\^{}61{]} of the series. Based on
these results, the program calculates prior adjustment factors that are
applied to the raw series. The series thus corrected, Table B1 of the
printouts, then proceeds to part B.

\begin{itemize}
\tightlist
\item
  \textbf{Part B: First automatic correction of the series}
\end{itemize}

This stage consists of a first estimation and down-weighting of the
extreme observations and, if requested, a first estimation of the
calendar effects. This stage is performed by applying the basic
algorithm detailed earlier. These operations lead to Table B20,
adjustment values for extreme observations, used to correct the
unadjusted series and result in the series from Table C1.

\begin{itemize}
\tightlist
\item
  \textbf{Part C: Second automatic correction of the series}
\end{itemize}

Applying the basic algorithm once again, this part leads to a more
precise estimation of replacement values of the extreme observations
(Table C20). The series, finally ``cleaned up'', is shown in Table D1 of
the printouts.

\begin{itemize}
\tightlist
\item
  \textbf{Part D: Seasonal adjustment}
\end{itemize}

This part, at which our basic algorithm is applied for the last time, is
that of the seasonal adjustment, as it leads to final estimates:

\begin{itemize}
\item
  of the seasonal component (Table D10);
\item
  of the seasonally adjusted series (Table D11);
\item
  of the trend component (Table D12);
\item
  of the irregular component (Table D13).
\end{itemize}

\begin{itemize}
\tightlist
\item
  \textbf{Part E: Components modified for large extreme values}
\end{itemize}

Parts E includes:

\begin{itemize}
\item
  Components modified for large extreme values;
\item
  Comparison the annual totals of the raw time series and seasonally
  adjusted time series;
\item
  Changes in the final seasonally adjusted series;
\item
  Changes in the final trend;
\item
  Robust estimation of the final seasonally adjusted series.
\end{itemize}

The results from part E are used in part F to calculate the quality
measures.

\begin{itemize}
\tightlist
\item
  \textbf{Part F: Seasonal adjustment quality measures}
\end{itemize}

Part F contains statistics for judging the quality of the seasonal
adjustment. JDemetra+ presents selected output for part F, i.e.:

\begin{itemize}
\item
  M and Q statistics;
\item
  Tables.
\end{itemize}

\begin{itemize}
\tightlist
\item
  \textbf{Part G: Graphics}
\end{itemize}

Part G presents spectra estimated for:

\begin{itemize}
\item
  Raw time series adjusted a priori (Table B1);
\item
  Seasonally adjusted time series modified for large extreme values
  (Table E2);
\item
  Final irregular component adjusted for large extreme values (Table
  E3).
\end{itemize}

Originally, graphics were displayed in character mode. In JDemetra+,
these graphics are replaced favourably by the usual graphics software.

\textbf{The Henderson moving average and the trend estimation}

In iteration B (Table B7), iteration C (Table C7) and iteration D (Table
D7 and Table D12) the trend component is extracted from an estimate of
the seasonally adjusted series using Henderson moving averages. The
length of the Henderson filter is chosen automatically by
X-13ARIMA-SEATS in a two-step procedure.

It is possible to specify the length of the Henderson moving average to
be used. X-13ARIMA-SEATS provides an automatic choice between a 9-term,
a 13-term or a 23-term moving average. The automatic choice of the order
of the moving average is based on the value of an indicator called
\$\frac{\overline{I}}{\overline{C}}\$ratio which compares the magnitude
of period-on-period movements in the irregular component with those in
the trend. The larger the ratio, the higher the order of the moving
average selected. Moreover, X-13ARIMA-SEATS allows the user to choose
manually any odd‑numbered Henderson moving average. The procedure used
in each part is very similar; the only differences are the number of
options available and the treatment of the observations in the both ends
of the series. The procedure below is applied for a monthly time series.

In order to calculate \$\frac{\overline{I}}{\overline{C}}\$ ratio a
first decomposition of the SA series (seasonally adjusted) is computed
using a 13-term Henderson moving average.

For both the trend (\(C\)) and irregular (\(I\)) components, the average
of the absolute values for monthly growth rates (multiplicative model)
or for monthly growth (additive model) are computed. They are denoted as
\$\overline{C}\$and \(\overline{I}\), receptively, where
\(\overline{C} = \frac{1}{n - 1}\sum_{t = 2}^{n}\left| C_{t} - C_{t - 1} \right|\)
and
\(\overline{I} = \frac{1}{n - 1}\sum_{t = 2}^{n}\left| I_{t} - I_{t - 1} \right|\).

Then the value of \$\frac{\overline{I}}{\overline{C}}\$ ratio is checked
and in iteration B:

\begin{itemize}
\item
  If the ratio is smaller than 1, a 9-term Henderson moving average is
  selected;
\item
  Otherwise, a 13-term Henderson moving average is selected.
\end{itemize}

Then the trend is computed by applying the selected Henderson filter to
the seasonally adjusted series from Table B6. The observations at the
beginning and at the end of the time series that cannot be computed by
means of symmetric Henderson filters are estimated by ad hoc asymmetric
moving averages.

In iterations C and D:

\begin{itemize}
\item
  If the ratio is smaller than 1, a 9-term Henderson moving average is
  selected;
\item
  If the ratio is greater than 3.5, a 23-term Henderson moving average
  is selected.
\item
  Otherwise, a 13-term Henderson moving average is selected.
\end{itemize}

The trend is computed by applying selected Henderson filter to the
seasonally adjusted series from Table C6, Table D7 or Table D12,
accordingly. At the both ends of the series, where a central Henderson
filter cannot be applied, the asymmetric ends weights for the 7 term
Henderson filter are used.

\hypertarget{choosing-the-composite-moving-averages-when-estimating-the-seasonal-component}{%
\paragraph{\texorpdfstring{\textbf{Choosing the composite moving
averages when estimating the seasonal
component}}{Choosing the composite moving averages when estimating the seasonal component}}\label{choosing-the-composite-moving-averages-when-estimating-the-seasonal-component}}

In iteration D, Table D10 shows an estimate of the seasonal factors
implemented on the basis of the modified SI (Seasonal -- Irregular)
factors estimated in Tables D4 and D9bis. This component will have to be
smoothed to estimate the seasonal component; depending on the importance
of the irregular in the SI component, we will have to use moving
averages of varying length as in the estimate of the Trend/Cycle where
the \$\frac{\overline{I}}{\overline{C}}\$ ratio was used to select the
length of the Henderson moving average. The estimation includes several
steps.

\textbf{\emph{Step 1: Estimating the irregular and seasonal components}}

An estimate of the seasonal component is obtained by smoothing, month by
month and therefore column by column, Table D9bis using a simple 7-term
moving average, i.e.~of coefficients

\[\frac{1}{7} \left\{1,\ 1,\ 1,\ 1,\ 1,\ 1,\ 1\right\}\]. In order not
to lose three points at the beginning and end of each column, all
columns are completed as follows. Let us assume that the column that
corresponds to the month is composed of \(N\) values \[
\left\{ x_{1},\ x_{2},\ x_{3},\ \ldots x_{N - 1},\ x_{N} \right\}.
\] It will be transformed into a series

\[\left\{ {x_{- 2},x_{- 1}{,x}_{0},x}_{1},\ x_{2},\ x_{3},\ \ldots x_{N - 1},\ x_{N},x_{N + 1},\ x_{N + 1},\ x_{N + 2},\ x_{N + 3} \right\}\\]

with \[x_{- 2} = x_{- 1} = x_{0} = \frac{x_{1} + x_{2} + x_{3}}{3}\] and

\[x_{N + 1} = x_{N + 2} = x_{N + 3} = \frac{x_{N} + x_{N - 1} + x_{N - 2}}{3}\].
We then have the required estimates: \(S = M_{7}(D9bis)\) and
\(I = D9bis - S\).

\textbf{\emph{Step 2: Calculating the Moving Seasonality Ratios}}

For each \(i^{\text{th}}\) month the mean annual changes for each
component is obtained by calculating

\[{\overline{S}}_{i} = \frac{1}{N_{i} - 1}\sum_{t = 2}^{N_{i}}\left| S_{i,t} - S_{i,t - 1} \right|\]

and
\[{\overline{I}}_{i} = \frac{1}{N_{i} - 1}\sum_{t = 2}^{N_{i}}\left| I_{i,t} - I_{i,t - 1} \right|\],

where \(N_{i}\) refers to the number of months \(\text{i}\)in the data,
and the moving seasonality ratio of month \(i\):

\[MSR_{i} = \frac{\ {\overline{I}}_{i}}{ {\overline{S}}_{i}}\]. These
ratios are presented in \emph{Details} of the \emph{Quality Measures}
node under the \emph{Decomposition (X11)} section. These ratios are used
to compare the year-on-year changes in the irregular component with
those in the seasonal component. The idea is to obtain, for each month,
an indicator capable of selecting the appropriate moving average for the
removal of any noise and providing a good estimate of the seasonal
factor. The higher the ratio, the more erratic the series, and the
greater the order of the moving average should be used. As for the rest,
by default the program selects the same moving average for each month,
but the user can select different moving averages for each month.

\textbf{\emph{Step 3: Calculating the overall Moving Seasonality Ratio}}

The overall Moving Seasonality Ratio is calculated as follows:

\[\text{MSR}_{i} = \frac{\sum_{i}^{}{N_{i}\ }\ {\overline{I}}_{i}}{\sum_{i}^{}N_{i}{\overline{S}}_{i}}\]

\[13\]

\textbf{\emph{Step 4: Selecting a moving average and estimating the
seasonal component}}

Depending on the value of the ratio, the program automatically selects a
moving average that is applied, column by column (i.e.~month by month)
to the Seasonal/Irregular component in Table D8 modified, for extreme
values, using values in Table D9.

The default selection procedure of a moving average is based on the
Moving Seasonality Ratio in the following way:

\begin{itemize}
\item
  If this ratio occurs within zone A (MSR \textless{} 2.5), a
  \(3 \times 3\) moving average is used; if it occurs within zone C (3.5
  \textless{} MSR \textless{} 5.5), a \(3 \times 5\) moving average is
  selected; if it occurs within zone E (MSR~\textgreater~6.5), a
  \(3 \times 9\) moving average is used;
\item
  If the MSR occurs within zone B or D, one year of observations is
  removed from the end of the series, and the MSR is recalculated.
  \textgreater{} If the ratio again occurs within zones B or D, we start
  over \textgreater{} again, removing a maximum of five years of
  observations. If this \textgreater{} does not work, i.e.~if we are
  again within zones B or D, a \textgreater{} \(3 \times 5\) moving
  average is selected.
\end{itemize}

The chosen symmetric moving average corresponds, as the case may be 5
(\(3 \times 3\)), 7 \$(3 \times 5)\$or 11 (\(3 \times 9\)\(3 \times 9)\)
terms, and therefore does not provide an estimate for the values of
seasonal factors in the first 2 (or 3 or 5) and the last 2 (or 3 or 5)
years. These are then calculated using associated asymmetric moving
averages.

\textbf{Moving average selection procedure, source: DAGUM, E. B.(1999)}

\hypertarget{identification-and-replacement-of-extreme-values}{%
\paragraph{\texorpdfstring{\textbf{Identification and replacement of
extreme
values}}{Identification and replacement of extreme values}}\label{identification-and-replacement-of-extreme-values}}

X-13ARIMA-SEATS detects and removes outliers in the RegARIMA part.
However, if there is a seasonal heteroscedasticity in a time series i.e.
the variance of the irregular component is different in different
calendar months. Examples for this effect could be the weather and
snow-dependent output of the construction sector in Germany during
winter, or changes in Christmas allowances in Germany and resulting from
this a transformation in retail trade turnover before Christmas. The
ARIMA model is not on its own able to cope with this characteristic. The
practical consequence is given by the detection of additional extreme
values by X-11. This may not be appropriate if the seasonal
heteroscedasticity is produced by political interventions or other
influences. The ARIMA models assume a constant variance and are
therefore not by themselves able to cope with this problem. Choosing
longer (in the case of diverging weather conditions in the winter time
for the construction sector) or shorter filters (in the case of a
changing pattern of retail trade turnover in the Christmas time) may be
reasonable in such cases. It may even be sensible to take into account
the possibility of period-specific (e.g.~month-specific) standard
deviations, which can be done by changing the default settings of the
\textbf{calendarsigma} parameter (see
\href{../reference-manual/sa-spec-X13.html}{Specifications-X13}
section). The value of the \textbf{calendarsigma} parameter will have an
impact on the method of calculation of the moving standard deviation in
the procedure for extreme values detection presented below.

\textbf{\emph{Step 1: Estimating the seasonal component}}

The seasonal component is estimated by smoothing the SI component
separately for each period using a \(3 \times 3\) moving average, i.e.:

\[
  \frac{1}{9} \times \begin{Bmatrix}   
  1,0,0,0,0,0,0,0,0,0,0,0, \\           
  2,0,0,0,0,0,0,0,0,0,0,0, \\           
  3,0,0,0,0,0,0,0,0,0,0,0, \\           
  2,0,0,0,0,0,0,0,0,0,0,0, \\           
  1,0,0,0,0,0,0,0,0,0,0,0, \\           
  \end{Bmatrix}
  \] \[14\]

\textbf{\emph{Step 2: Normalizing the seasonal factors}}

The preliminary seasonal factors are normalized in such a way that for
one year their average is equal to zero (additive model) or to unity
(multiplicative model).

\textbf{\emph{Step 3: Estimating the irregular component}}

The initial normalized seasonal factors are removed from the
Seasonal-Irregular component to provide an estimate of the irregular
component.

\textbf{\emph{Step 4: Calculating a moving standard deviation}}

By default, a moving standard deviation of the irregular component is
calculated at five-year intervals. Each standard deviation is associated
with the central year used to calculate it. The values in the central
year, which in the absolute terms deviate from average by more than the
\textbf{Usigma} parameter are marked as extreme values and assigned a
zero weight. After excluding the extreme values the moving standard
deviation is calculated once again.

\textbf{\emph{Step 5: Detecting extreme values and weighting the
irregular}}

The default settings for assigning a weight to each value of irregular
component are:

\begin{itemize}
\item
  Values which are more than \textbf{Usigma} (2.5, by default) standard
  deviations away (in the absolute terms) from the 0 (additive) or 1
  (multiplicative) are assigned a zero weight;
\item
  Values which are less than 1.5 standard deviations away (in the
  absolute terms) from the 0 (additive) or 1 (multiplicative) are
  assigned a full weight (equal to one);
\item
  Values which lie between 1.5 and 2.5 standard deviations away (in the
  absolute terms) from the 0 (additive) or 1 (multiplicative) are
  assigned a weight that varies linearly between 0 and 1 depending on
  their position.
\end{itemize}

The default boundaries for the detection of the extreme values can be
changed with \textbf{LSigma} and \textbf{USigma} parameters

\textbf{\emph{Step 6: Adjusting extreme values of the seasonal-irregular
component}}

Values of the SI component are considered extreme when a weight less
than 1 is assigned to their irregular. Those values are replaced by a
weighted average of five values:

\begin{itemize}
\item
  The value itself with its weight;
\item
  The two preceding values, for the same period, having a full weight(if
  available);
\item
  The next two values, for the same period, having full a weight (if
  available).
\end{itemize}

When the four full-weight values are not available, then a simple
average of all the values available for the given period is taken.

This general algorithm is used with some modification in parts B and C
for detection and replacement of extreme values.

\hypertarget{x-11-tables}{%
\paragraph{\texorpdfstring{\textbf{X-11
tables}}{X-11 tables}}\label{x-11-tables}}

The list of tables produced by JDemetra+ is presented below. It is not
identical to the output produced by the original X-11 program.

\textbf{Part A -- Preliminary Estimation of Outliers and Calendar
Effects.}

This part includes prior modifications to the original data made in the
RegARIMA part:

\begin{itemize}
\item
  Table A1 -- Original series;
\item
  Table A1a -- Forecast of Original Series;
\item
  Table A2 -- Leap year effect;
\item
  Table A6 -- Trading Day effect (1 or 6 variables);
\item
  Table A7 -- The Easter effect;
\item
  Table A8 -- Total Outlier Effect;
\item
  Table A8i -- Additive outlier effect;
\item
  Table A8t -- Level shift effect;
\item
  Table A8s -- Transitory effect;
\item
  Table A9 -- Effect of user-defined regression variables assigned to
  the seasonally adjusted series or for which the component has not been
  defined;
\item
  Table 9sa -- Effect of user-defined regression variables assigned to
  the seasonally adjusted series;
\item
  Table9u -- Effect of user-defined regression variables for which the
  component has not been defined.
\end{itemize}

\textbf{Part B -- Preliminary Estimation of the Time Series Components:}

\begin{itemize}
\item
  Table B1 -- Original series after adjustment by the RegARIMA model;
\item
  Table B2 -- Unmodified Trend (preliminary estimation using composite
  moving average);
\item
  Table B3 -- Unmodified Seasonal -- Irregular Component (preliminary
  estimation);
\item
  Table B4 -- Replacement Values for Extreme SI Values;
\item
  Table B5 -- Seasonal Component;
\item
  Table B6 -- Seasonally Adjusted Series;
\item
  Table B7 -- Trend (estimation using Henderson moving average);
\item
  Table B8 -- Unmodified Seasonal -- Irregular Component;
\item
  Table B9 -- Replacement Values for Extreme SI Values;
\item
  Table B10 -- Seasonal Component;
\item
  Table B11 -- Seasonally Adjusted Series;
\item
  Table B13 -- Irregular Component;
\item
  Table B17 -- Preliminary Weights for the Irregular;
\item
  Table B20 -- Adjustment Values for Extreme Irregulars.
\end{itemize}

\textbf{Part C -- Final Estimation of Extreme Values and Calendar
Effects:}

\begin{itemize}
\item
  Table C1 -- Modified Raw Series;
\item
  Table C2 -- Trend (preliminary estimation using composite moving
  average);
\item
  Table C4 -- Modified Seasonal -- Irregular Component;
\item
  Table C5 -- Seasonal Component;
\item
  Table C6 -- Seasonally Adjusted Series;
\item
  Table C7 -- Trend (estimation using Henderson moving average);
\item
  Table C9 -- Seasonal -- Irregular Component;
\item
  Table C10 -- Seasonal Component;
\item
  Table C11 -- Seasonally Adjusted Series;
\item
  Table C13 -- Irregular Component;
\item
  Table C20 -- Adjustment Values for Extreme Irregulars.
\end{itemize}

\textbf{Part D -- Final Estimation of the Different Components:}

\begin{itemize}
\item
  Table D1 -- Modified Raw Series;
\item
  Table D2 -- Trend (preliminary estimation using composite moving
  average);
\item
  Table D4 -- Modified Seasonal -- Irregular Component;
\item
  Table D5 -- Seasonal Component;
\item
  Table D6 -- Seasonally Adjusted Series;
\item
  Table D7 -- Trend (estimation using Henderson moving average);
\item
  Table D8 -- Unmodified Seasonal -- Irregular Component;
\item
  Table D9 -- Replacement Values for Extreme SI Values;
\item
  Table D10 -- Final Seasonal Factors;
\item
  Table D10A -- Forecast of Final Seasonal Factors;
\item
  Table D11 -- Final Seasonally Adjusted Series;
\item
  Table D11A -- Forecast of Final Seasonally Adjusted Series;
\item
  Table D12 -- Final Trend (estimation using Henderson moving average);
\item
  Table D12A -- Forecast of Final Trend Component;
\end{itemize}

\begin{itemize}
\item
  Table D13 -- Final Irregular Component;
\item
  Table D16 -- Seasonal and Calendar Effects;
\item
  Table D16A -- Forecast of Seasonal and Calendar Component;
\item
  Table D18 -- Combined Calendar Effects Factors.
\end{itemize}

\textbf{Part E -- Components Modified for Large Extreme Values:}

\begin{itemize}
\item
  Table E1 -- Raw Series Modified for Large Extreme Values;
\item
  Table E2 -- SA Series Modified for Large Extreme Values;
\item
  Table E3 -- Final Irregular Component Adjusted for Large Extreme
  Values;
\item
  Table E11 -- Robust Estimation of the Final SA Series.
\end{itemize}

\textbf{Part F -- Quality indicators:}

\begin{itemize}
\item
  Table F2A -- Changes, in the absolute values, of the principal
  components;
\item
  Table F2B -- Relative contribution of components to changes in the raw
  series;
\item
  Table F2C -- Averages and standard deviations of changes as a function
  of the time lag;
\item
  Table F2D -- Average duration of run;
\item
  Table F2E -- I/C ratio for periods span;
\item
  Table F2F -- Relative contribution of components to the variance of
  the stationary part of the original series;
\item
  Table F2G -- Autocorrelogram of the irregular component.
\end{itemize}

\hypertarget{filter-length-choice}{%
\subsubsection{Filter length choice}\label{filter-length-choice}}

A seasonal filter is a weighted average of a moving span of fixed length
within a time series that can be used to remove a fixed seasonal
pattern. X-13ARIMA-SEATS uses several of these filters, according to the
needs of the different stages of the program. As only X-13ARIMA-SEATS
allows the user to manually select seasonal filters, this case study can
be applied only to the X-13ARIMA-SEATS specifications.

The automatic seasonal adjustment procedure uses the default options to
select the most appropriate moving average. However there are occasions
when the user will need to specify a different seasonal moving\\
average to that identified by the program. For example, if the SI values
do not closely follow the seasonal component, it may be appropriate to
use a shorter moving average. Also the presence of sudden breaks in the
seasonal pattern -- e.g.~due to changes in the methodology -- can
negatively impact on the automatic selection of the most appropriate
seasonal filter. In such cases the usage of short seasonal filters in
the selected months or quarters can be considered. Usually, a shorter
seasonal filter \((3 \times 1)\) allows seasonality to change very
rapidly over time. However, a very short seasonal filter should not
normally be used, as it might often lead to large revisions as new data
becomes available. If a short filter is to be used it will usually be
limited to one month/quarter with a known reason for wanting to capture
a rapidly changing seasonality.

In the standard situation one seasonal filter is applied to all
individual months/quarters. The estimation of seasonal movements is
therefore based on the sample windows of equal lengths for each
individual month/quarter (i.e.~for each month/quarter the seasonal
filter length or the number of years representing the major part of the
seasonal filter weights is identical). This approach relies on the
assumption that the number of past periods in which the conditions
causing seasonal behaviour are sufficiently homogenous is the same in
all months/quarters. However, this assumption does not always hold.
Seasonal causes may change in one month, while staying the same in
others\footnote{When the series are non-stationary differentiation is
  performed before the seasonality tests.}. For instance, seasonal
heteroskedasticity might require different filter lengths in different
months or quarters.

Another interesting example is industrial production in Germany. It can
be influenced by school holidays, since many employees have school-age
children, which interrupt their working pattern during these school
holidays. Consequently, businesses may temporarily suspend or lower
production during these periods. Since school holidays do not occur at
the same time throughout Germany and their timing varies from year to
year in the individual federal states, the effect is not completely
captured by seasonal adjustment. And since school holidays are treated
as usual working days, these effects are not captured by calendar
adjustment either. The majority of school holidays in Germany can take
place either in July or in August. This yields higher variances in the
irregular component for these months compared to the rest of the year.
Therefore, in this case a longer seasonal filter is used for these
months to account for this.

Another example might be given by German retail trade. Due to changes in
the consumers' behaviour around Christmas -- possibly more gifts of
money -- the seasonal peak in December has become steadily less
pronounced. To account for this moving seasonality, shorter seasonal
filters in December than during the rest of the year need to be applied.

JDemetra+ offers the options to assign a different seasonal filter
length to each period (month or quarter). The program offers these
options in the \emph{single spec} mode as well as in the
\emph{multispec} mode, albeit they are available only in the
\emph{Specifications} window, after a document is created.

\hypertarget{model-based-decomposition}{%
\subsection{Model based decomposition}\label{model-based-decomposition}}

SEATS is a program for estimating unobserved components in a time
series. It follows the ARIMA-model-based (AMB) method, developed from
the work of CLEVELAND, W.P., and TIAO, G.C. (1976), BURMAN, J.P. (1980),
HILLMER, S.C., and TIAO, G.C. (1982), BELL, W.R., and HILLMER, S.C.
(1984) and MARAVALL, A., and PIERCE, D.A. (1987).

In JDemetra+ the input for the model based signal extraction procedure
is always provided by TRAMO and includes the original series \(y_{t}\),
the linearized series \(x_{t}\) (i.e.~the original series
\$y\_\{t\}\$with the deterministic effects removed), the ARIMA model for
the stochastic (linearized) time series \(x_{t}\) and the deterministic
effects (calendar effects, outliers and other regression variable
effects)\footnote{In the original software SEATS can be used either with
  TRAMO, operating on the input received from the latter, or alone,
  fitting an ARIMA model to the series.}. SEATS decomposes the
linearized series (and the ARIMA model) into trend, seasonal, transitory
and irregular components, provides forecasts for these components,
together with the associated standard errors, and finally assign the
deterministic effects to each component yielding the \emph{final}
components\footnote{GÓMEZ, V., and MARAVALL, A. (1998).}. The Minimum
Mean Square Error (MMSE) estimators of the components are computed with
a Wiener-Kolmogorov filter applied to the finite series extended with
forecasts and backcasts\footnote{GÓMEZ, V., and MARAVALL, A. (1997).}.

One of the fundamental assumptions made by SEATS is that the linearized
time series \(x_{t}\) follows the ARIMA model

\[\phi(B)\delta\left( B \right)x_{t} = \theta(B)a_{t}\] \[1\]

where:

\(B\) -- the backshift operator \((Bx_{t} = x_{t - 1})\);

\(\delta\left( B \right)\) -- a non-stationary autoregressive (AR)
polynomial in \(B\) (unit roots);

\(\theta\left( B \right)\) -- an invertible moving average (MA)
polynomial in \(B\) and in \(B^{S}\), which can be expressed in the
multiplicative form

\(\left( 1 + \vartheta_{1}B + \ldots{+ \ \vartheta}{q}B^{q} \right)\left( \ 1 + \Theta{1}B^{s} + \ldots{+ \ \Theta}_{Q}B^{\text{sQ}} \right)\)
;

\(\phi(B)\) -- a stationary autoregressive (AR) polynomial in \(B\) and
in \$B\^{}\{S\}\$containing regular and seasonal unit roots, with
\emph{s} representing the number of observations per year;

\(a_{t}\) -- a white-noise variable with the variance\(\ V(a)\).

It should be noted that the stochastic time series can be predicted
using its past observations and making an error. The variable \(a_{t}\),
which is assumed to be white noise, is the fundamental \emph{innovation}
to the series at time \emph{t}, that is the part that cannot be
predicted based on the past history of the series.

Denoting \$\varphi\left( B \right) = \phi\left( B \right)\delta\left( B
\right),\$ \[1\] can be written in a more concise form as

\[\varphi\left( B \right)x_{t} = \theta(B)a_{t}\], \[2\]

where \(\varphi\left( B \right)\) contains both the stationary and the
nonstationary roots.

\hypertarget{derivation-of-the-models-for-the-components}{%
\subsubsection{Derivation of the models for the
components}\label{derivation-of-the-models-for-the-components}}

Let us consider the additive decomposition model

\[x_{t} = \sum_{i = 1}^{k}x_{\text{it}}\], \[3\]

where \emph{i} refers to the orthogonal components: trend, seasonal,
transitory or irregular. Apart from the irregular component, supposed to
be a white noise, it is assumed that each component follows the ARIMA
model which can be represented, using the notation of \[2\] , as:

\[\varphi_{i}\left( B \right)\ x_{\text{it}} = \theta_{i}(B)a_{\text{it}}\],

\[4\]

where
\(\varphi_{i}\left( B \right) = \phi_{i}\left( B \right)\delta_{i}\left( B \right),\ \ x_{\text{it}}\)
is the \emph{i}-\emph{th} unobserved component,
\(\varphi_{i}\left( B \right)\) and \(\theta_{i}\left( B \right)\) are
finite polynomials of order \(p_{i}\) and \(q_{i}\), respectively, and
\(a_{\text{it}},\) the disturbance associated with such component, is a
white noise process with zero mean and constant variance \(V(a_{i})\)
and \(a_{\text{it}}\) and \$a\_\{\text{jt}\}\$are not correlated for \$i
\neq j\$and for any \(t\).. These disturbances are functions of the
innovations in the series and are called ``pseudo-innovations'' in the
literature concerning the AMB decomposition as they refer to the
components that are never observed \footnote{GÓMEZ, V., and MARAVALL, A.
  (2001a).}. In the JDemetra+ documentation the term ``innovations'' is
used to refer to the ``pseudo-innovations''.

The following assumptions hold for \[4\] . For each \(\text{i}\) the
polynomials \(\phi_{i}\left( B \right)\), \(\delta_{i}\left( B \right)\)
and \(\theta_{i}(B)\) are prime and of finite order. The roots of
\(\delta_{i}\left( B \right)\) lies on the unit circle; those of
\(\phi_{i}\left( B \right)\) lie outside, while all the roots of
\$\theta\_\{i\}\left( B \right)\$are on or outside the unit circle. This
means that nonstationary and noninvertible components are allowed. Since
different roots of the AR polynomial induce peaks in the
spectrum\footnote{For description of the spectrum see section
  \href{../theory/spectral.html}{Spectral Analysis}.} of the series at
different frequencies, and given that different components are
associated with the spectral peaks for different frequencies, it is
assumed that for \(i \neq j\) the
polynomials\(\ \phi_{i}\left( B \right)\) and
\(\phi_{j}\left( B \right)\) do not share any common root (they are
coprime). Finally, it is assumed that the polynomials
\(\theta_{i}\left( B \right),\ i = 1,\ldots,k\) are prime share no unit
root in common, guaranteeing the invertibility of the overall series. In
fact, since the unit root of \(\theta_{i}\left( B \right)\) induce a
spectral zero, when the polynomials
\(\theta_{i}\left( B \right),\ i = 1,\ldots,k\) share no unit root in
common, there is no frequency for which all component spectra become
zero\footnote{MARAVALL, A. (1995).}.

Since aggregation of ARIMA models yields ARIMA models, the series
\$x\_\{t\}\$will also follow an ARIMA model, as in \[2\] , and
consequently the following identity can be derived:

\[\frac{\theta(B)}{\varphi(B)}a_{t} = \sum_{i = 1}^{k}{\frac{\theta_{i}(B)}{\varphi_{i}(B)}a_{\text{it}}}\].

\[5\]

In the ARIMA model based approach implemented in SEATS, the ARIMA model
identified and estimated for the observed series \(x_{t}\) is decomposed
to derive the models for the components. In particular, the AR
polynomials for the components, \(\varphi_{i}\left( B \right),\) are
easily derived through the factorization of the AR polynomial
\(\varphi\left( B \right)\):

\[\varphi\left( B \right) = \prod_{i = 1}^{k}{\varphi_{i}\left( B \right)}\],

\[6\]

while the MA polynomials for the components, together with the
innovation variances \(V(a_{i})\), cannot simply be obtained through the
relationship:

\[\theta(B)a_{t} = \sum_{i = 1}^{k}{\varphi_{\text{ni}}\left( B \right)}\theta_{i}(B)a_{\text{it}}\],

\[7\]

where \(\varphi_{\text{ni}}\left( B \right)\) is the product of all
\(\varphi_{j}\left( B \right),\ j = 1,\ldots,k\), except from
\(\varphi_{i}\left( B \right)\). Further assumptions are therefore
needed to cope with the underidentification problem: i)
\(p_{i} \geq q_{i}\) and ii) the canonical decomposition, i.e.~the
decomposition that allocate all additive white noise to the irregular
component (yielding noninvertible components except the irregular).

To understand how SEATS factorizes the AR polynomials, first a concept
of a root will be explored\footnote{Description based on KAISER, R., and
  MARAVALL, A. (2000) and MARAVALL, A. (2008c).}.

The equation \[2\] can be expressed as:

\[\psi^{- 1}(B)x_{t} = a_{t}(1 + \varphi_{1}B + \ldots\varphi_{p}B^{p})x_{t} =(1 + \theta_{1}B + \ldots\theta_{q}B^{q})a_{t}\],

\[8\]

Let us now consider \[2\] in the inverted form:

\[\theta\left( B \right)y_{t} = \varphi(B)a_{t}\], \[9\]

If both sides of \[8\] are multiplied by \(x_{t - k}\) with \(k > q\),
and expectations are taken, the right hand side of the equation vanishes
and the left hand side becomes:

\[\varphi(B)\gamma_{k} = \gamma_{k} + \varphi_{1}\gamma_{k - 1} + \ldots\varphi_{p}\gamma_{k - p} = 0 \],

\[10\]

where \(B\) operates on the subindex \(k\).

The autocorrelation function \(\gamma_{k}\) is a solution of \[10\] with
the characteristic equation:

\[z^{p} + \varphi_{1}z^{p - 1} + \ldots\varphi_{p - 1}z + \varphi_{p} = 0\].

\[11\]

If \(z_{1}\),\ldots,\(\ z_{p}\) are the roots of \[11\] , the solutions
of \[10\] can be expressed as:

\(\gamma_{k} = \sum_{i = 1}^{p}z_{i}^{k}\), \[12\]

and will converge to zero as \(k \rightarrow \infty\) when
\(\left| r_{i} \right| < 1,\ i = 1,\ldots,p\). From \[10\] and \[12\] it
can be noticed that \(z_{1} = B_{i}^{- 1}\), meaning that
\(z_{1}\),\ldots,\(\ z_{p}\) are the inverses of the roots
\(B_{1},\ldots,B_{p}\) of the polynomial \(\varphi(B)\). The convergence
of \(\gamma_{k}\) implies that the roots of the \(\varphi(B)\) are
larger than 1 in modulus (lie outside the unit circle). Therefore, from
the equation

\$\$

\{\varphi(B)\}\^{}\{- 1\} = \frac{1}{(1 - z_{1})\ldots(1 - z_{1})}
\[ \]13\$\$

it can be derived that \({\varphi(B)}^{- 1}\) is convergent and all its
inverse roots are less than 1 in modulus.

Equation \[11\] has real and complex roots (solutions). Complex number
\(x = a + bi\), with \(a\) and \(\text{b}\) both real numbers, can be
represented as \(x = r\left( cos(\omega) + i\ sin(\omega \right))\),
where \(i\) is the imaginary unit\({\ (i}^{2} = - 1)\), \(r\) is the
modulus of \(x\), that is
\(\ r = \left| x \right| = \sqrt{a^{2} + b^{2}}\) and \(\omega\) is the
argument (frequency). When roots are complex, they are always in pairs
of complex conjugates. The representation of the complex number
\(x = a + bi\) has a geometric interpretation in the complex plane
established by the real axis and the orthogonal imaginary axis.

\begin{figure}

{\centering \includegraphics{./All_images/UG_A_image3.png}

}

\caption{Text}

\end{figure}

\textbf{Geometric representation of a complex number and of its
conjugate}

Representing the roots of the characteristic equation \[11\] in the
complex plane enhances understanding how they are allocated to the
components. When the modulus \(r\) of the roots in \(\text{z}\) are
greater than 1 (i.e.~modulus of the roots in \(\varphi(B)\  < 1\)), the
solution of the characteristic equation has a systematic explosive
process, which means that the impact of the given impulse on the time
series is more and more pronounced in time. This behaviour is not in
line with the developments that can be identified in actual economic
series. Therefore, the models estimated by TRAMO-SEATS (and
X-13ARIMA-SEATS) have never inverse roots in \(B\) with modulus greater
than 1.

The characteristic equations associated with the regular and the
seasonal differences have roots in \(\varphi(B)\) with modulus
\(r = 1\). They are called non-stationary roots and can be represented
on the unit circle. Let us consider the seasonal differencing operator
applied to a quarterly time series \((1 - B^{4})\). Its characteristic
equation is \({(z}^{4} - 1) = 0\) with solutions given
by\(\ z = \sqrt[4]{1}\), i.e. \(z_{1,2} = \pm 1\) and
\(z_{3,4} = \pm i1\). The first two solutions are real and the last two
are complex conjugates. They are represented by the black points on the
unit circle on the figure below.

\begin{figure}

{\centering \includegraphics{./All_images/UG_A_image4.png}

}

\caption{Text}

\end{figure}

\textbf{Unit roots on the unit circle}

For the seasonal differencing operator \((1 - B^{12})\) applied to the
monthly time series the characteristic equation \({\ (z}^{12} - 1) = 0\)
has twelve non-stationary solutions given by\(\ z = \sqrt[12]{1}:\) two
real and ten complex conjugates, represented by the white circles in
unit roots figure above.

The complex conjugates roots generate the periodic movements of the
type:

\[z_{t} = A^{t}\cos\left( \omega t + W \right).\] \[14\]

where:

\(A\) -- amplitude;

\(\omega\) -- angular frequency (in radians);

\(W\) -- phase (angle at \(t = 0)\).

The frequency \(f\), i.e.~the number of cycles per unit time, is
\(\frac{\omega}{2\pi}\). If it is multiplied by \emph{s}, the number of
observations per year, the number of cycles completed in one year is
derived. The period of function \[14\] , denoted by \(\tau\), is the
number of units of time (months/quarters) it takes for a full circle to
be completed.

For quarterly series the seasonal movements are produced by complex
conjugates roots with angular frequencies at \(\frac{\pi}{2}\) (one
cycle per year) and \(\pi\) (two cycles per year). The corresponding
number of cycles per year and the length of the movements are presented
in the table below.

\textbf{Seasonal frequencies for a quarterly time series}

\{: .table .table-style\} \textbar{} \textbf{Angular frequency
(}\(\omega\)) \textbar{} \textbf{Frequency (cycles per unit time)
(}\(f\)) \textbar{} \textbf{Cycles per year} \textbar{} \textbf{Length
of the movement measured in quarters (}\(\tau\)) \textbar{}
\textbar-----------------\textbar-----------------\textbar-----------------\textbar-----------------\textbar{}
\textbar{} \(\frac{\pi}{2}\) \textbar{} 0.25 \textbar{} 1 \textbar{} 4
\textbar{} \textbar{} \(\pi\) \textbar{} 0.5 \textbar{} 2 \textbar{} 2
\textbar{}

For monthly time series the seasonal movements are produced by complex
conjugates roots at the angular frequencies:
\$~\frac{\pi}{6},\frac{\pi}{3},~\frac{\pi}{2},~\frac{2\pi}{3},~\frac{5\pi}{6}\$and
\(\pi\). The corresponding number of cycles per year and the length of
the movements are presented in the table below: Seasonal frequencies for
a monthly time series.

\textbf{Seasonal frequencies for a monthly time series}

\{: .table .table-style\} \textbar{} \textbf{Angular frequency
(}\(\omega\)) \textbar{} \textbf{Frequency (cycles per unit time)
(}\(f\)) \textbar{} \textbf{Cycles per year} \textbar{} \textbf{Length
of the movement measured in months (}\(\tau\)) \textbar{}
\textbar-----------------\textbar-----------------\textbar-----------------\textbar-----------------\textbar{}
\textbar{} \(\frac{\pi}{6}\) \textbar{} 0.083 \textbar{} 1 \textbar{} 12
\textbar{} \textbar{} \(\frac{\pi}{3}\) \textbar{} 0.167 \textbar{} 2
\textbar{} 6 \textbar{} \textbar{} \(\frac{\pi}{2}\) \textbar{} 0.250
\textbar{} 3 \textbar{} 4 \textbar{} \textbar{} \(\frac{2\pi}{3}\)
\textbar{} 0.333 \textbar{} 4 \textbar{} 3 \textbar{} \textbar{}
\(\frac{5\pi}{6}\) \textbar{} 0.417 \textbar{} 5 \textbar{} 2.4
\textbar{} \textbar{}

\[\pi\] \textbar{} 0.500 \textbar{} 6 \textbar{} 2 \textbar{}

In JDemetra+ SEATS assigns the roots of the AR full polynomial to the
components according to their associated modulus and frequency,
i.e.:\footnote{For details see MARAVALL, A., CAPORELLO, G., PÉREZ, D.,
  and LÓPEZ, R. (2014).}

\begin{itemize}
\item
  Roots of \(\left( 1 - B \right)^{d}\) are assigned to trend component.
\item
  Roots of
  \(\ \left( 1 - B^{s} \right)^{d_{s}} = {((1 - B)(1 + B + \ldots + B^{s - 1}))}^{d_{s}}\$are assigned to the trend component (root of\)\{~\left(
  1 - B
  \right)\}\^{}\{d\_\{s\}\}\() and to the seasonal component (roots of\)\{~(1
  + B + \ldots + B\^{}\{s - 1\})\}\^{}\{d\_\{s\}\}\$).
\item
  When the modulus of the inverse of a real positive root of
  \(\varphi(B)\) is greater than \(k\) or equal to \(k\), where \(k\) is
  the threshold value controlled by the \emph{Trend boundary}
  parameter(in the original SEATS it is controlled by
  \emph{rmod})\footnote{In JDemetra+ this argument is called \emph{Trend
    boundary}.}, then the root is assigned to the trend component.
  Otherwise it is assigned to the transitory component.
\item
  Real negative inverse roots of \$\text{ ϕ}\_\{p\}\left( B
  \right)\$associated with the seasonal two-period cycle are assigned to
  the seasonal component if their modulus is greater than \emph{k},
  where \(k\) is the threshold value controlled by the \emph{Seasonal
  boundary} and the \emph{Seas. boundary (unique)} parameters. Otherwise
  they are assigned to the transitory component.
\item
  Complex roots, for which the argument (angular frequency) is close
  enough to the seasonal frequency are assigned to the seasonal
  component. Closeness is controlled by the \emph{Seasonal tolerance}
  and \emph{Seasonal tolerance (unique)} parameters (in the original
  SEATS it is controlled by \emph{epsphi}). Otherwise they are assigned
  to the transitory component.
\item
  If \(d_{s}\$(seasonal differencing order) is present\)\$and
  \(\text{Bphi} < 0\) (\(\text{Bphi}\) is the estimate of the seasonal
  autoregressive parameter), the real positive inverse root is assigned
  to the trend component and the other (\(s - 1\)) inverse roots are
  assigned to the seasonal component. When \(d_{s} = 0\), the root is
  assigned to the seasonal when \(\text{Bphi} < - 0.2\) and/or the
  overall test for seasonality indicates presence of seasonality.
  Otherwise it goes to the transitory component. Also, when
  \(\text{Bphi} > 0\), roots are assigned to the transitory component.
\end{itemize}

For further details about JDemetra+ parameters see section
\href{../reference-manual/sa-spec-tramo.html}{TramoSeats}.

It should be highlighted that when\(\ Q > P\), where \(Q\) and \(P\)
denote the orders of the polynomials \(\varphi\left( B \right)\) and
\(\theta(B)\), the SEATS decomposition yields a pure MA \((Q - P)\)
component (hence transitory). In this case the transitory component will
appear even when there is no AR factor allocated to it.

Once these rules are applied, the factorization of the AR polynomial
presented by \[2\] yields to the identification of the AR polynomials
for the components which contain, respectively, the AR roots associated
with the trend component, the seasonal component and the transitory
component.\footnote{The AR roots close to or at the trading day
  frequency generates a stochastic trading day component. A stochastic
  trading day component is always modelled as a stationary ARMA(2,2),
  where the AR part contains the roots close to the TD frequency, and
  the MA(2) is obtained from the model decomposition (MARAVALL, A., and
  PÉREZ, D. (2011)). This component, estimated by SEATS, is not
  implemented by the current version of JDemetra+.}

Then with the partial fraction expansion the spectrum of the final
components are obtained.

For example, the Airline model for a monthly time series:

\[(1 - B)(1 - B^{12})x_{t} = (1 + \theta_{1}B)(1 + \Theta_{1}B^{12})\ a_{t}\],

\[15\]

is decomposed by SEATS into the model for the trend component:

\[(1 - B)(1 - B)c_{t} = (1 + \theta_{c,1}B + \theta_{c,2}B^{2})a_{c,t}\],

\[16\]

and the model for the seasonal component:

\[\left( 1 + B + \ldots + B^{11} \right)s_{t} = \left( 1 + \theta_{s,1}B + \ldots + {\theta_{s,11}B}^{11} \right)a_{s,t},\]

\[17\]

As a result, the Airline model is decomposed as follows:

\[\frac{(1 + \theta_{1}B)(1 + \Theta_{1}B^{12})}{(1 - B)(1 - B)}a_{t} = \frac{\left( 1 + \theta_{s,1}B + \ldots + {\theta_{s,11}B}^{11} \right)}{\left( 1 + B + \ldots + B^{11} \right)}a_{s,t} + \frac{(1 + \theta_{c,1}B + \theta_{c,2}B^{2})}{(1 - B)(1 - B)}a_{c,t} + u_{t}\].
\[18\]

The transitory component is not present in this case and the irregular
component is the white noise.

The partial fractions decomposition is performed in a frequency domain.
In essence, it consists in portioning of the pseudo-spectrum\footnote{The
  term pseudo-spectrum is used for a non-stationary time series, while
  the term spectrum is used for a stationary time series.} of \(x_{t}\)
into additive spectra of the components. When the AMB decomposition of
the ARIMA model results in the non-negative spectra for all components,
the decomposition is called admissible\footnote{If the ARIMA model
  estimated in TRAMO does not accept an admissible decomposition, SEATS
  replaces it with a decomposable approximation. The modified model is
  therefore used to decompose the series. There are also other rare
  situations when the ARIMA model chosen by TRAMO is changed by SEATS.
  It happens when, for example, the ARIMA models generate unstable
  seasonality or produce a senseless decomposition. Such examples are
  discussed by MARAVALL, A. (2009).}. In such case an infinite number of
admissible decompositions exists, i.e. decompositions that yield the
non-negative spectra of all components. Therefore, the MA polynomials
and the innovation variances cannot be yet identified from the model of
\(x_{t}\). As sketched above, to solve this underidentification problem
and identify a unique decomposition, it is assumed that for each
component the order of the MA polynomial is no greater than the order of
the AR polynomial and the canonical solution of S.C. Hillmer and G.C.
Tiao is applied\footnote{HILLMER, S.C., and TIAO, G.C. (1982).}, i.e.
all additive white noise is added to the irregular component As a
consequence all components derived from the canonical decomposition,
except from the irregular, have a spectral minimum of zero and are thus
noninvertible\footnote{GÓMEZ, V., and MARAVALL, A. (2001a).}. Given the
stochastic features of the series, it can be shown by that the canonical
decomposition produces as stable as possible trend and seasonal
components since it maximizes the variance of the irregular and
minimizes the variance of the other components\footnote{HILLMER, S.C.,
  and TIAO, G.C. (1982).}. However, there is a price to be paid as
canonical components can produce larger revisions in the preliminary
estimators of the component\footnote{MARAVALL, A. (1986).} than any
other admissible decomposition.

The figure below represents the pseudo-spectrum for the canonical trend
and an admissible trend.

\begin{figure}

{\centering \includegraphics{./All_images/UG_A_image5.png}

}

\caption{Text}

\end{figure}

\textbf{A comparison of canonical trend and admissible trend}

A pseudo-spectrum is denoted by\(\ g_{i}(\omega)\), where \(\omega\)
represents the angular frequency. The pseudo-spectrum of
\(x_{\text{it}}\) is defined as the Fourier transform of ACGF
of\(\ x_{t}\) which is expressed as:

\[\frac{\psi_{i}\left( B \right)\psi_{i}\left( F \right)}{\delta_{i}\left( B \right)\delta_{i}\left( F \right)}V(a_{i})\],
\[19\]

where:

\(\psi_{i}\left( F \right) = \frac{\theta_{i}\left( F \right)}{\phi_{i}\left( F \right)}\)

\(\psi_{i}\left( B \right) = \frac{\theta_{i}\left( B \right)}{\phi_{i}\left( B \right)}\)

\(B\) is the backward operator,

\(F\) is the forward operator.

A pseudo-spectrum for a monthly time series \$x\_\{t\}\$is presented in
the figure below: The pseudo-spectrum for a monthly series. The
frequency \(\omega = 0\) is associated with the trend, frequencies in
the range

\[$0 + \epsilon_{1},\ \frac{\pi}{6} - \epsilon_{2}\]\$ with
\(\left[0 + \epsilon_{1},\ \frac{\pi}{6} - \epsilon_{2}\right]\)
\(\epsilon_{1},\ \epsilon_{2} > 0\) and \$\epsilon\emph{\{1\}
\textless{} ~\frac{\pi}{6} - \epsilon}\{2\}\$ are usually associated
with the business-cycle and correspond to a period longer than a year
and bounded\footnote{Ibid.}. The frequencies in the range
\[$\frac{\pi}{6},\ \pi\]\$ are associated with the short term movements,

whose cycle is completed in less than a year. If a series contains an
important periodic component, its spectrum reveals a peak around the
corresponding frequency and in the ARIMA model it is captured by an AR
root. In the example below spectral peaks occur at the frequency
\(\omega = 0\) and at the seasonal frequencies ( \(\frac{\pi}{6}\),
\(\frac{2\pi}{6},\ \frac{3\pi}{6},\ \frac{4\pi}{6},\frac{5\pi}{6},\pi\)).
\footnote{KAISER, R., and MARAVALL, A. (2000).}

\begin{figure}

{\centering \includegraphics{./All_images/UG_A_image6.png}

}

\caption{Text}

\end{figure}

\textbf{The pseudo-spectrum for a monthly series}

In the decomposition procedure, the pseudo-spectrum of the time series
\(x_{t}\) is divided into the spectra of its components (in the example
figure below, four components were obtained).

\begin{figure}

{\centering \includegraphics{./All_images/UG_A_image7.png}

}

\caption{Text}

\end{figure}

\textbf{The pseudo-spectra for the components}

\hypertarget{estimation-of-the-components-with-the-wiener-kolmogorow-filter}{%
\subsubsection{Estimation of the components with the Wiener-Kolmogorow
filter}\label{estimation-of-the-components-with-the-wiener-kolmogorow-filter}}

The various components are estimated using Wiener-Kolmogorow (WK)
filters. JDemetra+ includes three options to estimate the WK filter,
namely \emph{Burman}, \emph{KalmanSmoother} and
\emph{MCElroyMatrix}\footnote{The choice of the estimation method is
  controlled by the \emph{Method} parameter, explained in the
  \href{../reference-manual/sa-spec-tramo.html}{SEATS specification}
  section.}. Here the first of abovementioned options, proposed by
BURMAN, J.P. (1980) will be explained.

The estimation procedure and the properties of the WK filter are easier
to explain with a two-component model. Let the seasonally adjusted
series (\(s_{t}\)) be the signal of interest and the seasonal component
(\(n_{t}\)) be the remainder, ``the noise''. The series is given by the
model \[2\] and from \[4\] the models for theoretical components are:

\[\varphi_{s}(B)s_{t} = \theta_{s}(B)a_{\text{st}}\] \[20\]

and

\[\varphi_{n}(B)n_{t} = \theta_{n}(B)a_{\text{nt}}\]. \[21\]

From \[6\] and \[7\] it is clear that
\(\varphi\left( B \right) = \varphi_{s}(B)\varphi_{n}(B)\) and
\(\theta\left( B \right)a_{t} = \theta_{s}(B)a_{\text{st}}+\theta_{n}(B)a_{\text{nt}}\).

As the time series components are never observed, their estimators have
to be used. Let us note \(X_{T}\) an infinite realization of the time
series \(x_{t}\). SEATS computes the Minimum Mean Square Error (MMSE)
estimator of \(s_{t}\), e.g.~the estimator \[\widehat{s}_{t}\] that
minimizes

\[E\lbrack\left({s_{t}-{\widehat{s}}_{t})}^{2}|X_{T} \right)\rbrack\].
Under the normality assumption \[{\widehat{s}}_{t|T}\] is also equal to
the conditional expectation \[E\left(s_{t}|X_{T}\right)\], so it can be
presented as a linear function of the elements in \[X_{T}\].\footnote{MARAVALL,
  A. (2008c).} WHITTLE (1963) shows that the MMSE

estimator of \[{\widehat{s}}_{t}\] is:

\[{\widehat{s}}_{t} = k_{s}\frac{\psi_{s}(B)\psi_{s}(F)}{\psi(B)\psi(F)}x_{t}\],
\[22\]

where \[\psi(B)= \frac{\theta(B)}{\phi(B)}\],

\[F = B^{- 1}\] and \[k_{s}=\frac{V(a_{s})}{V(a)}\],

\[V(a_{s})\] is the variance of \[a_{st}\] and \[V(a)\] is the variance

of \[a_{t}\].

Expressing the \[\psi\left(B\right)\] polynomials as functions of the AR
and MA polynomials, after cancelation of roots, the estimator of

\[s_{t}\] can be expressed as:

\[{\widehat{s}}_{t} = k_{s}\frac{\theta_{s}\left(B\right)\theta_{s}\left(F\right)\varphi_{n}\left(B \right)\delta_{n}\left(B\right)\varphi_{n}\left(F\right)\delta_{n}\left(F\right)}{\theta\left(B\right)\theta\left(F \right)}x_{t}\],

\[23\]

where:

\[\nu_{s}\left( B,F \right) = k_{s}\frac{\theta_{s}\left( B \right)\theta_{s}\left( F \right)\varphi_{n}\left( B \right)\delta_{n}\left( B \right)\varphi_{n}\left( F \right)\delta_{n}\left( F \right)}{\theta\left( B \right)\theta\left( F \right)}\]

\[24\]

is a WK filter.

Equation \[24\] shows that the WK filter is two-sided (uses observations
both from the past and from the future), centered (the number of points
in the past is the same as in the future) and symmetric (for any \(k\)
the weight applied to \(x_{t - k}\) and \(x_{t + k}\) is the same),
which allows the phase effect to be avoided. Due to invertibility of
\(\theta\left( B \right)\) (and \(\theta\left( F \right)\)) the filter
is convergent in the past and in the future.

The estimator can be presented as

\[{\widehat{s}}_{t} = \nu_{i}\left(B,F\right)x_{t}\], \[25\]

where

\[\nu_{i}\left(B,F\right)=\nu_{0}+ \sum_{j = 1}^{\infty}\nu_{ij}(B^{j}+F^{j})\]

is the WK filter.

The example of the WK filters obtained for the pseudo-spectra of the
series illustrated above is shown on the figure below: WK filters for
components.

\begin{figure}

{\centering \includegraphics{./All_images/UG_A_image8.png}

}

\caption{Text}

\end{figure}

\textbf{WK filters for components}

The WK filter from \[24\] can also be expressed as a ratio of two
pseudo-autocovariance generating functions (p-ACGF). The p-ACGF function
summarizes the sequence of absolutely summable autocovariances of a
stationary process \(x_{t}\) (see section section
\href{../theory/spectral.html}{Spectral Analysis}).

The ACGF function of an ARIMA process is expressed as:

\[acgf(B) = \frac{\theta\left( B \right)\theta\left( F \right)}{\phi\left( B \right)\delta\left( B \right)\phi\left( F \right)\delta\left( F \right)}V(a)\]

\[26\]

And, the WK filter can be rewritten as:

\[\nu_{s}\left( B,F \right) = \frac{\gamma_{s}(B,F)}{\gamma(B,F)}\],

\[27\]

where:

\[\gamma_{s}\left( B,F \right) = \frac{\theta_{s}\left( B \right)\theta_{s}\left( F \right)}{\phi_{s}\left( B \right)\delta_{s}\left( B \right)\phi_{s}\left( F \right)\delta_{s}\left( F \right)}V(a_{s})\]

is the p-ACGF of \[s_{t}\];

\(\gamma\left( B,F \right) = \frac{\theta\left( B \right)\theta\left( F \right)}{\phi\left( B \right)\delta\left( B \right)\phi\left( F \right)\delta\left( F \right)}V(a)\)
is the p-ACGF of \(x_{t}\).

From \[24\] it can be seen that the WK filter depends on both the
component and the series models. Consequently, the estimator of the
component and the WK filter reflect the characteristic of data and by
construction, the WK filter adapts itself to the series under
consideration. Therefore, the ARIMA model is of particular importance
for the SEATS method. Its misspecification results in an incorrect
decomposition.

This adaptability, if the model has been correctly determined, avoids
the dangers of under and overestimation with an ad-hoc filtering. For
example, for the series with a highly stochastic seasonal component the
filter adapts to the width of the seasonal peaks and the seasonally
adjusted series does not display any spurious seasonality\footnote{MARAVALL,
  A. (1995).}. Examples of WK filters for stochastic and stable seasonal
components are presented on the figure below.

\begin{figure}

{\centering \includegraphics{./All_images/UG_A_image9.png}

}

\caption{Text}

\end{figure}

\textbf{WK filters for stable and stochastic seasonal components}

The derivation of the components requires an infinite realization of
\(x_{t}\) in the direction of the past and of the future. However, the
convergence of the WK filter guarantees that, in practice, it could be
approximated by a truncated (finite) filter and, in most applications,
for large \[k\] the estimator for the central periods of the series can
be safely seen as generated by the WK filter\footnote{MARAVALL, A., and
  PLANAS, C. (1999).}:

\[{\widehat{s}}_{t}=\nu_{k}x_{t-k} + \ldots + \nu_{0}x_{t} + \ldots + \nu_{k}x_{t+k}\].

\[28\]

When \(T > 2L + 1\), where \(T\) is the last observed period, and \(L\)
is an a priori number that typically expands between 3 and 5 years, the
estimator expressed by \[23\] can be assumed as the final (historical)
estimator for the central observations of the series\footnote{MARAVALL,
  A. (1998).}. In practice, the Wiener-Kolmogorov filter is applied to
\(x_{t}\) extended with forecasts and backcasts from the ARIMA model.
The final or historical estimator of \[{\widehat{s}}_{t}\], is obtained
with a doubly infinite filter, and

therefore contains an error \[e_{st}\] called final estimation error,
which is equal \[e_{st}=s_{t}-{\widehat{s}}_{t}\].

In the frequency domain, the Wiener-Kolmogorov filter\(\ \nu(B,F)\) that
provides the final estimator of \$s\_\{t\}\$is expressed as the ratio of
the \$s\_\{t\}\$and \(x_{t}\) pseudo-spectra:

\[\widetilde{\nu}\left( \omega \right) = \frac{g_{s}(\omega)}{g_{x}(\omega)}\].
\[29\]

The function \$\widetilde{\nu}\left( \omega \right)\$is also referred as
the gain of the filter.\footnote{GÓMEZ, V., and MARAVALL, A. (2001a).}
GÓMEZ, V., and MARAVALL, A. (2001a) show that when for some frequency
the signal (the seasonally adjusted series) dominates the noise
(seasonal fluctuations) the gain
\(\widetilde{\nu}\left( \omega \right)\) approaches 1. On the contrary,
when for some frequency the noise dominates the gain
\$\widetilde{\nu}\left( \omega \right)\$approaches 0.

The spectrum of the estimator of the seasonal component is expressed as:

\[g_{\widehat{s}}\left( \omega \right) = \left\lbrack \frac{g_{s}(\omega)}{g_{x}(\omega)} \right\rbrack^{2}g_{x}(\omega)\],
\[30\]

where\(\ \left\lbrack \widetilde{\nu}\left( \omega \right) \right\rbrack^{2} = \left\lbrack \frac{g_{s}(\omega)}{g_{x}(\omega)} \right\rbrack^{2} = \left\lbrack \frac{g_{s}(\omega)}{g_{s}(\omega) + g_{n}(\omega)} \right\rbrack^{2} = \left\lbrack \frac{1}{1 + \frac{1}{r(\omega)}} \right\rbrack^{2}\)
is the squared gain of the filter and
\(r\left( \omega \right) = \frac{g_{s}(\omega)}{g_{n}(\omega)}\)
represents the signal-to-noise ratio.

For each \(\omega\), the MMSE estimation gives the signal-to-noise
ratio. If this ratio is high, then the contribution of that frequency to
the estimation of the signal will be also high. Assume that the trend is
a signal that needs to be extracted from a seasonal time series. Then
\$R\left( 0 \right) = 1\$and the frequency \(\omega = 0\) will only be
used for trend estimations. For seasonal frequencies
\(R\left( \omega \right) = 0,\) so that these frequencies are ignored in
computing the trend resulting in spectral zeros in
\(g_{\widehat{s}}\left( \omega \right)\). For this reason, unlike the
spectrum of the component, the component spectrum contains dips as it
can be seen on the figure below: Component spectrum and estimator
spectrum for trend.

\begin{figure}

{\centering \includegraphics{./All_images/UG_A_image10.png}

}

\caption{Text}

\end{figure}

\textbf{Component spectrum and estimator spectrum for trend}

From the equation \[29\] it is clear that the squared gain of the filter
determines how the variance of the series contributes to the variance of
the seasonal component for the different frequencies. When
\(\widetilde{\nu}\left( \omega \right) = 1\), the full variation of
\(x_{t}\) for that frequency is passed to \[{\widehat{s}}_{t}\], while
if

\[\widetilde{\nu}\left(\omega\right) = 0 \] the variation of \(x_{t}\)
for that frequency is fully ignored in the computation of
\[{\widehat{s}}_{t}\]. These two cases are well illustrated by the

figure below that shows the square gain of the WK filter for two series
already analysed in the figure above (Figure: WK filters for stable and
stochastic seasonal components).

\begin{figure}

{\centering \includegraphics{./All_images/UG_A_image11.png}

}

\caption{Text}

\end{figure}

\textbf{The squared gain of the WK filter for stable and stochastic
seasonal components.}

Since \(r\left( \omega \right) \geq 0\), then
\(\widetilde{\nu}\left( \omega \right) \leq 1\) and from \[29\] it can
be derived that
\(g_{\widehat{s}}\left( \omega \right) = \widetilde{\nu}\left( \omega \right)g_{s}(\omega)\).
As a result, the estimator will always underestimate the component, i.e.
it will be always more stable that the component.\footnote{Ibid.}

Since
\(g_{\widehat{n}}\left( \omega \right) < g_{n}\left( \omega \right)\)
and\(\ g_{\widehat{s}}\left( \omega \right) < g_{s}\left( \omega \right)\)
the expression:
\(g_{x}\left( \omega \right) - \left\lbrack g_{\widehat{n}}\left( \omega \right) + g_{\widehat{s}}\left( \omega \right) \right\rbrack \geq 0\)
is the cross-spectrum. As it is positive, the MMSE yields correlated
estimators. This effect emerges since variance of estimator is smaller
than the variance of component. Nevertheless, if at least one
non-stationary component exists, cross-correlations estimated by
TRAMO-SEATS will tend to zero as cross-covariances between estimators of
the components are finite. In practice, the inconvenience caused by this
property will likely be of little relevance.

\textbf{Preliminary estimators for the components}

GÓMEZ, V., and MARAVALL, A. (2001a) point out that \emph{the properties
of the estimators have been derived for the final (or historical)
estimators. For a finite (long enough) realization, they can be assumed
to characterize the estimators for the central observations of the
series, but for periods close to the beginning of the end the filter
cannot be completed and some preliminary estimator has to be used}.
Indeed, the historical estimator shown in \[28\] is obtained for the
central periods of the series. However, when \(t\) approaches \(T\)
(last observation), the WK filter requires observations, which are not
available yet. For this reason a preliminary estimator needs to be used.

To introduce preliminary estimators let us consider a semi-finite
realization \(\lbrack x_{- \infty}\),\ldots{}\(\ x_{T}\){]}, where \(T\)
is the last observed period. The preliminary estimator of
\[x_{\text{it}}\] obtained at \(T\) \[(T - t = k \geq 0)\] can be
expressed as

\[
{\widehat{x}}_{it|t + k}=\nu_{i}\left(B,F\right)x_{t|T}^{e}
\], \[31\]

where \[\nu_{i}\left(B,F \right)\] is the WK filter and \[x_{t|T}^{e}\]
is the extended series, such that \(x_{t|T}^{e} = x_{t}\) for
\(t \leq T\) and \[x_{t|T}^{e}={\widehat{x}}_{t|T}\] for \[t>T\], where

\[{\widehat{x}}_{t|T}\] denotes the forecast of \(x_{t}\) obtained at
period \(T\).

The future \(k\) values necessary to apply the filter are not yet
available and are replaced by their optimal forecasts from the ARIMA
model on \[x_{t}\]. When \[k=0\] the preliminary estimator becomes the
concurrent estimator. As the forecasts are linear functions of present
and past observations of \[x_{t}\], the preliminary estimator
\[{\widehat{x}}_{it}\] will be a truncated asymmetric filter applied to

\[x_{t}\] that generates a phase effect\footnote{KAISER, R., and
  MARAVALL, A. (2000).}.

When a new observation \[x_{T + 1}\] becomes available the forecast
\[{\widehat{x}}_{T + 1|T}\] is replaced by the observation and the

forecast \[{\widehat{x}}_{iT + j|T}\], \[j > 1\] are updated to

\[x_{T + j|T + 1}\] resulting in the revision error\footnote{MARAVALL,
  A. (1995).}. The total error in the preliminary estimator
\[d_{it|t + k}\] is expressed as a sum of the final estimation error
(\[e_{it}\]) and the revision error (\[r_{it|t + k}\]), i.e.:

\$\$

d\_\{it\textbar t + k\} = x\_\{it\}-\{\widehat{x}\}\emph{\{it\textbar t
+ k\} = \left(x}\{it\} - \{\widehat{x}\}\emph{\{it\}\right) + \left(
\{\widehat{x}\}}\{it\} - \{\widehat{x}\}\emph{\{it\textbar t + k\}
\right) = e}\{it\} + r\_\{it\textbar t + k\}

\[, \]32\$\$

where:

\[x_{it}-i^{th}\] component;

\[{\widehat{x}}_{it|t + k}\]- the estimator of \[x_{it}\] when the last
observation is \[x_{t + k}\].

Therefore the preliminary estimator is subject not only to the final
error but also to a revision error, which are orthogonal to each
other\footnote{MARAVALL, A. (2009).}. The revision error decreases as
\[k\] increases, until it can be assumed equal to 0 for large enough
\[k\].

It's worth remembering that SEATS estimates the unobservable components
of the time series so the ``true'' components are never observed.
Therefore, MARAVALL, A. (2009) stresses that \emph{the error in the
historical estimator is more of academic rather than practical interest.
In practice, interest centres on revisions. (\ldots) the revision
standard deviation will be an indicator of how far we can expect to be
from the optimal estimator that will be eventually attained, and the
speed of convergence of} \({\theta\left( B \right)\ }^{- 1}\) \emph{will
dictate the speed of convergence of the preliminary estimator to the
historical one.} The analysis of an error is therefore useful for making
decision concerning the revision policy, including the policy for
revisions and horizon of revisions.

\hypertarget{psie-weights}{%
\subsubsection{PsiE-weights}\label{psie-weights}}

The estimator of the component is calculated as
\[{\widehat{x}}_{it} = \nu_{s}\left(B,F\right)x_{t}\]. By replacing

\[x_{it}=\frac{\theta(B)}{\gamma(B)\delta(B)}a_{t}\], the component
estimator can be expressed as\footnote{The section is based on KAISER,
  R., and MARAVALL, A. (2000).}:

\$\$

\{\widehat{x}\}\emph{\{it\} = \xi}\{s\}\left(B,F\right)a\_\{t\}
\[, \]33\$\$

where
\(\xi_{s}\left( B,F \right) = \ldots + \xi_{j}B^{j} + \ldots + \xi_{1}B + \xi_{0} + \xi_{- 1}F\ldots\xi_{- j}F^{j} + \ldots\).

This representation shows the estimator as a filter applied to the
innovation \[a_{t}\], rather than on the series

\[x_{t}\]\footnote{See section PsiE-weights. For further details see
  MARAVALL, A. (2008).}. Hence, the filter from \[32\]

can be divided into two components: the first one, i.e.

\[\ldots + \xi_{j}B^{j}+ \ldots+ \xi_{1}B + \xi_{0}\], applies to prior
and concurrent innovations, the second one, i.e.
\[\xi_{- 1}F + \ldots + \xi_{- j}F^{j}\] applies to future (i.e.

posterior to \[t\]) innovations. Consequently, \[\xi_{j}\] determines
the contribution of \[a_{t - j}\] to \[{\widehat{s}}_{t}\] while

\[\xi_{- j}\] determines the contribution of \[a_{t + j}\] to
\[{\widehat{s}}_{t}\]. Finally, the estimator of the component can be

expressed as:

\[
  {\widehat{x}}_{it} =\xi_{i}(B)^{-}a_{t} + \xi_{i}(F)^{+}a_{t + 1}
  \], \[34\]

where:

\(\xi_{i}{(B)}^{-}a_{t}\) is an effect of starting conditions, present
and past innovations in series;

\(\xi_{i}{(F)}^{+}a_{t + 1}\) is an effect of future innovations.

For the two cases already presented in figure \emph{WK filters for
stable and stochastic seasonal components} and figure \emph{The squared
gain of the WK filter for stable and stochastic seasonal components}
above, the psi-weights are shown in the figure below.

\begin{figure}

{\centering \includegraphics{./All_images/UG_A_image12.png}

}

\caption{Text}

\end{figure}

It can be shown that \[{\xi}_{- 1},\ldots,\xi_{- j}\] are convergent and

\[\xi_{j},\ldots, {\xi}_{1},\xi_{0}\] are divergent. From \[33\]

, the concurrent estimator is equal to \[
{\widehat{x}}_{it|t} = E_{t}x_{it}=E_{t}{\widehat{x}}_{it} = {\xi}_{i}(B)^{-}a_{t}
\], \[35\]

so that the revision \$\$ r\_\{it\} = \{\widehat{x}\}\emph{\{it\} -
\{\widehat{x}\}}\{it\textbar t\} = \xi\emph{\{i\}(F)\^{}\{+\}a}\{t + 1\}

\[ \]36\$\$

is a zero-mean stationary MA process. As a result, historical and
preliminary estimators are cointegrated. From expression \[25\] the
relative size of the full revision and the speed of convergence can be
obtained.

\hypertarget{quality-assesment-of-the-seasonal-adjustement-process}{%
\subsection{Quality assesment of the seasonal adjustement
process}\label{quality-assesment-of-the-seasonal-adjustement-process}}

\hypertarget{diagnostics-reading}{%
\subsubsection{Diagnostics reading}\label{diagnostics-reading}}

\hypertarget{residual-seasonality}{%
\subsubsection{Residual seasonality}\label{residual-seasonality}}

We consider below tests on the seasonally adjusted series (\(sa_t\)) or
on the irregular component (\(irr_t\)). When the reasoning applies on
both components, we will use \(y_t\). The functions \(stdev\) stands for
``standard deviation'' and \(rms\) for ``root mean squares''

The tests are computed on the log-transformed components in the case of
multiplicative decomposition.

\hypertarget{non-significant-irregular-1}{%
\subsection{Non significant
irregular}\label{non-significant-irregular-1}}

When \(ir_t\) is not significant, we don't compute the tests on it, to
avoid irrelevant results. We consider that \(ir_t\) is significant if
\(stdev( ir_t)>0.01\) (multiplicative case) or if
\(stdev(ir_t)/rms(sa_t) >0.01\) (additive case).

\hypertarget{qs-test}{%
\subsection{QS test}\label{qs-test}}

The QS test is similar to the Ljung-Box test computed on the first two
seasonal lags, except that negative auto-correlations are set to 0. The
tests are computed on \(\tilde \Delta sa_t\) and on
\(\tilde \Delta irr_t\). The operator \(\tilde \Delta\) applies as much
differencing as needed on the (log-transformed) series and corrects the
result for mean effect.

The tests are not computed if the differences are not ``significant''.
\(\tilde \Delta y_t\) is significant if
\(stdev(\tilde \Delta y_t)/rms(y_t) >0.005\).

\hypertarget{f-test-1}{%
\subsection{F test}\label{f-test-1}}

We compute by OLS the following model (SD = contrasts of seasonal
dummies, freq-1 variables):

\[y_t=\mu + \alpha y_{t-1} + \beta SD_t + \epsilon_t \]

The tests are the usual joint F-tests on \(\beta \quad (H_0:\beta=0)\).
By default, we compute the tests on the 8 last years of the components,
so that they might highlight moving seasonal effects.

We could consider various modelling of the series. For instance, \(y_t\)
could follow an ARIMA model, pre-specified or automatically identified.
The solution used in JD+ 2.2 has the advantage of being fast and robust.
It is rather similar to what is used, for instance, in the Canova-Hansen
test.

\hypertarget{revision-history}{%
\subsubsection{Revision history}\label{revision-history}}

\begin{itemize}
\tightlist
\item
  link to implementation in GUI
\item
  add implemenation in R ?
\item
  add illustrations ? (gui graphs ? estp training)
\end{itemize}

Revisions are calculated as differences between the first (earliest)
adjustment of an observation at time \(t\), computed when this
observation is the last observation of the time series (\emph{concurrent
adjustment,} denoted as \(A_{t|t}\)) and a later adjustment based on all
future data available at the time of the diagnostic analysis (\emph{the}
\emph{most recent} \emph{adjustment,} denoted as \(A_{t|N}\)).

In the case of the multiplicative decomposition the revision history of
the seasonal adjustment from time \$N\_\{0\}\$to \(N_{1}\) is a sequence
of \(R_{t|N}^{A}\) calculated in the following way :

\$\$

R\_\{t\textbar N\}\^{}\{A\} = 100
\times \frac{A_{t|N} - A_{t|t}}{A_{t|t}} \$\$

The revision history of the trend is computed in the same manner.

With an additive decomposition \(R_{t|N}^{A}\) is calculated in the same
way if all values \(A_{t|t}\) have the same sign. Otherwise differences
are calculated as:

\[
  R_{t|N}^{A} = A_{t|N} - A_{t|t}
  \]

The analogous expression for the trend component is:

\$\$

R\_\{t\textbar N\}\^{}\{T\} = T\_\{t\textbar N\} - T\_\{t\textbar t\}
\$\$

Revision in the period-to-period (month-on-month or quarter-to-quarter)
change in the seasonally adjusted series at time \(t\) calculated from
the series \(y_{1},y_{2},\ldots y_{n}\) is defined as:

\[
  R_{t}^{A} = C_{t|N} - C_{t|t}
  \]

where \[
\text{C}_{t|n}^{A} = \frac{A_{t|n} - A_{t - 1|n}}{A_{t - 1|n}}
\].

Revisions for the period-to-period changes in the trend component are
computed in the same manner.

\hypertarget{sliding-spans}{%
\subsection{Sliding spans}\label{sliding-spans}}

The sliding spans technique involves the comparison of the correlated
seasonal adjustments of a given period obtained by applying the
adjustment procedure to a sequence of two, three or four overlapping
spans of data, all of which contain this period (month or
quarter)\footnote{FINDLEY, D., MONSELL, B.C., SHULMAN, H.B., and PUGH,
  M.G. (1990).}.

Each period that belongs to more than one span is examined to see if its
seasonal adjustments vary more than a specified amount across the
spans\footnote{FINDLEY, D., MONSELL, B.C., BELL, W., OTTO, M., and CHEN,
  B.-C. (1990).}. For the multiplicative decomposition a seasonal factor
is regarded to be unreliable if the following condition is fulfilled:

\[
  S_{t}^{\max} = \frac{\max_{k \in N_{t}}S_{t}\left( k \right) - \min_{k \in N_{t}}S_{t}(k)}{\min_{k \in N_{t}}S_{t}(k)} > 0.03
  \]

where:

\(S_{t}(k)\) -- the seasonal factor estimated from span \(k\) for month
(quarter) \(t\);

\(N_{t}\) -- \{\(\text{k}\): month (quarter) \(\text{t}\) is in the
\(k\)-th span\}.

For the additive decomposition JDemetra+ uses the rule in equation
{[}2{]} for checking for the reliability of the seasonal factor.

\$\$

S\_\{t\}\^{}\{\max\} =
\frac{\max_{k \in N_{t}}S_{t}\left( k \right) - \min_{k \in N_{t}}S_{t}(k)}{\sqrt{\frac{\sum_{i}^{n}y_{i}^{2}}{n}}}
\textgreater{} 0.03 \$\$

where:

\(n\) -- number of observations of the orginal time series \(y_{i}\).

The month-to-month percentage change in the seasonally adjusted value
from span \(k\) for month \(t\) is calculated as:

\[
 \text{MM}_{m}\left(k\right) = \frac{A_{m}\left(k\right) - A_{m - 1}\left(k\right)}{A_{m - 1}\left(k\right)}
 \]

where:

\(A_{m}\left( k \right)\) -- the seasonally (and trading day) adjusted
value from span \(k\) for month \(t\);

\(\text{MM}_{m}\left( k \right)\) is considered unreliable if the
statistics below is higher than 0.03.

\$\$

\text{MM}\emph{\{m\}\^{}\{\max\} = \max}\{k
\in {N1}\emph{\{m\}\}\text{MM}}\{m\}\left( k \right) - \min\emph{\{k
\in {N1}}\{m\}\}\text{MM}\_\{m\}\left( k \right) \textgreater{} 0.03

\$\$

where:

\({N1}_{t}\) -- \{\(\text{k}\): month \(\text{t}\) and \(t\)-1 are in
the \(k\)-th span\}.

The respective formula for the quarter-to-quarter percentage change in
the seasonally adjusted value from span \(k\) for quarter \(t\) is
calculated as:

\$\$

\text{QQ}\_\{q\}\left( k \right) =
\frac{A_{q}\left( k \right) - A_{q - 1}\left( k \right)}{A_{q - 1}\left( k \right)}

\$\$

where:

\(A_{q}\left( k \right)\) -- the seasonally (and trading day) adjusted
value from span \(k\) for quarter \(q\).

\(\text{QQ}_{q}\left( k \right)\) is considered unreliable if the
statistics below is higher than 0.03.

\$\$

\text{QQ}\emph{\{q\}\^{}\{\max\} = \max}\{k
\in {N1}\emph{\{q\}\}\text{QQ}}\{q\}\left( k \right) - \min\emph{\{k
\in {N1}}\{t\}\}\text{QQ}\_\{q\}\left( k \right) \textgreater{} 0.03

\$\$

where:

\({N1}_{q}\) -- \{\(\text{k}\): quarter \(\text{t}\) and \(t\)-1 are in
the \(k\)-th span\}.

The respective diagnostic can be also performed for the trading
days/working days component.

\hypertarget{revision-policies}{%
\subsection{Revision policies}\label{revision-policies}}

\begin{itemize}
\tightlist
\item
  harmonize voc (cf.~cruncher vignette) and guidelines pb
\item
  add controlled current
\item
  add bbk plugin
\item
  replace descp by my pdf ? (how to allow modif in the project ?)
  (auxiliary file)
\end{itemize}

A description of the options is presented in the following table.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.7500}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Option}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Description}
\end{minipage} \\
\midrule()
\endhead
Partial concurrent adjustment → Fixed model & The ARIMA model, outliers
and other regression parameters are not re-identified and the values of
all parameters are fixed. The transformation type remains unchanged. \\
Partial concurrent adjustment → Estimate regression coefficients & The
ARIMA model, outliers and other regression parameters are not
re-identified. The coefficients of the ARIMA model are fixed, other
coefficients are re-estimated. The transformation type remains
unchanged. \\
Partial concurrent adjustment → Estimate regression coefficients + Arima
parameters & The ARIMA model, outliers and other regression parameters
are not re-identified. All parameters of the RegARIMA model are
re-estimated. The transformation type remains unchanged. \\
Partial concurrent adjustment → Estimate regression coefficients + Last
outliers & The ARIMA model, outliers (except from the outliers in the
last year of the sample) and other regression parameters are not
re-identified. All parameters of the RegARIMA model are re-estimated.
The outliers in the last year of the sample are re-identified. The
transformation type remains unchanged. \\
Partial concurrent adjustment → Estimate regression coefficients + all
outliers & The ARIMA model and regression parameters, except from
outliers) are not re-identified. All parameters of the RegARIMA model
are re-estimated. All outliers are re-identified. The transformation
type remains unchanged. \\
Partial concurrent adjustment → Estimate regression coefficients + Arima
model & Re-identification of the ARIMA model, outliers and regression
variables, except from the calendar variables. The transformation type
remains unchanged. \\
Concurrent & Re-identification of the whole RegARIMA model. \\
\bottomrule()
\end{longtable}

\hypertarget{partial-concurrent-adjustment}{%
\paragraph{\texorpdfstring{\textbf{Partial concurrent
adjustment}}{Partial concurrent adjustment}}\label{partial-concurrent-adjustment}}

According to the \emph{ESS Guidelines on Seasonal Adjustment} (2015),
partial concurrent adjustment is the strategy in which the model,
filters, outliers and calendar regressors are re-identified once a year
and the respective parameters and factors re-estimated every time new or
revised data become available. JDemetra+ offers several types of partial
concurrent adjustment.

\hypertarget{practical-steps}{%
\subsection{Practical steps}\label{practical-steps}}

\hypertarget{graphical-analysis-recommendations}{%
\subsubsection{Graphical analysis
recommendations}\label{graphical-analysis-recommendations}}

(edit) (link to GUI graphical capabilities, graphs with rjd) The Tools
menu includes, among other functionalities, tools that are helpful for
the graphical analysis of a time series. The
\href{https://www.census.gov/ts/x13as/docX13ASHTML.pdf}{\emph{`X-13ARIMA-SEATS
Reference Manual' (2015)}} strongly recommends studying a high
resolution plot of the time series as it is helpful to gain insight on
issues, such as, seasonal patterns, potential outliers and stochastic
non-stationarity. Also the \emph{`ESS Guidelines on Seasonal Adjustment'
(2015)} recommends carrying out a graphical analysis on both unadjusted
data and the initial run of the seasonal adjustment. Graphical analysis
should consider: * The length of the series and the model span;\\
* The presence of zeros, outliers or problems in the data;\\
* The structure of the series: presence of; long term and cyclical
movements, seasonal components, volatility etc.;\\
* The presence of possible breaks in the seasonal behaviour;\\
* The decomposition scheme (additive, multiplicative).

The
\href{https://ec.europa.eu/eurostat/documents/3859598/6830795/KS-GQ-15-001-EN-N.pdf/d8f1e5f5-251b-4a69-93e3-079031b74bd3}{\emph{`ESS
Guidelines on Seasonal Adjustment' (2015)}}, recommend that this
exercise should be performed and documented for the most important
series to be adjusted at least once a year. The following
functionalities are available from the \emph{Tools} menu:

\hypertarget{seasonal-adjutsment-of-high-frequency-data}{%
\chapter{Seasonal adjutsment of high frequency
data}\label{seasonal-adjutsment-of-high-frequency-data}}

\hypertarget{motivation-1}{%
\section{Motivation}\label{motivation-1}}

\hypertarget{ubiquitous-use}{%
\subsection{Ubiquitous use}\label{ubiquitous-use}}

\hypertarget{data-specificities}{%
\subsection{Data specificities}\label{data-specificities}}

\hypertarget{tools}{%
\section{Tools}\label{tools}}

code here and/or link to R packages chapter

\hypertarget{unobserved-components}{%
\section{Unobserved Components}\label{unobserved-components}}

\hypertarget{seasonality-tests-1}{%
\section{Seasonality tests}\label{seasonality-tests-1}}

\hypertarget{spectral-analysis-1}{%
\subsection{Spectral analysis}\label{spectral-analysis-1}}

\hypertarget{pre-adjustment-1}{%
\section{Pre-adjustment}\label{pre-adjustment-1}}

\hypertarget{calendar-correction-1}{%
\subsection{Calendar correction}\label{calendar-correction-1}}

\hypertarget{outliers-and-intervention-variables-1}{%
\subsection{Outliers and intervention
variables}\label{outliers-and-intervention-variables-1}}

\hypertarget{linearization}{%
\subsection{Linearization}\label{linearization}}

\hypertarget{decomposition-1}{%
\section{Decomposition}\label{decomposition-1}}

\hypertarget{extended-x-11}{%
\subsection{Extended X-11}\label{extended-x-11}}

\hypertarget{stl-decompostion}{%
\subsection{STL decompostion}\label{stl-decompostion}}

\hypertarget{arima-model-based-amb-decomposition}{%
\subsection{Arima Model Based (AMB)
Decomposition}\label{arima-model-based-amb-decomposition}}

\hypertarget{state-space-framework}{%
\subsection{State Space framework}\label{state-space-framework}}

\hypertarget{quality-assessment}{%
\section{Quality assessment}\label{quality-assessment}}

\hypertarget{residual-seasonality-1}{%
\subsection{Residual seasonality}\label{residual-seasonality-1}}

\hypertarget{residual-celendar-effects}{%
\subsection{Residual Celendar effects}\label{residual-celendar-effects}}

\hypertarget{arima-model}{%
\subsection{Arima Model}\label{arima-model}}

\hypertarget{forecasting}{%
\section{Forecasting}\label{forecasting}}

\hypertarget{references-4}{%
\chapter*{References}\label{references-4}}
\addcontentsline{toc}{chapter}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{0}{0}
\end{CSLReferences}



\end{document}
